{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 109A/STAT 121A/AC 209A/CSCI E-109A: Homework 4\n",
    "# Regularization, High Dimensionality, PCA\n",
    "\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2017**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Kevin Rader, Rahul Dave, Margo Levine\n",
    "\n",
    "---\n",
    "\n",
    "### INSTRUCTIONS\n",
    "\n",
    "- To submit your assignment follow the instructions given in canvas.\n",
    "- Restart the kernel and run the whole notebook again before you submit. \n",
    "- Do not include your name(s) in the notebook even if you are submitting as a group. \n",
    "- If you submit individually and you have worked with someone, please include the name of your [one] partner below. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your partner's name (if you submit separately):\n",
    "\n",
    "Enrollment Status (109A, 121A, 209A, or E109A):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.api import OLS\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuing Bike Sharing Usage Data\n",
    "\n",
    "In this homework, we will focus on multiple linear regression, regularization, dealing with high dimensionality, and PCA. We will continue to build regression models for the Capital Bikeshare program in Washington D.C.  See Homework 3 for more information about the data.\n",
    "\n",
    "*Note: please make sure you use all the processed data from HW 3 Part (a)...you make want to save the data set on your computer and reread the csv/json file here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>holiday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>count</th>\n",
       "      <th>season_1.0</th>\n",
       "      <th>season_2.0</th>\n",
       "      <th>season_3.0</th>\n",
       "      <th>...</th>\n",
       "      <th>day_of_week_4.0</th>\n",
       "      <th>day_of_week_5.0</th>\n",
       "      <th>day_of_week_6.0</th>\n",
       "      <th>weather_1.0</th>\n",
       "      <th>weather_2.0</th>\n",
       "      <th>weather_3.0</th>\n",
       "      <th>temp_norm</th>\n",
       "      <th>atemp_norm</th>\n",
       "      <th>humidity_norm</th>\n",
       "      <th>windspeed_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>56.2083</td>\n",
       "      <td>0.194037</td>\n",
       "      <td>3830.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.341801</td>\n",
       "      <td>-1.363792</td>\n",
       "      <td>-0.500703</td>\n",
       "      <td>0.040945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>65.2917</td>\n",
       "      <td>0.350133</td>\n",
       "      <td>2114.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.431146</td>\n",
       "      <td>-1.665877</td>\n",
       "      <td>0.132958</td>\n",
       "      <td>2.036025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>56.8333</td>\n",
       "      <td>0.149883</td>\n",
       "      <td>915.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.695943</td>\n",
       "      <td>1.757749</td>\n",
       "      <td>-0.457103</td>\n",
       "      <td>-0.523392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>49.0833</td>\n",
       "      <td>0.268033</td>\n",
       "      <td>4322.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.805728</td>\n",
       "      <td>-0.759623</td>\n",
       "      <td>-0.997746</td>\n",
       "      <td>0.986696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>69.7083</td>\n",
       "      <td>0.215171</td>\n",
       "      <td>6591.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.981180</td>\n",
       "      <td>0.952190</td>\n",
       "      <td>0.441062</td>\n",
       "      <td>0.311061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   holiday  workingday  temp  atemp  humidity  windspeed   count  season_1.0  \\\n",
       "0      0.0         1.0   2.0    6.0   56.2083   0.194037  3830.0           1   \n",
       "1      0.0         1.0   1.0    3.0   65.2917   0.350133  2114.0           1   \n",
       "2      0.0         1.0  36.0   37.0   56.8333   0.149883   915.0           0   \n",
       "3      0.0         1.0   8.0   12.0   49.0833   0.268033  4322.0           1   \n",
       "4      0.0         0.0  28.0   29.0   69.7083   0.215171  6591.0           0   \n",
       "\n",
       "   season_2.0  season_3.0       ...        day_of_week_4.0  day_of_week_5.0  \\\n",
       "0           0           0       ...                      1                0   \n",
       "1           0           0       ...                      1                0   \n",
       "2           1           0       ...                      1                0   \n",
       "3           0           0       ...                      0                0   \n",
       "4           1           0       ...                      0                0   \n",
       "\n",
       "   day_of_week_6.0  weather_1.0  weather_2.0  weather_3.0  temp_norm  \\\n",
       "0                0            1            0            0  -1.341801   \n",
       "1                0            0            1            0  -1.431146   \n",
       "2                0            0            1            0   1.695943   \n",
       "3                0            1            0            0  -0.805728   \n",
       "4                0            1            0            0   0.981180   \n",
       "\n",
       "   atemp_norm  humidity_norm  windspeed_norm  \n",
       "0   -1.363792      -0.500703        0.040945  \n",
       "1   -1.665877       0.132958        2.036025  \n",
       "2    1.757749      -0.457103       -0.523392  \n",
       "3   -0.759623      -0.997746        0.986696  \n",
       "4    0.952190       0.441062        0.311061  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('./data/train_processed.csv')\n",
    "test_df = pd.read_csv('./data/test_processed.csv')\n",
    "# I don't know why this keeps happening\n",
    "train_df = train_df.drop('Unnamed: 0', 1)\n",
    "test_df = test_df.drop('Unnamed: 0', 1)\n",
    "train_df.head()\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (f): Regularization/Penalization Methods\n",
    "\n",
    "As an alternative to selecting a subset of predictors and fitting a regression model on the subset, one can fit a linear regression model on all predictors, but shrink or regularize the coefficient estimates to make sure that the model does not \"overfit\" the training set. \n",
    "\n",
    "Use the following regularization techniques to fit linear models to the training set:\n",
    "- Ridge regression\n",
    "- Lasso regression\n",
    "    \n",
    "You may choose the shrikage parameter $\\lambda$ from the set $\\{10^{-5}, 10^{-4},...,10^{4},10^{5}\\}$ using cross-validation. In each case, \n",
    "\n",
    "- How do the estimated coefficients compare to or differ from the coefficients estimated by a plain linear regression (without shrikage penalty) in Part (b) fropm HW 3? Is there a difference between coefficients estimated by the two shrinkage methods? If so, give an explantion for the difference.\n",
    "- List the predictors that are assigned a coefficient value close to 0 (say < 1e-10) by the two methods. How closely do these predictors match the redundant predictors (if any) identified in Part (c) from HW 3?\n",
    "- Is there a difference in the way Ridge and Lasso regression assign coefficients to the predictors `temp` and `atemp`? If so, explain the reason for the difference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rmitchell/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEOCAYAAACaQSCZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116049780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Lasso: 10\n",
      "Best Ridge: 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=100, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use Ridge and Lasso regression to create models on the training set\n",
    "\n",
    "# Remove the numeric columns that have been normalized separately, as well as the redundant categorical columns\n",
    "remove_cols = ['temp', 'atemp', 'humidity', 'windspeed', 'count', 'season_3.0', 'month_12.0', 'day_of_week_6.0', 'weather_3.0']\n",
    "\n",
    "Xtrain = train_df[train_df.columns.difference(remove_cols)]\n",
    "Xtest  = test_df[test_df.columns.difference(remove_cols)]\n",
    "predictors = list(Xtrain)\n",
    "#  Create response \n",
    "ytrain = train_df['count']\n",
    "ytest  = test_df['count']\n",
    "\n",
    "alphas = [.00001, .0001, .001, .01, .1, 1, 5, 8, 10, 12, 20, 30, 35, 40, 45, 50, 100, 1000, 10000, 100000]\n",
    "r2s = {'ridge':[], 'lasso':[]}\n",
    "\n",
    "for alpha in alphas:\n",
    "    lasso = Lasso(alpha=alpha, fit_intercept=True)\n",
    "    lasso.fit(Xtrain, ytrain)\n",
    "    #print(\"ALPHA \"+str(alpha))\n",
    "    #print(clf.coef_)\n",
    "    #print(clf.intercept_)\n",
    "    lasso_preds = lasso.predict(Xtest)\n",
    "    r2s['lasso'].append(r2_score(ytest, lasso_preds))\n",
    "    \n",
    "    ridge = Ridge(alpha=alpha, fit_intercept=True)\n",
    "    ridge.fit(Xtrain, ytrain)\n",
    "    ridge_preds = ridge.predict(Xtest)\n",
    "    r2s['ridge'].append(r2_score(ytest, ridge_preds))\n",
    "    \n",
    "\n",
    "plt.semilogx(alphas, r2s['ridge'], label='Ridge')\n",
    "plt.semilogx(alphas, r2s['lasso'], label='Lasso')\n",
    "plt.ylabel(\"R2 Score\")\n",
    "plt.xlabel(\"Alpha\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "winningRidge = np.argmax(r2s['ridge'])\n",
    "winningLasso = np.argmax(r2s['lasso'])\n",
    "print(\"Best Lasso: \"+str(alphas[winningLasso]))\n",
    "print(\"Best Ridge: \"+str(alphas[winningRidge]))\n",
    "\n",
    "lasso = Lasso(alpha=alphas[winningLasso], fit_intercept=True)\n",
    "lasso.fit(Xtrain, ytrain)\n",
    "\n",
    "ridge = Ridge(alpha=alphas[winningRidge], fit_intercept=True)\n",
    "ridge.fit(Xtrain, ytrain)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predictors</th>\n",
       "      <th>Ridge</th>\n",
       "      <th>Lasso</th>\n",
       "      <th>Normal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>atemp_norm</td>\n",
       "      <td>471.6568679646016</td>\n",
       "      <td>452.3203677444383</td>\n",
       "      <td>312.4340718888971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>day_of_week_0.0</td>\n",
       "      <td>-84.42496035271257</td>\n",
       "      <td>-306.2418651503776</td>\n",
       "      <td>-465.1450099570902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>day_of_week_1.0</td>\n",
       "      <td>-89.06248939039172</td>\n",
       "      <td>-201.48785387674454</td>\n",
       "      <td>-256.6500506662668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>day_of_week_2.0</td>\n",
       "      <td>-31.18972949990181</td>\n",
       "      <td>-144.64983857205678</td>\n",
       "      <td>-328.18450689571296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>day_of_week_3.0</td>\n",
       "      <td>39.96112767844801</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.61277259517351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>day_of_week_4.0</td>\n",
       "      <td>25.985191169271925</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-71.64254439858638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>day_of_week_5.0</td>\n",
       "      <td>57.02446103414827</td>\n",
       "      <td>9.5111491691475</td>\n",
       "      <td>-21.83167487926312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>holiday</td>\n",
       "      <td>-43.34246184082051</td>\n",
       "      <td>-179.77933843895505</td>\n",
       "      <td>-616.602710298544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>humidity_norm</td>\n",
       "      <td>-354.47636370519723</td>\n",
       "      <td>-567.6253916153483</td>\n",
       "      <td>-548.492949058217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>month_1.0</td>\n",
       "      <td>-157.8301130654051</td>\n",
       "      <td>-65.66318839582621</td>\n",
       "      <td>118.83581871663205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>month_10.0</td>\n",
       "      <td>169.79217059417746</td>\n",
       "      <td>489.4999509358794</td>\n",
       "      <td>605.0867223839238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>month_11.0</td>\n",
       "      <td>17.283029518460193</td>\n",
       "      <td>0.0</td>\n",
       "      <td>231.51746393832212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>month_2.0</td>\n",
       "      <td>-79.93730267287751</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>207.77591139109884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>month_3.0</td>\n",
       "      <td>-23.00985804619239</td>\n",
       "      <td>40.220468628239814</td>\n",
       "      <td>358.01671712429095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>month_4.0</td>\n",
       "      <td>71.45743490019231</td>\n",
       "      <td>208.63177313325096</td>\n",
       "      <td>452.1849051340183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>month_5.0</td>\n",
       "      <td>47.91033771830972</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.02331873120352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>month_6.0</td>\n",
       "      <td>-40.65841108902188</td>\n",
       "      <td>-402.69284311000814</td>\n",
       "      <td>-673.4270797861233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>month_7.0</td>\n",
       "      <td>-126.4923107815563</td>\n",
       "      <td>-753.783096982615</td>\n",
       "      <td>-1161.1511875434737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>month_8.0</td>\n",
       "      <td>1.6273831785744657</td>\n",
       "      <td>-193.05295512099838</td>\n",
       "      <td>-657.6396711861676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>month_9.0</td>\n",
       "      <td>214.07402106448737</td>\n",
       "      <td>620.6678530499136</td>\n",
       "      <td>523.9803848202396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>season_1.0</td>\n",
       "      <td>-334.466387576266</td>\n",
       "      <td>-753.074227005702</td>\n",
       "      <td>-1032.8815748448596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>season_2.0</td>\n",
       "      <td>85.69731487079278</td>\n",
       "      <td>66.08814637433419</td>\n",
       "      <td>-134.0525353543187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>season_4.0</td>\n",
       "      <td>221.74064082352544</td>\n",
       "      <td>356.045957258328</td>\n",
       "      <td>193.30496780729308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>temp_norm</td>\n",
       "      <td>468.71723781348066</td>\n",
       "      <td>680.2124587130345</td>\n",
       "      <td>925.7338498566323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>weather_1.0</td>\n",
       "      <td>172.46275257419288</td>\n",
       "      <td>740.3489505883251</td>\n",
       "      <td>1581.978283607945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>weather_2.0</td>\n",
       "      <td>-7.096763515352191</td>\n",
       "      <td>727.7028379089465</td>\n",
       "      <td>1565.4116995877046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>windspeed_norm</td>\n",
       "      <td>-215.92299481288208</td>\n",
       "      <td>-248.16085928013868</td>\n",
       "      <td>-255.12258899234195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>workingday</td>\n",
       "      <td>46.061022832395146</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-24.093293946092842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         predictors                Ridge                Lasso  \\\n",
       "0        atemp_norm    471.6568679646016    452.3203677444383   \n",
       "1   day_of_week_0.0   -84.42496035271257   -306.2418651503776   \n",
       "2   day_of_week_1.0   -89.06248939039172  -201.48785387674454   \n",
       "3   day_of_week_2.0   -31.18972949990181  -144.64983857205678   \n",
       "4   day_of_week_3.0    39.96112767844801                  0.0   \n",
       "5   day_of_week_4.0   25.985191169271925                 -0.0   \n",
       "6   day_of_week_5.0    57.02446103414827      9.5111491691475   \n",
       "7           holiday   -43.34246184082051  -179.77933843895505   \n",
       "8     humidity_norm  -354.47636370519723   -567.6253916153483   \n",
       "9         month_1.0   -157.8301130654051   -65.66318839582621   \n",
       "10       month_10.0   169.79217059417746    489.4999509358794   \n",
       "11       month_11.0   17.283029518460193                  0.0   \n",
       "12        month_2.0   -79.93730267287751                 -0.0   \n",
       "13        month_3.0   -23.00985804619239   40.220468628239814   \n",
       "14        month_4.0    71.45743490019231   208.63177313325096   \n",
       "15        month_5.0    47.91033771830972                  0.0   \n",
       "16        month_6.0   -40.65841108902188  -402.69284311000814   \n",
       "17        month_7.0   -126.4923107815563    -753.783096982615   \n",
       "18        month_8.0   1.6273831785744657  -193.05295512099838   \n",
       "19        month_9.0   214.07402106448737    620.6678530499136   \n",
       "20       season_1.0    -334.466387576266    -753.074227005702   \n",
       "21       season_2.0    85.69731487079278    66.08814637433419   \n",
       "22       season_4.0   221.74064082352544     356.045957258328   \n",
       "23        temp_norm   468.71723781348066    680.2124587130345   \n",
       "24      weather_1.0   172.46275257419288    740.3489505883251   \n",
       "25      weather_2.0   -7.096763515352191    727.7028379089465   \n",
       "26   windspeed_norm  -215.92299481288208  -248.16085928013868   \n",
       "27       workingday   46.061022832395146                  0.0   \n",
       "\n",
       "                 Normal  \n",
       "0     312.4340718888971  \n",
       "1    -465.1450099570902  \n",
       "2    -256.6500506662668  \n",
       "3   -328.18450689571296  \n",
       "4     37.61277259517351  \n",
       "5    -71.64254439858638  \n",
       "6    -21.83167487926312  \n",
       "7     -616.602710298544  \n",
       "8     -548.492949058217  \n",
       "9    118.83581871663205  \n",
       "10    605.0867223839238  \n",
       "11   231.51746393832212  \n",
       "12   207.77591139109884  \n",
       "13   358.01671712429095  \n",
       "14    452.1849051340183  \n",
       "15    53.02331873120352  \n",
       "16   -673.4270797861233  \n",
       "17  -1161.1511875434737  \n",
       "18   -657.6396711861676  \n",
       "19    523.9803848202396  \n",
       "20  -1032.8815748448596  \n",
       "21   -134.0525353543187  \n",
       "22   193.30496780729308  \n",
       "23    925.7338498566323  \n",
       "24    1581.978283607945  \n",
       "25   1565.4116995877046  \n",
       "26  -255.12258899234195  \n",
       "27  -24.093293946092842  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(Xtrain, ytrain)\n",
    "\n",
    "predictors = np.array(predictors)\n",
    "coefficients = np.transpose([predictors, ridge.coef_, lasso.coef_, linreg.coef_])\n",
    "coefficients_df = pd.DataFrame(coefficients, columns=['predictors', 'Ridge', 'Lasso', 'Normal'])\n",
    "coefficients_df.head(len(coefficients_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next analyze the performance of the two shrinkage methods for different training sample sizes:\n",
    "- Generate random samples of sizes 100, 150, ..., 400 from the training set. You may use the following code to draw a random sample of a specified size from the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rmitchell/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 1.1102230246251565e-16, 0.0, 1.1102230246251565e-16]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116403748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#--------  sample\n",
    "# A function to select a random sample of size k from the training set\n",
    "# Input: \n",
    "#      x (n x d array of predictors in training data)\n",
    "#      y (n x 1 array of response variable vals in training data)\n",
    "#      k (size of sample) \n",
    "# Return: \n",
    "#      chosen sample of predictors and responses\n",
    "\n",
    "def get_samples(x, y, k):\n",
    "    n = x.shape[0] # No. of training points\n",
    "    \n",
    "    # Choose random indices of size 'k'\n",
    "    subset_ind = np.random.choice(np.arange(n), k)\n",
    "    \n",
    "    # Get predictors and reponses with the indices\n",
    "    x_subset = x[subset_ind, :]\n",
    "    y_subset = y[subset_ind]\n",
    "    return (x_subset, y_subset)\n",
    "\n",
    "sample_sizes = [100, 150, 200, 250, 300, 350, 400]\n",
    "r2s = {'ridge':{}, 'lasso':{}}\n",
    "\n",
    "for sample_size in sample_sizes:\n",
    "    X_sample, y_sample = get_samples(Xtrain.values, ytrain.values, sample_size)\n",
    "\n",
    "    r2s['ridge'][sample_size] = {'test': {'samples': [], 'std':None, 'mean':None}, 'train': {'samples': [], 'std':None, 'mean':None}}\n",
    "    r2s['lasso'][sample_size] = {'test': {'samples': [], 'std':None, 'mean':None}, 'train': {'samples': [], 'std':None, 'mean':None}}\n",
    "    \n",
    "    for i in range(10):\n",
    "        ridge = Ridge()\n",
    "        ridge.fit(X_sample, y_sample)\n",
    "        ridge_train_predictions = ridge.predict(X_sample)\n",
    "        r2s['ridge'][sample_size]['train']['samples'].append(r2_score(y_sample, ridge_train_predictions))\n",
    "        \n",
    "        ridge_test_predictions = ridge.predict(Xtest)\n",
    "        r2s['ridge'][sample_size]['test']['samples'].append(r2_score(ytest, ridge_test_predictions))\n",
    "        \n",
    "        lasso = Lasso()\n",
    "        lasso.fit(X_sample, y_sample)\n",
    "        lasso_train_predictions = lasso.predict(X_sample)\n",
    "        r2s['lasso'][sample_size]['train']['samples'].append(r2_score(y_sample, lasso_train_predictions))\n",
    "        \n",
    "        lasso_test_predictions = lasso.predict(Xtest)\n",
    "        r2s['lasso'][sample_size]['test']['samples'].append(r2_score(ytest, lasso_test_predictions))\n",
    "        \n",
    "        \n",
    "    r2s['ridge'][sample_size]['train']['mean'] = np.mean(r2s['ridge'][sample_size]['train']['samples'])\n",
    "    r2s['ridge'][sample_size]['train']['std'] = np.std(r2s['ridge'][sample_size]['train']['samples'])\n",
    "    \n",
    "    r2s['lasso'][sample_size]['train']['mean'] = np.mean(r2s['lasso'][sample_size]['train']['samples'])\n",
    "    r2s['lasso'][sample_size]['train']['std'] = np.std(r2s['lasso'][sample_size]['train']['samples'])\n",
    "    \n",
    "    r2s['ridge'][sample_size]['test']['mean'] = np.mean(r2s['ridge'][sample_size]['test']['samples'])\n",
    "    r2s['ridge'][sample_size]['test']['std'] = np.std(r2s['ridge'][sample_size]['test']['samples'])\n",
    "    \n",
    "    r2s['lasso'][sample_size]['test']['mean'] = np.mean(r2s['lasso'][sample_size]['test']['samples'])\n",
    "    r2s['lasso'][sample_size]['test']['std'] = np.std(r2s['lasso'][sample_size]['test']['samples'])\n",
    "\n",
    "\n",
    "print([r2s['lasso'][s]['train']['std'] for s in sample_sizes])\n",
    "plt.figure()\n",
    "plt.errorbar(sample_sizes, [r2s['lasso'][s]['train']['mean'] for s in sample_sizes], yerr=[r2s['lasso'][s]['train']['std'] for s in sample_sizes])\n",
    "plt.errorbar(sample_sizes, [r2s['ridge'][s]['train']['mean'] for s in sample_sizes], yerr=[r2s['ridge'][s]['train']['std'] for s in sample_sizes])\n",
    "\n",
    "\n",
    "plt.errorbar(sample_sizes, [r2s['lasso'][s]['test']['mean'] for s in sample_sizes], yerr=[r2s['lasso'][s]['test']['std'] for s in sample_sizes])\n",
    "plt.errorbar(sample_sizes, [r2s['ridge'][s]['test']['mean'] for s in sample_sizes], yerr=[r2s['ridge'][s]['test']['std'] for s in sample_sizes])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fit linear, Ridge and Lasso regression models to each of the generated sample. In each case, compute the $R^2$ score for the model on the training sample on which it was fitted, and on the test set.\n",
    "- Repeat the above experiment for 10 random trials/splits, and compute the average train and test $R^2$ across the trials for each training sample size. Also, compute the standard deviation (SD) in each case.\n",
    "- Make a plot of the mean training $R^2$ scores for the linear, Ridge and Lasso regression methods as a function of the training sample size. Also, show a confidence interval for the mean scores extending from **mean - SD** to **mean + SD**. Make a similar plot for the test $R^2$ scores.\n",
    "\n",
    "How do the training and test $R^2$ scores compare for the three methods? Give an explanation for your observations. How do the confidence intervals for the estimated $R^2$ change with training sample size? Based on the plots, which of the three methods would you recommend when one needs to fit a regression model using a small training sample?\n",
    "\n",
    "*Hint:* You may use `sklearn`'s `RidgeCV` and `LassoCV` classes to implement Ridge and Lasso regression. These classes automatically perform cross-validation to tune the parameter $\\lambda$ from a given range of values. You may use the `plt.errorbar` function to plot confidence bars for the average $R^2$ scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your answers here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (g): Polynomial & Interaction Terms\n",
    "\n",
    "Moving beyond linear models, we will now try to improve the performance of the regression model in Part (b) from HW 3 by including higher-order polynomial and interaction terms. \n",
    "\n",
    "- For each continuous predictor $X_j$, include additional polynomial terms $X^2_j$, $X^3_j$, and $X^4_j$, and fit a multiple regression model to the expanded training set. How does the $R^2$ of this model on the test set compare with that of the linear model fitted in Part (b) from HW 3? Using a t-test, find out which of estimated coefficients for the polynomial terms are statistically significant at a significance level of 5%. \n",
    "\n",
    "- Fit a multiple linear regression model with additional interaction terms $\\mathbb{I}_{month = 12} \\times temp$ and $\\mathbb{I}_{workingday = 1} \\times \\mathbb{I}_{weathersit = 1}$ and report the test $R^2$ for the fitted model. How does this compare with the $R^2$ obtained using linear model in Part (b) from HW 3? Are the estimated coefficients for the interaction terms statistically significant at a significance level of 5%?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predictor</th>\n",
       "      <th>coef</th>\n",
       "      <th>std err</th>\n",
       "      <th>t</th>\n",
       "      <th>P&gt;|t|</th>\n",
       "      <th>[0.025</th>\n",
       "      <th>0.975]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>const</td>\n",
       "      <td>3651.0945</td>\n",
       "      <td>1061.216</td>\n",
       "      <td>3.440</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1564.133</td>\n",
       "      <td>5738.056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>temp_norm_2</td>\n",
       "      <td>-2759.4370</td>\n",
       "      <td>1002.647</td>\n",
       "      <td>-2.752</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-4731.218</td>\n",
       "      <td>-787.656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>weather_1.0</td>\n",
       "      <td>1984.3455</td>\n",
       "      <td>736.827</td>\n",
       "      <td>2.693</td>\n",
       "      <td>0.007</td>\n",
       "      <td>535.319</td>\n",
       "      <td>3433.372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>atemp_norm_4</td>\n",
       "      <td>-376.7993</td>\n",
       "      <td>147.915</td>\n",
       "      <td>-2.547</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-667.686</td>\n",
       "      <td>-85.913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>weather_2.0</td>\n",
       "      <td>1774.8474</td>\n",
       "      <td>694.078</td>\n",
       "      <td>2.557</td>\n",
       "      <td>0.011</td>\n",
       "      <td>409.890</td>\n",
       "      <td>3139.805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>atemp_norm_2</td>\n",
       "      <td>2198.9751</td>\n",
       "      <td>903.790</td>\n",
       "      <td>2.433</td>\n",
       "      <td>0.015</td>\n",
       "      <td>421.604</td>\n",
       "      <td>3976.346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>day_of_week_5.0</td>\n",
       "      <td>521.6191</td>\n",
       "      <td>230.018</td>\n",
       "      <td>2.268</td>\n",
       "      <td>0.024</td>\n",
       "      <td>69.271</td>\n",
       "      <td>973.967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>day_of_week_4.0</td>\n",
       "      <td>475.1143</td>\n",
       "      <td>223.309</td>\n",
       "      <td>2.128</td>\n",
       "      <td>0.034</td>\n",
       "      <td>35.960</td>\n",
       "      <td>914.269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>humidity_norm</td>\n",
       "      <td>-454.5818</td>\n",
       "      <td>215.594</td>\n",
       "      <td>-2.109</td>\n",
       "      <td>0.036</td>\n",
       "      <td>-878.564</td>\n",
       "      <td>-30.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>month_4.0</td>\n",
       "      <td>-1376.1298</td>\n",
       "      <td>715.532</td>\n",
       "      <td>-1.923</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-2783.277</td>\n",
       "      <td>31.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>temp_norm_4</td>\n",
       "      <td>361.9131</td>\n",
       "      <td>191.667</td>\n",
       "      <td>1.888</td>\n",
       "      <td>0.060</td>\n",
       "      <td>-15.014</td>\n",
       "      <td>738.841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>windspeed_norm_2</td>\n",
       "      <td>157.6822</td>\n",
       "      <td>109.340</td>\n",
       "      <td>1.442</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-57.344</td>\n",
       "      <td>372.708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>day_of_week_3.0</td>\n",
       "      <td>-307.2298</td>\n",
       "      <td>222.657</td>\n",
       "      <td>-1.380</td>\n",
       "      <td>0.168</td>\n",
       "      <td>-745.101</td>\n",
       "      <td>130.641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>holiday</td>\n",
       "      <td>664.2758</td>\n",
       "      <td>495.431</td>\n",
       "      <td>1.341</td>\n",
       "      <td>0.181</td>\n",
       "      <td>-310.026</td>\n",
       "      <td>1638.578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>month_11.0</td>\n",
       "      <td>-584.2320</td>\n",
       "      <td>469.408</td>\n",
       "      <td>-1.245</td>\n",
       "      <td>0.214</td>\n",
       "      <td>-1507.359</td>\n",
       "      <td>338.895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>season_4.0</td>\n",
       "      <td>646.5739</td>\n",
       "      <td>629.044</td>\n",
       "      <td>1.028</td>\n",
       "      <td>0.305</td>\n",
       "      <td>-590.488</td>\n",
       "      <td>1883.636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>windspeed_norm</td>\n",
       "      <td>-204.0507</td>\n",
       "      <td>203.277</td>\n",
       "      <td>-1.004</td>\n",
       "      <td>0.316</td>\n",
       "      <td>-603.810</td>\n",
       "      <td>195.709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>humidity_norm_2</td>\n",
       "      <td>-101.8628</td>\n",
       "      <td>123.931</td>\n",
       "      <td>-0.822</td>\n",
       "      <td>0.412</td>\n",
       "      <td>-345.582</td>\n",
       "      <td>141.856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>atemp_norm_3</td>\n",
       "      <td>-210.8119</td>\n",
       "      <td>257.823</td>\n",
       "      <td>-0.818</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-717.841</td>\n",
       "      <td>296.217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>workingday</td>\n",
       "      <td>-161.4396</td>\n",
       "      <td>201.612</td>\n",
       "      <td>-0.801</td>\n",
       "      <td>0.424</td>\n",
       "      <td>-557.924</td>\n",
       "      <td>235.045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>month_10.0</td>\n",
       "      <td>-405.5347</td>\n",
       "      <td>519.172</td>\n",
       "      <td>-0.781</td>\n",
       "      <td>0.435</td>\n",
       "      <td>-1426.527</td>\n",
       "      <td>615.457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>season_1.0</td>\n",
       "      <td>-501.2072</td>\n",
       "      <td>644.233</td>\n",
       "      <td>-0.778</td>\n",
       "      <td>0.437</td>\n",
       "      <td>-1768.140</td>\n",
       "      <td>765.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>day_of_week_0.0</td>\n",
       "      <td>-240.2722</td>\n",
       "      <td>324.616</td>\n",
       "      <td>-0.740</td>\n",
       "      <td>0.460</td>\n",
       "      <td>-878.653</td>\n",
       "      <td>398.109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>temp_norm</td>\n",
       "      <td>1284.5216</td>\n",
       "      <td>1817.555</td>\n",
       "      <td>0.707</td>\n",
       "      <td>0.480</td>\n",
       "      <td>-2289.837</td>\n",
       "      <td>4858.880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>month_2.0</td>\n",
       "      <td>-338.0900</td>\n",
       "      <td>528.390</td>\n",
       "      <td>-0.640</td>\n",
       "      <td>0.523</td>\n",
       "      <td>-1377.209</td>\n",
       "      <td>701.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>month_7.0</td>\n",
       "      <td>524.4671</td>\n",
       "      <td>903.709</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.562</td>\n",
       "      <td>-1252.745</td>\n",
       "      <td>2301.679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>month_5.0</td>\n",
       "      <td>-448.9093</td>\n",
       "      <td>782.753</td>\n",
       "      <td>-0.574</td>\n",
       "      <td>0.567</td>\n",
       "      <td>-1988.252</td>\n",
       "      <td>1090.433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>windspeed_norm_4</td>\n",
       "      <td>-12.1547</td>\n",
       "      <td>23.476</td>\n",
       "      <td>-0.518</td>\n",
       "      <td>0.605</td>\n",
       "      <td>-58.323</td>\n",
       "      <td>34.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>humidity_norm_3</td>\n",
       "      <td>39.5809</td>\n",
       "      <td>78.155</td>\n",
       "      <td>0.506</td>\n",
       "      <td>0.613</td>\n",
       "      <td>-114.117</td>\n",
       "      <td>193.279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>month_3.0</td>\n",
       "      <td>-254.9052</td>\n",
       "      <td>535.600</td>\n",
       "      <td>-0.476</td>\n",
       "      <td>0.634</td>\n",
       "      <td>-1308.203</td>\n",
       "      <td>798.392</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           predictor       coef   std err      t  P>|t|    [0.025    0.975]\n",
       "0              const  3651.0945  1061.216  3.440  0.001  1564.133  5738.056\n",
       "29       temp_norm_2 -2759.4370  1002.647 -2.752  0.006 -4731.218  -787.656\n",
       "25       weather_1.0  1984.3455   736.827  2.693  0.007   535.319  3433.372\n",
       "34      atemp_norm_4  -376.7993   147.915 -2.547  0.011  -667.686   -85.913\n",
       "26       weather_2.0  1774.8474   694.078  2.557  0.011   409.890  3139.805\n",
       "32      atemp_norm_2  2198.9751   903.790  2.433  0.015   421.604  3976.346\n",
       "7    day_of_week_5.0   521.6191   230.018  2.268  0.024    69.271   973.967\n",
       "6    day_of_week_4.0   475.1143   223.309  2.128  0.034    35.960   914.269\n",
       "9      humidity_norm  -454.5818   215.594 -2.109  0.036  -878.564   -30.600\n",
       "15         month_4.0 -1376.1298   715.532 -1.923  0.055 -2783.277    31.017\n",
       "31       temp_norm_4   361.9131   191.667  1.888  0.060   -15.014   738.841\n",
       "38  windspeed_norm_2   157.6822   109.340  1.442  0.150   -57.344   372.708\n",
       "5    day_of_week_3.0  -307.2298   222.657 -1.380  0.168  -745.101   130.641\n",
       "8            holiday   664.2758   495.431  1.341  0.181  -310.026  1638.578\n",
       "12        month_11.0  -584.2320   469.408 -1.245  0.214 -1507.359   338.895\n",
       "23        season_4.0   646.5739   629.044  1.028  0.305  -590.488  1883.636\n",
       "27    windspeed_norm  -204.0507   203.277 -1.004  0.316  -603.810   195.709\n",
       "35   humidity_norm_2  -101.8628   123.931 -0.822  0.412  -345.582   141.856\n",
       "33      atemp_norm_3  -210.8119   257.823 -0.818  0.414  -717.841   296.217\n",
       "28        workingday  -161.4396   201.612 -0.801  0.424  -557.924   235.045\n",
       "11        month_10.0  -405.5347   519.172 -0.781  0.435 -1426.527   615.457\n",
       "21        season_1.0  -501.2072   644.233 -0.778  0.437 -1768.140   765.725\n",
       "2    day_of_week_0.0  -240.2722   324.616 -0.740  0.460  -878.653   398.109\n",
       "24         temp_norm  1284.5216  1817.555  0.707  0.480 -2289.837  4858.880\n",
       "13         month_2.0  -338.0900   528.390 -0.640  0.523 -1377.209   701.029\n",
       "18         month_7.0   524.4671   903.709  0.580  0.562 -1252.745  2301.679\n",
       "16         month_5.0  -448.9093   782.753 -0.574  0.567 -1988.252  1090.433\n",
       "40  windspeed_norm_4   -12.1547    23.476 -0.518  0.605   -58.323    34.014\n",
       "36   humidity_norm_3    39.5809    78.155  0.506  0.613  -114.117   193.279\n",
       "14         month_3.0  -254.9052   535.600 -0.476  0.634 -1308.203   798.392"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_summary_df(X, y):\n",
    "    ols = sm.OLS(y, X).fit()\n",
    "    \n",
    "    summary_data = ols.summary().tables[1].data\n",
    "    header = summary_data.pop(0)\n",
    "    header[0] = 'predictor'\n",
    "\n",
    "    # We get the data as strings -- convert here. \n",
    "    for i in range(len(summary_data)):\n",
    "        summary_data[i][1] = float(summary_data[i][1])\n",
    "        summary_data[i][2] = float(summary_data[i][2])\n",
    "        summary_data[i][3] = float(summary_data[i][3])\n",
    "        summary_data[i][4] = float(summary_data[i][4])\n",
    "        summary_data[i][5] = float(summary_data[i][5])\n",
    "        summary_data[i][6] = float(summary_data[i][6])\n",
    "        \n",
    "    summary_df = pd.DataFrame(summary_data, columns=header)\n",
    "    return summary_df\n",
    "\n",
    "# Predictors\n",
    "numeric_cols = ['temp_norm', 'atemp_norm', 'humidity_norm', 'windspeed_norm']\n",
    "Xtrain_poly = Xtrain.copy()\n",
    "Xtest_poly = Xtest.copy()\n",
    "\n",
    "for numeric_col in numeric_cols:\n",
    "    Xtrain_poly[numeric_col+'_2'] = Xtrain[numeric_col]**2\n",
    "    Xtrain_poly[numeric_col+'_3'] = Xtrain[numeric_col]**3\n",
    "    Xtrain_poly[numeric_col+'_4'] = Xtrain[numeric_col]**4\n",
    "    \n",
    "    Xtest_poly[numeric_col+'_2'] = Xtest[numeric_col]**2\n",
    "    Xtest_poly[numeric_col+'_3'] = Xtest[numeric_col]**3\n",
    "    Xtest_poly[numeric_col+'_4'] = Xtest[numeric_col]**4\n",
    "Xtrain_poly.head()\n",
    "\n",
    "Xtest_intercept = sm.add_constant(Xtest_poly)\n",
    "summary_df = get_summary_df(Xtest_intercept, ytest)\n",
    "summary_df.sort_values('P>|t|', inplace=True)\n",
    "summary_df.head(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predictor</th>\n",
       "      <th>coef</th>\n",
       "      <th>std err</th>\n",
       "      <th>t</th>\n",
       "      <th>P&gt;|t|</th>\n",
       "      <th>[0.025</th>\n",
       "      <th>0.975]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>const</td>\n",
       "      <td>5323.2503</td>\n",
       "      <td>1189.500</td>\n",
       "      <td>4.475</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2984.009</td>\n",
       "      <td>7662.491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>temp_month_7.0</td>\n",
       "      <td>-5212.4875</td>\n",
       "      <td>1361.987</td>\n",
       "      <td>-3.827</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-7890.938</td>\n",
       "      <td>-2534.037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>temp_month_6.0</td>\n",
       "      <td>-6283.1998</td>\n",
       "      <td>1163.960</td>\n",
       "      <td>-5.398</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-8572.215</td>\n",
       "      <td>-3994.185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>weather_2.0</td>\n",
       "      <td>2840.8660</td>\n",
       "      <td>638.731</td>\n",
       "      <td>4.448</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1584.752</td>\n",
       "      <td>4096.980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>weather_1.0</td>\n",
       "      <td>2705.0879</td>\n",
       "      <td>593.435</td>\n",
       "      <td>4.558</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1538.054</td>\n",
       "      <td>3872.122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>temp_norm</td>\n",
       "      <td>5816.2396</td>\n",
       "      <td>1405.182</td>\n",
       "      <td>4.139</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3052.843</td>\n",
       "      <td>8579.636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>temp_month_11.0</td>\n",
       "      <td>-4170.4130</td>\n",
       "      <td>1183.890</td>\n",
       "      <td>-3.523</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-6498.623</td>\n",
       "      <td>-1842.203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>month_11.0</td>\n",
       "      <td>-4274.6630</td>\n",
       "      <td>1159.220</td>\n",
       "      <td>-3.688</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-6554.356</td>\n",
       "      <td>-1994.970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>month_4.0</td>\n",
       "      <td>-3592.7561</td>\n",
       "      <td>1113.269</td>\n",
       "      <td>-3.227</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-5782.084</td>\n",
       "      <td>-1403.428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>month_10.0</td>\n",
       "      <td>-2669.7716</td>\n",
       "      <td>915.400</td>\n",
       "      <td>-2.917</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-4469.974</td>\n",
       "      <td>-869.569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>temp_month_9.0</td>\n",
       "      <td>-3169.4930</td>\n",
       "      <td>1122.236</td>\n",
       "      <td>-2.824</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-5376.455</td>\n",
       "      <td>-962.531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>temp_month_5.0</td>\n",
       "      <td>-3238.5064</td>\n",
       "      <td>1166.612</td>\n",
       "      <td>-2.776</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-5532.738</td>\n",
       "      <td>-944.275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>month_3.0</td>\n",
       "      <td>-2787.7265</td>\n",
       "      <td>1053.555</td>\n",
       "      <td>-2.646</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-4859.621</td>\n",
       "      <td>-715.832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>temp_month_3.0</td>\n",
       "      <td>-2342.6245</td>\n",
       "      <td>905.140</td>\n",
       "      <td>-2.588</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-4122.650</td>\n",
       "      <td>-562.599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>month_9.0</td>\n",
       "      <td>-2889.7974</td>\n",
       "      <td>1109.950</td>\n",
       "      <td>-2.604</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-5072.597</td>\n",
       "      <td>-706.997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>temp_month_8.0</td>\n",
       "      <td>-3846.2080</td>\n",
       "      <td>1497.563</td>\n",
       "      <td>-2.568</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-6791.279</td>\n",
       "      <td>-901.137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>temp_month_10.0</td>\n",
       "      <td>-2613.9373</td>\n",
       "      <td>1055.756</td>\n",
       "      <td>-2.476</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-4690.162</td>\n",
       "      <td>-537.713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>month_2.0</td>\n",
       "      <td>-3078.6300</td>\n",
       "      <td>1265.010</td>\n",
       "      <td>-2.434</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-5566.368</td>\n",
       "      <td>-590.892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>windspeed_norm</td>\n",
       "      <td>-252.2981</td>\n",
       "      <td>104.102</td>\n",
       "      <td>-2.424</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-457.022</td>\n",
       "      <td>-47.574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>month_5.0</td>\n",
       "      <td>-2810.7932</td>\n",
       "      <td>1162.245</td>\n",
       "      <td>-2.418</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-5096.436</td>\n",
       "      <td>-525.150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>day_of_week_5.0</td>\n",
       "      <td>544.0854</td>\n",
       "      <td>229.540</td>\n",
       "      <td>2.370</td>\n",
       "      <td>0.018</td>\n",
       "      <td>92.677</td>\n",
       "      <td>995.493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>day_of_week_4.0</td>\n",
       "      <td>522.1777</td>\n",
       "      <td>222.696</td>\n",
       "      <td>2.345</td>\n",
       "      <td>0.020</td>\n",
       "      <td>84.230</td>\n",
       "      <td>960.126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>temp_month_4.0</td>\n",
       "      <td>-2363.6563</td>\n",
       "      <td>1023.367</td>\n",
       "      <td>-2.310</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-4376.184</td>\n",
       "      <td>-351.129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>temp_month_2.0</td>\n",
       "      <td>-2313.1626</td>\n",
       "      <td>996.692</td>\n",
       "      <td>-2.321</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-4273.234</td>\n",
       "      <td>-353.092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>atemp_norm</td>\n",
       "      <td>-2179.3053</td>\n",
       "      <td>1057.687</td>\n",
       "      <td>-2.060</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-4259.326</td>\n",
       "      <td>-99.285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>humidity_norm</td>\n",
       "      <td>-250.1867</td>\n",
       "      <td>122.230</td>\n",
       "      <td>-2.047</td>\n",
       "      <td>0.041</td>\n",
       "      <td>-490.562</td>\n",
       "      <td>-9.811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>temp_month_1.0</td>\n",
       "      <td>-1731.0895</td>\n",
       "      <td>1013.778</td>\n",
       "      <td>-1.708</td>\n",
       "      <td>0.089</td>\n",
       "      <td>-3724.761</td>\n",
       "      <td>262.582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>working_weather_11.0</td>\n",
       "      <td>-681.1341</td>\n",
       "      <td>399.019</td>\n",
       "      <td>-1.707</td>\n",
       "      <td>0.089</td>\n",
       "      <td>-1465.836</td>\n",
       "      <td>103.568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>month_1.0</td>\n",
       "      <td>-1817.0467</td>\n",
       "      <td>1390.417</td>\n",
       "      <td>-1.307</td>\n",
       "      <td>0.192</td>\n",
       "      <td>-4551.407</td>\n",
       "      <td>917.314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>month_8.0</td>\n",
       "      <td>-2334.5197</td>\n",
       "      <td>1810.988</td>\n",
       "      <td>-1.289</td>\n",
       "      <td>0.198</td>\n",
       "      <td>-5895.964</td>\n",
       "      <td>1226.925</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               predictor       coef   std err      t  P>|t|    [0.025  \\\n",
       "0                  const  5323.2503  1189.500  4.475  0.000  2984.009   \n",
       "35        temp_month_7.0 -5212.4875  1361.987 -3.827  0.000 -7890.938   \n",
       "34        temp_month_6.0 -6283.1998  1163.960 -5.398  0.000 -8572.215   \n",
       "26           weather_2.0  2840.8660   638.731  4.448  0.000  1584.752   \n",
       "25           weather_1.0  2705.0879   593.435  4.558  0.000  1538.054   \n",
       "24             temp_norm  5816.2396  1405.182  4.139  0.000  3052.843   \n",
       "39       temp_month_11.0 -4170.4130  1183.890 -3.523  0.000 -6498.623   \n",
       "12            month_11.0 -4274.6630  1159.220 -3.688  0.000 -6554.356   \n",
       "15             month_4.0 -3592.7561  1113.269 -3.227  0.001 -5782.084   \n",
       "11            month_10.0 -2669.7716   915.400 -2.917  0.004 -4469.974   \n",
       "37        temp_month_9.0 -3169.4930  1122.236 -2.824  0.005 -5376.455   \n",
       "33        temp_month_5.0 -3238.5064  1166.612 -2.776  0.006 -5532.738   \n",
       "14             month_3.0 -2787.7265  1053.555 -2.646  0.009 -4859.621   \n",
       "31        temp_month_3.0 -2342.6245   905.140 -2.588  0.010 -4122.650   \n",
       "20             month_9.0 -2889.7974  1109.950 -2.604  0.010 -5072.597   \n",
       "36        temp_month_8.0 -3846.2080  1497.563 -2.568  0.011 -6791.279   \n",
       "38       temp_month_10.0 -2613.9373  1055.756 -2.476  0.014 -4690.162   \n",
       "13             month_2.0 -3078.6300  1265.010 -2.434  0.015 -5566.368   \n",
       "27        windspeed_norm  -252.2981   104.102 -2.424  0.016  -457.022   \n",
       "16             month_5.0 -2810.7932  1162.245 -2.418  0.016 -5096.436   \n",
       "7        day_of_week_5.0   544.0854   229.540  2.370  0.018    92.677   \n",
       "6        day_of_week_4.0   522.1777   222.696  2.345  0.020    84.230   \n",
       "32        temp_month_4.0 -2363.6563  1023.367 -2.310  0.021 -4376.184   \n",
       "30        temp_month_2.0 -2313.1626   996.692 -2.321  0.021 -4273.234   \n",
       "1             atemp_norm -2179.3053  1057.687 -2.060  0.040 -4259.326   \n",
       "9          humidity_norm  -250.1867   122.230 -2.047  0.041  -490.562   \n",
       "29        temp_month_1.0 -1731.0895  1013.778 -1.708  0.089 -3724.761   \n",
       "40  working_weather_11.0  -681.1341   399.019 -1.707  0.089 -1465.836   \n",
       "10             month_1.0 -1817.0467  1390.417 -1.307  0.192 -4551.407   \n",
       "19             month_8.0 -2334.5197  1810.988 -1.289  0.198 -5895.964   \n",
       "\n",
       "      0.975]  \n",
       "0   7662.491  \n",
       "35 -2534.037  \n",
       "34 -3994.185  \n",
       "26  4096.980  \n",
       "25  3872.122  \n",
       "24  8579.636  \n",
       "39 -1842.203  \n",
       "12 -1994.970  \n",
       "15 -1403.428  \n",
       "11  -869.569  \n",
       "37  -962.531  \n",
       "33  -944.275  \n",
       "14  -715.832  \n",
       "31  -562.599  \n",
       "20  -706.997  \n",
       "36  -901.137  \n",
       "38  -537.713  \n",
       "13  -590.892  \n",
       "27   -47.574  \n",
       "16  -525.150  \n",
       "7    995.493  \n",
       "6    960.126  \n",
       "32  -351.129  \n",
       "30  -353.092  \n",
       "1    -99.285  \n",
       "9     -9.811  \n",
       "29   262.582  \n",
       "40   103.568  \n",
       "10   917.314  \n",
       "19  1226.925  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain_interaction = Xtrain.copy()\n",
    "Xtest_interaction = Xtest.copy()\n",
    "\n",
    "months = ['1.0', '2.0', '3.0', '4.0', '5.0', '6.0', '7.0', '8.0', '9.0', '10.0', '11.0']\n",
    "weathers = ['1.0', '2.0']\n",
    "\n",
    "for month in months:\n",
    "    Xtrain_interaction['temp_month_'+month] = Xtrain['month_'+month]* Xtrain['temp_norm']\n",
    "    Xtest_interaction['temp_month_'+month] = Xtest['month_'+month]* Xtest['temp_norm']\n",
    "\n",
    "for weather in weathers:\n",
    "    Xtrain_interaction['working_weather_'+month] = Xtrain['weather_'+weather] * Xtrain['workingday']\n",
    "    Xtest_interaction['working_weather_'+month] = Xtest['weather_'+weather] * Xtest['workingday']\n",
    "    \n",
    "Xtrain_interaction.head()\n",
    "Xtest_intercept = sm.add_constant(Xtest_interaction)\n",
    "summary_df = get_summary_df(Xtest_intercept, ytest)\n",
    "summary_df.sort_values('P>|t|', inplace=True)\n",
    "summary_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "your answers here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (h): PCA to deal with high dimensionality\n",
    "\n",
    "We would like to fit a model to include all main effects, polynomial terms up to the $4^{th}$ order, and all interactions between all possible predictors and polynomial terms (not including the interactions between $X^1_j$, $X^2_j$, $X^3_j$, and $X^4_j$ as they would just create higher order polynomial terms).  \n",
    "\n",
    "- Create an expanded training set including all the desired terms mentioned above.  What are the dimensions of this 'design matrix' of all the predictor variables?   What are the issues with attempting to fit a regression model using all of these predictors?\n",
    "\n",
    "- Instead of using the usual approaches for model selection, let's instead use principal components analysis (PCA) to fit the model.  First, create the principal component vectors in python (consider: should you normalize first?).  Then fit 5 different regression models: (1) using just the first PCA vector, (2) using the first two PCA vectors, (3) using the first three PCA vectors, etc...  Briefly summarize how these models compare in the training set.\n",
    "\n",
    "- Use the test set to decide which of the 5 models above is best to predict out of sample.  How does this model compare to the previous models you've fit?  What are the interpretations of this model's coefficients?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is:\n",
      "(331, 1000)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>atemp_norm</th>\n",
       "      <th>day_of_week_0.0</th>\n",
       "      <th>day_of_week_1.0</th>\n",
       "      <th>day_of_week_2.0</th>\n",
       "      <th>day_of_week_3.0</th>\n",
       "      <th>day_of_week_4.0</th>\n",
       "      <th>day_of_week_5.0</th>\n",
       "      <th>holiday</th>\n",
       "      <th>humidity_norm</th>\n",
       "      <th>month_1.0</th>\n",
       "      <th>...</th>\n",
       "      <th>windspeed_norm_4Xworkingday</th>\n",
       "      <th>windspeed_norm_4Xmonth_8.0</th>\n",
       "      <th>windspeed_norm_4Xweather_2.0</th>\n",
       "      <th>windspeed_norm_4Xmonth_5.0</th>\n",
       "      <th>windspeed_norm_4Xmonth_3.0</th>\n",
       "      <th>windspeed_norm_4Xday_of_week_4.0</th>\n",
       "      <th>windspeed_norm_4Xday_of_week_5.0</th>\n",
       "      <th>windspeed_norm_4Xseason_4.0</th>\n",
       "      <th>windspeed_norm_4Xmonth_6.0</th>\n",
       "      <th>windspeed_norm_4Xday_of_week_0.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.310000e+02</td>\n",
       "      <td>331.000000</td>\n",
       "      <td>331.000000</td>\n",
       "      <td>331.000000</td>\n",
       "      <td>331.000000</td>\n",
       "      <td>331.000000</td>\n",
       "      <td>331.000000</td>\n",
       "      <td>331.000000</td>\n",
       "      <td>3.310000e+02</td>\n",
       "      <td>331.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.310000e+02</td>\n",
       "      <td>3.310000e+02</td>\n",
       "      <td>3.310000e+02</td>\n",
       "      <td>3.310000e+02</td>\n",
       "      <td>3.310000e+02</td>\n",
       "      <td>3.310000e+02</td>\n",
       "      <td>3.310000e+02</td>\n",
       "      <td>3.310000e+02</td>\n",
       "      <td>3.310000e+02</td>\n",
       "      <td>3.310000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-5.433720e-17</td>\n",
       "      <td>0.160121</td>\n",
       "      <td>0.175227</td>\n",
       "      <td>0.135952</td>\n",
       "      <td>0.123867</td>\n",
       "      <td>0.123867</td>\n",
       "      <td>0.145015</td>\n",
       "      <td>0.033233</td>\n",
       "      <td>2.012489e-17</td>\n",
       "      <td>0.078550</td>\n",
       "      <td>...</td>\n",
       "      <td>6.641213e-17</td>\n",
       "      <td>1.972239e-16</td>\n",
       "      <td>3.595647e-16</td>\n",
       "      <td>-2.327779e-16</td>\n",
       "      <td>4.645495e-17</td>\n",
       "      <td>-1.133702e-16</td>\n",
       "      <td>1.931151e-16</td>\n",
       "      <td>6.557360e-17</td>\n",
       "      <td>3.765031e-17</td>\n",
       "      <td>1.304764e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.367273</td>\n",
       "      <td>0.380736</td>\n",
       "      <td>0.343256</td>\n",
       "      <td>0.329929</td>\n",
       "      <td>0.329929</td>\n",
       "      <td>0.352649</td>\n",
       "      <td>0.179515</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.269442</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.572131e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.648736e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.759446e-01</td>\n",
       "      <td>-7.771600e-02</td>\n",
       "      <td>-2.120063e-01</td>\n",
       "      <td>-1.281258e-01</td>\n",
       "      <td>-1.111391e-01</td>\n",
       "      <td>-1.352397e-01</td>\n",
       "      <td>-1.210952e-01</td>\n",
       "      <td>-2.492307e-01</td>\n",
       "      <td>-1.060354e-01</td>\n",
       "      <td>-1.285487e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-8.603176e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-7.452412e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.759446e-01</td>\n",
       "      <td>-7.771600e-02</td>\n",
       "      <td>-2.120063e-01</td>\n",
       "      <td>-1.281258e-01</td>\n",
       "      <td>-1.111391e-01</td>\n",
       "      <td>-1.352397e-01</td>\n",
       "      <td>-1.210952e-01</td>\n",
       "      <td>-2.492307e-01</td>\n",
       "      <td>-1.060354e-01</td>\n",
       "      <td>-1.285487e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.466312e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.562743e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.741981e-01</td>\n",
       "      <td>-7.771600e-02</td>\n",
       "      <td>-2.120063e-01</td>\n",
       "      <td>-1.281258e-01</td>\n",
       "      <td>-1.111391e-01</td>\n",
       "      <td>-1.352397e-01</td>\n",
       "      <td>-1.210952e-01</td>\n",
       "      <td>-2.492307e-01</td>\n",
       "      <td>-1.060354e-01</td>\n",
       "      <td>-1.285487e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.508005e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.055719e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.735898e-01</td>\n",
       "      <td>-7.771600e-02</td>\n",
       "      <td>-2.058362e-01</td>\n",
       "      <td>-1.281258e-01</td>\n",
       "      <td>-1.111391e-01</td>\n",
       "      <td>-1.352397e-01</td>\n",
       "      <td>-1.210952e-01</td>\n",
       "      <td>-2.492306e-01</td>\n",
       "      <td>-1.060354e-01</td>\n",
       "      <td>-1.285487e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.959139e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.362380e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>9.827090e+00</td>\n",
       "      <td>1.774307e+01</td>\n",
       "      <td>1.259210e+01</td>\n",
       "      <td>1.395634e+01</td>\n",
       "      <td>1.434265e+01</td>\n",
       "      <td>1.563538e+01</td>\n",
       "      <td>1.589538e+01</td>\n",
       "      <td>9.483028e+00</td>\n",
       "      <td>1.678698e+01</td>\n",
       "      <td>1.626381e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         atemp_norm  day_of_week_0.0  day_of_week_1.0  day_of_week_2.0  \\\n",
       "count  3.310000e+02       331.000000       331.000000       331.000000   \n",
       "mean  -5.433720e-17         0.160121         0.175227         0.135952   \n",
       "std    1.000000e+00         0.367273         0.380736         0.343256   \n",
       "min   -2.572131e+00         0.000000         0.000000         0.000000   \n",
       "25%   -8.603176e-01         0.000000         0.000000         0.000000   \n",
       "50%    1.466312e-01         0.000000         0.000000         0.000000   \n",
       "75%    7.508005e-01         0.000000         0.000000         0.000000   \n",
       "max    1.959139e+00         1.000000         1.000000         1.000000   \n",
       "\n",
       "       day_of_week_3.0  day_of_week_4.0  day_of_week_5.0     holiday  \\\n",
       "count       331.000000       331.000000       331.000000  331.000000   \n",
       "mean          0.123867         0.123867         0.145015    0.033233   \n",
       "std           0.329929         0.329929         0.352649    0.179515   \n",
       "min           0.000000         0.000000         0.000000    0.000000   \n",
       "25%           0.000000         0.000000         0.000000    0.000000   \n",
       "50%           0.000000         0.000000         0.000000    0.000000   \n",
       "75%           0.000000         0.000000         0.000000    0.000000   \n",
       "max           1.000000         1.000000         1.000000    1.000000   \n",
       "\n",
       "       humidity_norm   month_1.0                ...                 \\\n",
       "count   3.310000e+02  331.000000                ...                  \n",
       "mean    2.012489e-17    0.078550                ...                  \n",
       "std     1.000000e+00    0.269442                ...                  \n",
       "min    -2.648736e+00    0.000000                ...                  \n",
       "25%    -7.452412e-01    0.000000                ...                  \n",
       "50%    -6.562743e-03    0.000000                ...                  \n",
       "75%     7.055719e-01    0.000000                ...                  \n",
       "max     2.362380e+00    1.000000                ...                  \n",
       "\n",
       "       windspeed_norm_4Xworkingday  windspeed_norm_4Xmonth_8.0  \\\n",
       "count                 3.310000e+02                3.310000e+02   \n",
       "mean                  6.641213e-17                1.972239e-16   \n",
       "std                   1.000000e+00                1.000000e+00   \n",
       "min                  -2.759446e-01               -7.771600e-02   \n",
       "25%                  -2.759446e-01               -7.771600e-02   \n",
       "50%                  -2.741981e-01               -7.771600e-02   \n",
       "75%                  -1.735898e-01               -7.771600e-02   \n",
       "max                   9.827090e+00                1.774307e+01   \n",
       "\n",
       "       windspeed_norm_4Xweather_2.0  windspeed_norm_4Xmonth_5.0  \\\n",
       "count                  3.310000e+02                3.310000e+02   \n",
       "mean                   3.595647e-16               -2.327779e-16   \n",
       "std                    1.000000e+00                1.000000e+00   \n",
       "min                   -2.120063e-01               -1.281258e-01   \n",
       "25%                   -2.120063e-01               -1.281258e-01   \n",
       "50%                   -2.120063e-01               -1.281258e-01   \n",
       "75%                   -2.058362e-01               -1.281258e-01   \n",
       "max                    1.259210e+01                1.395634e+01   \n",
       "\n",
       "       windspeed_norm_4Xmonth_3.0  windspeed_norm_4Xday_of_week_4.0  \\\n",
       "count                3.310000e+02                      3.310000e+02   \n",
       "mean                 4.645495e-17                     -1.133702e-16   \n",
       "std                  1.000000e+00                      1.000000e+00   \n",
       "min                 -1.111391e-01                     -1.352397e-01   \n",
       "25%                 -1.111391e-01                     -1.352397e-01   \n",
       "50%                 -1.111391e-01                     -1.352397e-01   \n",
       "75%                 -1.111391e-01                     -1.352397e-01   \n",
       "max                  1.434265e+01                      1.563538e+01   \n",
       "\n",
       "       windspeed_norm_4Xday_of_week_5.0  windspeed_norm_4Xseason_4.0  \\\n",
       "count                      3.310000e+02                 3.310000e+02   \n",
       "mean                       1.931151e-16                 6.557360e-17   \n",
       "std                        1.000000e+00                 1.000000e+00   \n",
       "min                       -1.210952e-01                -2.492307e-01   \n",
       "25%                       -1.210952e-01                -2.492307e-01   \n",
       "50%                       -1.210952e-01                -2.492307e-01   \n",
       "75%                       -1.210952e-01                -2.492306e-01   \n",
       "max                        1.589538e+01                 9.483028e+00   \n",
       "\n",
       "       windspeed_norm_4Xmonth_6.0  windspeed_norm_4Xday_of_week_0.0  \n",
       "count                3.310000e+02                      3.310000e+02  \n",
       "mean                 3.765031e-17                      1.304764e-16  \n",
       "std                  1.000000e+00                      1.000000e+00  \n",
       "min                 -1.060354e-01                     -1.285487e-01  \n",
       "25%                 -1.060354e-01                     -1.285487e-01  \n",
       "50%                 -1.060354e-01                     -1.285487e-01  \n",
       "75%                 -1.060354e-01                     -1.285487e-01  \n",
       "max                  1.678698e+01                      1.626381e+01  \n",
       "\n",
       "[8 rows x 1000 columns]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_predictors = list(Xtrain_poly)\n",
    "non_poly_predictors = set(list(Xtrain)) - set(numeric_cols)\n",
    "\n",
    "Xtrain_big = Xtrain_poly.copy()\n",
    "Xtest_big = Xtest_poly.copy()\n",
    "for predictor1 in all_predictors:\n",
    "    for predictor2 in non_poly_predictors:\n",
    "        Xtrain_big[predictor1+'X'+predictor2] = Xtrain_poly[predictor1]* Xtrain_poly[predictor2]\n",
    "        Xtest_big[predictor1+'X'+predictor2] = Xtest_poly[predictor1]* Xtest_poly[predictor2]\n",
    "\n",
    "print(\"Shape is:\")\n",
    "print(Xtrain_big.shape)\n",
    "\n",
    "numeric_strings = ['humidity', 'temp', 'windspeed']\n",
    "# Get a list of all the columns with either temp, humidity, or windspeed\n",
    "\n",
    "numeric_cols = [name for name in list(Xtrain_big) if any(num in name for num in numeric_strings)]\n",
    "\n",
    "#Normalize the data\n",
    "for numeric_col in numeric_cols:\n",
    "    Xtest_big[numeric_col] = Xtest_big[numeric_col].transform(lambda x, m = Xtrain_big[numeric_col].mean(), \n",
    "                                                                          s = Xtrain_big[numeric_col].std()\n",
    "                                                                          : (x - m)/s )\n",
    "    # Rescale train data using mean and std computed from train\n",
    "    Xtrain_big[numeric_col] = Xtrain_big[numeric_col].transform(lambda x: (x - x.mean()) / x.std())\n",
    "\n",
    "Xtrain_big.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PCA 0']\n",
      "(331,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The indices for endog and exog are not aligned",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-180-c1d28d409377>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mPCA_vector_subset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPCA_vectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'PCA '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mytrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0msummary_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_summary_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPCA_vector_subset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0msummary_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-128-baea9b3c6c49>\u001b[0m in \u001b[0;36mget_summary_df\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_summary_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOLS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msummary_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mols\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummary_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rmitchell/anaconda3/lib/python3.6/site-packages/statsmodels/regression/linear_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    629\u001b[0m                  **kwargs):\n\u001b[1;32m    630\u001b[0m         super(OLS, self).__init__(endog, exog, missing=missing,\n\u001b[0;32m--> 631\u001b[0;31m                                   hasconst=hasconst, **kwargs)\n\u001b[0m\u001b[1;32m    632\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"weights\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"weights\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rmitchell/anaconda3/lib/python3.6/site-packages/statsmodels/regression/linear_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, weights, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         super(WLS, self).__init__(endog, exog, missing=missing,\n\u001b[0;32m--> 526\u001b[0;31m                                   weights=weights, hasconst=hasconst, **kwargs)\n\u001b[0m\u001b[1;32m    527\u001b[0m         \u001b[0mnobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rmitchell/anaconda3/lib/python3.6/site-packages/statsmodels/regression/linear_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \"\"\"\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRegressionModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_attr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pinv_wexog'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wendog'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wexog'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rmitchell/anaconda3/lib/python3.6/site-packages/statsmodels/base/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLikelihoodModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rmitchell/anaconda3/lib/python3.6/site-packages/statsmodels/base/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mhasconst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hasconst'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         self.data = self._handle_data(endog, exog, missing, hasconst,\n\u001b[0;32m---> 63\u001b[0;31m                                       **kwargs)\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_constant\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_constant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rmitchell/anaconda3/lib/python3.6/site-packages/statsmodels/base/model.py\u001b[0m in \u001b[0;36m_handle_data\u001b[0;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_handle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhasconst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhasconst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;31m# kwargs arrays could have changed, easier to just attach here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rmitchell/anaconda3/lib/python3.6/site-packages/statsmodels/base/data.py\u001b[0m in \u001b[0;36mhandle_data\u001b[0;34m(endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0mklass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_data_class_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m     return klass(endog, exog=exog, missing=missing, hasconst=hasconst,\n\u001b[0;32m--> 630\u001b[0;31m                  **kwargs)\n\u001b[0m",
      "\u001b[0;32m/Users/rmitchell/anaconda3/lib/python3.6/site-packages/statsmodels/base/data.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# this has side-effects, attaches k_constant and const_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_constant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhasconst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresettable_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rmitchell/anaconda3/lib/python3.6/site-packages/statsmodels/base/data.py\u001b[0m in \u001b[0;36m_check_integrity\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    493\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'index'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'index'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m                 not self.orig_endog.index.equals(self.orig_exog.index)):\n\u001b[0;32m--> 495\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The indices for endog and exog are not aligned\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    496\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPandasData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The indices for endog and exog are not aligned"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wfW0X5cNeOLbwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11510c908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=331)\n",
    "pca.fit(Xtrain_big)\n",
    "plt.plot(pca.explained_variance_[0:5])\n",
    "\n",
    "PCA_vectors = pca.components_[1:6]\n",
    "\n",
    "    \n",
    "for i in range(1,6):\n",
    "    print(['PCA '+str(j) for j in range(i)])\n",
    "    PCA_vector_subset = pd.DataFrame(PCA_vectors[0:i].transpose(), columns=['PCA '+str(j) for j in range(i)])\n",
    "    print(ytrain.shape)\n",
    "    summary_df = get_summary_df(PCA_vector_subset, ytrain)\n",
    "    summary_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your answers here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (i): Beyond Squared Error\n",
    "\n",
    "We have seen in class that the multiple linear regression method optimizes the Mean Squared Error (MSE) on the training set. Consider the following alternate evaluation metric, referred to as the Root Mean Squared Logarthmic Error (RMSLE):\n",
    "\n",
    "$$\n",
    "\\sqrt{\\frac{1}{n}\\sum_{i=1}^n (log(y_i+1) - log(\\hat{y}_i+1))^2}.\n",
    "$$\n",
    "\n",
    "The *lower* the RMSLE the *better* is the performance of a model. The RMSLE penalizes errors on smaller responses more heavily than errors on larger responses. For example, the RMSLE penalizes a prediction of $\\hat{y} = 15$ for a true response of $y=10$ more heavily than a prediction of $\\hat{y} = 105$ for a true response of $100$, though the difference in predicted and true responses are the same in both cases. \n",
    "\n",
    "This is a natural evaluation metric for bike share demand prediction, as in this application, it is more important that the prediction model is accurate on days where the demand is low (so that the few customers who arrive are served satisfactorily), compared to days on which the demand is high (when it is less damaging to lose out on some customers).\n",
    "\n",
    "The following code computes the RMSLE for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------  rmsle\n",
    "# A function for evaluating Root Mean Squared Logarithmic Error (RMSLE)\n",
    "# of the linear regression model on a data set\n",
    "# Input: \n",
    "#      y_test (n x 1 array of response variable vals in testing data)\n",
    "#      y_pred (n x 1 array of response variable vals in testing data)\n",
    "# Return: \n",
    "#      RMSLE (float) \n",
    "\n",
    "def rmsle(y, y_pred):     \n",
    "    # Evaluate sqaured error, against target labels\n",
    "    # rmsle = \\sqrt(1/n \\sum_i (log (y[i]+1) - log (y_pred[i]+1))^2)\n",
    "    rmsle_ = np.sqrt(np.mean(np.square(np.log(y+1) - np.log(y_pred+1))))\n",
    "    \n",
    "    return rmsle_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the above code to compute the training and test RMSLE for the polynomial regression model you fit in Part (g). \n",
    "\n",
    "You are required to develop a strategy to fit a regression model by optimizing the RMSLE on the training set. Give a justification for your proposed approach. Does the model fitted using your approach yield lower train RMSLE than the model in Part (g)? How about the test RMSLE of the new model? \n",
    "\n",
    "**Note:** We do not require you to implement a new regression solver for RMSLE. Instead, we ask you to think about ways to use existing built-in functions to fit a model that performs well on RMSLE. Your regression model may use the same polynomial terms used in Part (g)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your answers here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (j): Dealing with Erroneous Labels\n",
    "\n",
    "Due to occasional system crashes, some of the bike counts reported in the data set have been recorded manually. These counts are not very unreliable and are prone to errors. It is known that roughly 5% of the labels in the training set are erroneous (i.e. can be arbitrarily different from the true counts), while all the labels in the test set were confirmed to be accurate. Unfortunately, the identities of the erroneous records in the training set are not available. Can this information about presence of 5% errors in the training set labels (without details about the specific identities of the erroneous rows) be used to improve the performance of the model in Part (g)? Note that we are interested in improving the $R^2$ performance of the model on the test set (not the training $R^2$ score). \n",
    "\n",
    "As a final task, we require you to come up with a strategy to fit a regression model, taking into account the errors in the training set labels. Explain the intuition behind your approach (we do not expect a detailed mathematical justification). Use your approach to fit a regression model on the training set, and compare its test $R^2$ with the model in Part (g).\n",
    "\n",
    "**Note:** Again, we do not require you to implement a new regression solver for handling erroneous labels. It is sufficient that you to come up with an approach that uses existing built-in functions. Your regression model may use the same polynomial terms used in Part (g)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "your answers here"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
