{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 109A/STAT 121A/AC 209A/CSCI E-109A: Homework 4\n",
    "# Regularization, High Dimensionality, PCA\n",
    "\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2017**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Kevin Rader, Rahul Dave, Margo Levine\n",
    "\n",
    "---\n",
    "\n",
    "### INSTRUCTIONS\n",
    "\n",
    "- To submit your assignment follow the instructions given in canvas.\n",
    "- Restart the kernel and run the whole notebook again before you submit. \n",
    "- Do not include your name(s) in the notebook even if you are submitting as a group. \n",
    "- If you submit individually and you have worked with someone, please include the name of your [one] partner below. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your partner's name (if you submit separately): Ryan Mitchell\n",
    "\n",
    "Enrollment Status (109A, 121A, 209A, or E109A): 109A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.api import OLS\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuing Bike Sharing Usage Data\n",
    "\n",
    "In this homework, we will focus on multiple linear regression, regularization, dealing with high dimensionality, and PCA. We will continue to build regression models for the Capital Bikeshare program in Washington D.C.  See Homework 3 for more information about the data.\n",
    "\n",
    "*Note: please make sure you use all the processed data from HW 3 Part (a)...you make want to save the data set on your computer and reread the csv/json file here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>holiday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>count</th>\n",
       "      <th>season_1.0</th>\n",
       "      <th>season_2.0</th>\n",
       "      <th>season_3.0</th>\n",
       "      <th>...</th>\n",
       "      <th>day_of_week_4.0</th>\n",
       "      <th>day_of_week_5.0</th>\n",
       "      <th>day_of_week_6.0</th>\n",
       "      <th>weather_1.0</th>\n",
       "      <th>weather_2.0</th>\n",
       "      <th>weather_3.0</th>\n",
       "      <th>temp_norm</th>\n",
       "      <th>atemp_norm</th>\n",
       "      <th>humidity_norm</th>\n",
       "      <th>windspeed_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>56.2083</td>\n",
       "      <td>0.194037</td>\n",
       "      <td>3830.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.341801</td>\n",
       "      <td>-1.363792</td>\n",
       "      <td>-0.500703</td>\n",
       "      <td>0.040945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>65.2917</td>\n",
       "      <td>0.350133</td>\n",
       "      <td>2114.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.431146</td>\n",
       "      <td>-1.665877</td>\n",
       "      <td>0.132958</td>\n",
       "      <td>2.036025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>56.8333</td>\n",
       "      <td>0.149883</td>\n",
       "      <td>915.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.695943</td>\n",
       "      <td>1.757749</td>\n",
       "      <td>-0.457103</td>\n",
       "      <td>-0.523392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>49.0833</td>\n",
       "      <td>0.268033</td>\n",
       "      <td>4322.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.805728</td>\n",
       "      <td>-0.759623</td>\n",
       "      <td>-0.997746</td>\n",
       "      <td>0.986696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>69.7083</td>\n",
       "      <td>0.215171</td>\n",
       "      <td>6591.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.981180</td>\n",
       "      <td>0.952190</td>\n",
       "      <td>0.441062</td>\n",
       "      <td>0.311061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   holiday  workingday  temp  atemp  humidity  windspeed   count  season_1.0  \\\n",
       "0      0.0         1.0   2.0    6.0   56.2083   0.194037  3830.0           1   \n",
       "1      0.0         1.0   1.0    3.0   65.2917   0.350133  2114.0           1   \n",
       "2      0.0         1.0  36.0   37.0   56.8333   0.149883   915.0           0   \n",
       "3      0.0         1.0   8.0   12.0   49.0833   0.268033  4322.0           1   \n",
       "4      0.0         0.0  28.0   29.0   69.7083   0.215171  6591.0           0   \n",
       "\n",
       "   season_2.0  season_3.0       ...        day_of_week_4.0  day_of_week_5.0  \\\n",
       "0           0           0       ...                      1                0   \n",
       "1           0           0       ...                      1                0   \n",
       "2           1           0       ...                      1                0   \n",
       "3           0           0       ...                      0                0   \n",
       "4           1           0       ...                      0                0   \n",
       "\n",
       "   day_of_week_6.0  weather_1.0  weather_2.0  weather_3.0  temp_norm  \\\n",
       "0                0            1            0            0  -1.341801   \n",
       "1                0            0            1            0  -1.431146   \n",
       "2                0            0            1            0   1.695943   \n",
       "3                0            1            0            0  -0.805728   \n",
       "4                0            1            0            0   0.981180   \n",
       "\n",
       "   atemp_norm  humidity_norm  windspeed_norm  \n",
       "0   -1.363792      -0.500703        0.040945  \n",
       "1   -1.665877       0.132958        2.036025  \n",
       "2    1.757749      -0.457103       -0.523392  \n",
       "3   -0.759623      -0.997746        0.986696  \n",
       "4    0.952190       0.441062        0.311061  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('./data/train_processed.csv')\n",
    "test_df = pd.read_csv('./data/test_processed.csv')\n",
    "train_df = train_df.drop('Unnamed: 0', 1)\n",
    "test_df = test_df.drop('Unnamed: 0', 1)\n",
    "train_df.head()\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (f): Regularization/Penalization Methods\n",
    "\n",
    "As an alternative to selecting a subset of predictors and fitting a regression model on the subset, one can fit a linear regression model on all predictors, but shrink or regularize the coefficient estimates to make sure that the model does not \"overfit\" the training set. \n",
    "\n",
    "Use the following regularization techniques to fit linear models to the training set:\n",
    "- Ridge regression\n",
    "- Lasso regression\n",
    "    \n",
    "You may choose the shrikage parameter $\\lambda$ from the set $\\{10^{-5}, 10^{-4},...,10^{4},10^{5}\\}$ using cross-validation. In each case, \n",
    "\n",
    "- How do the estimated coefficients compare to or differ from the coefficients estimated by a plain linear regression (without shrikage penalty) in Part (b) fropm HW 3? Is there a difference between coefficients estimated by the two shrinkage methods? If so, give an explantion for the difference.\n",
    "- List the predictors that are assigned a coefficient value close to 0 (say < 1e-10) by the two methods. How closely do these predictors match the redundant predictors (if any) identified in Part (c) from HW 3?\n",
    "- Is there a difference in the way Ridge and Lasso regression assign coefficients to the predictors `temp` and `atemp`? If so, explain the reason for the difference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEOCAYAAACaQSCZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1141e5198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Lasso: 10\n",
      "Best Ridge: 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=100, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use Ridge and Lasso regression to create models on the training set\n",
    "\n",
    "# Remove the numeric columns that have been normalized separately, as well as the redundant categorical columns\n",
    "remove_cols = ['temp', 'atemp', 'humidity', 'windspeed', 'count', 'season_3.0', 'month_12.0', 'day_of_week_6.0', 'weather_3.0']\n",
    "\n",
    "Xtrain = train_df[train_df.columns.difference(remove_cols)]\n",
    "Xtest  = test_df[test_df.columns.difference(remove_cols)]\n",
    "predictors = list(Xtrain)\n",
    "\n",
    "# Create response \n",
    "ytrain = train_df['count']\n",
    "ytest  = test_df['count']\n",
    "\n",
    "# Added additional alpha values around 10 in order to get more precise data around this location\n",
    "alphas = [.00001, .0001, .001, .01, .1, 1, 5, 8, 10, 12, 20, 30, 35, 40, 45, 50, 100, 1000, 10000, 100000]\n",
    "r2s = {'ridge':[], 'lasso':[]}\n",
    "\n",
    "for alpha in alphas:\n",
    "    # Fit Lasso regression and add r2 results to r2 object\n",
    "    lasso = Lasso(alpha=alpha, fit_intercept=True)\n",
    "    lasso.fit(Xtrain, ytrain)\n",
    "    lasso_preds = lasso.predict(Xtest)\n",
    "    r2s['lasso'].append(r2_score(ytest, lasso_preds))\n",
    "    \n",
    "    # Fit Ridge regression add r2 results to r2 object\n",
    "    ridge = Ridge(alpha=alpha, fit_intercept=True)\n",
    "    ridge.fit(Xtrain, ytrain)\n",
    "    ridge_preds = ridge.predict(Xtest)\n",
    "    r2s['ridge'].append(r2_score(ytest, ridge_preds))\n",
    "    \n",
    "# Use a semilogx plot to display alpha values on a normalized scale\n",
    "plt.semilogx(alphas, r2s['ridge'], label='Ridge')\n",
    "plt.semilogx(alphas, r2s['lasso'], label='Lasso')\n",
    "plt.ylabel(\"R2 Score\")\n",
    "plt.xlabel(\"Alpha\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Get the best r2 score found for both ridge and lasso\n",
    "winningRidge = np.argmax(r2s['ridge'])\n",
    "winningLasso = np.argmax(r2s['lasso'])\n",
    "print(\"Best Lasso: \"+str(alphas[winningLasso]))\n",
    "print(\"Best Ridge: \"+str(alphas[winningRidge]))\n",
    "\n",
    "# Plot all r2 scores by their alpha value\n",
    "lasso = Lasso(alpha=alphas[winningLasso], fit_intercept=True)\n",
    "lasso.fit(Xtrain, ytrain)\n",
    "\n",
    "ridge = Ridge(alpha=alphas[winningRidge], fit_intercept=True)\n",
    "ridge.fit(Xtrain, ytrain)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predictors</th>\n",
       "      <th>Ridge</th>\n",
       "      <th>Lasso</th>\n",
       "      <th>Normal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>atemp_norm</td>\n",
       "      <td>471.65686796460324</td>\n",
       "      <td>452.32036774443935</td>\n",
       "      <td>312.43407188889796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>day_of_week_0.0</td>\n",
       "      <td>-84.42496035271255</td>\n",
       "      <td>-306.2418651503781</td>\n",
       "      <td>-465.1450099570902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>day_of_week_1.0</td>\n",
       "      <td>-89.06248939039168</td>\n",
       "      <td>-201.48785387674346</td>\n",
       "      <td>-256.6500506662598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>day_of_week_2.0</td>\n",
       "      <td>-31.189729499901865</td>\n",
       "      <td>-144.6498385720568</td>\n",
       "      <td>-328.1845068957066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>day_of_week_3.0</td>\n",
       "      <td>39.961127678447966</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.61277259517965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>day_of_week_4.0</td>\n",
       "      <td>25.985191169271868</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-71.64254439857939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>day_of_week_5.0</td>\n",
       "      <td>57.02446103414826</td>\n",
       "      <td>9.51114916914812</td>\n",
       "      <td>-21.831674879257434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>holiday</td>\n",
       "      <td>-43.34246184082053</td>\n",
       "      <td>-179.77933843895644</td>\n",
       "      <td>-616.6027102985504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>humidity_norm</td>\n",
       "      <td>-354.4763637051971</td>\n",
       "      <td>-567.6253916153483</td>\n",
       "      <td>-548.4929490582165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>month_1.0</td>\n",
       "      <td>-157.83011306540536</td>\n",
       "      <td>-65.66318839582553</td>\n",
       "      <td>118.83581871663228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>month_10.0</td>\n",
       "      <td>169.79217059417743</td>\n",
       "      <td>489.4999509358776</td>\n",
       "      <td>605.086722383922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>month_11.0</td>\n",
       "      <td>17.28302951846026</td>\n",
       "      <td>0.0</td>\n",
       "      <td>231.5174639383212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>month_2.0</td>\n",
       "      <td>-79.9373026728777</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>207.77591139109774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>month_3.0</td>\n",
       "      <td>-23.009858046192416</td>\n",
       "      <td>40.220468628239814</td>\n",
       "      <td>358.01671712429106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>month_4.0</td>\n",
       "      <td>71.45743490019235</td>\n",
       "      <td>208.63177313325096</td>\n",
       "      <td>452.18490513402173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>month_5.0</td>\n",
       "      <td>47.910337718309776</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.02331873120545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>month_6.0</td>\n",
       "      <td>-40.65841108902176</td>\n",
       "      <td>-402.69284311000763</td>\n",
       "      <td>-673.4270797861232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>month_7.0</td>\n",
       "      <td>-126.49231078155596</td>\n",
       "      <td>-753.7830969826168</td>\n",
       "      <td>-1161.1511875434717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>month_8.0</td>\n",
       "      <td>1.6273831785746764</td>\n",
       "      <td>-193.0529551209989</td>\n",
       "      <td>-657.6396711861653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>month_9.0</td>\n",
       "      <td>214.07402106448762</td>\n",
       "      <td>620.6678530499136</td>\n",
       "      <td>523.9803848202395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>season_1.0</td>\n",
       "      <td>-334.4663875762657</td>\n",
       "      <td>-753.0742270057004</td>\n",
       "      <td>-1032.8815748448567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>season_2.0</td>\n",
       "      <td>85.69731487079282</td>\n",
       "      <td>66.08814637433402</td>\n",
       "      <td>-134.052535354319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>season_4.0</td>\n",
       "      <td>221.74064082352493</td>\n",
       "      <td>356.04595725832917</td>\n",
       "      <td>193.3049678072952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>temp_norm</td>\n",
       "      <td>468.7172378134785</td>\n",
       "      <td>680.2124587130347</td>\n",
       "      <td>925.7338498566318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>weather_1.0</td>\n",
       "      <td>172.46275257419305</td>\n",
       "      <td>740.3489505883257</td>\n",
       "      <td>1581.9782836079432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>weather_2.0</td>\n",
       "      <td>-7.09676351535211</td>\n",
       "      <td>727.7028379089464</td>\n",
       "      <td>1565.4116995877023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>windspeed_norm</td>\n",
       "      <td>-215.92299481288222</td>\n",
       "      <td>-248.16085928013857</td>\n",
       "      <td>-255.12258899234206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>workingday</td>\n",
       "      <td>46.06102283239511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-24.093293946099294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         predictors                Ridge                Lasso  \\\n",
       "0        atemp_norm   471.65686796460324   452.32036774443935   \n",
       "1   day_of_week_0.0   -84.42496035271255   -306.2418651503781   \n",
       "2   day_of_week_1.0   -89.06248939039168  -201.48785387674346   \n",
       "3   day_of_week_2.0  -31.189729499901865   -144.6498385720568   \n",
       "4   day_of_week_3.0   39.961127678447966                  0.0   \n",
       "5   day_of_week_4.0   25.985191169271868                 -0.0   \n",
       "6   day_of_week_5.0    57.02446103414826     9.51114916914812   \n",
       "7           holiday   -43.34246184082053  -179.77933843895644   \n",
       "8     humidity_norm   -354.4763637051971   -567.6253916153483   \n",
       "9         month_1.0  -157.83011306540536   -65.66318839582553   \n",
       "10       month_10.0   169.79217059417743    489.4999509358776   \n",
       "11       month_11.0    17.28302951846026                  0.0   \n",
       "12        month_2.0    -79.9373026728777                 -0.0   \n",
       "13        month_3.0  -23.009858046192416   40.220468628239814   \n",
       "14        month_4.0    71.45743490019235   208.63177313325096   \n",
       "15        month_5.0   47.910337718309776                  0.0   \n",
       "16        month_6.0   -40.65841108902176  -402.69284311000763   \n",
       "17        month_7.0  -126.49231078155596   -753.7830969826168   \n",
       "18        month_8.0   1.6273831785746764   -193.0529551209989   \n",
       "19        month_9.0   214.07402106448762    620.6678530499136   \n",
       "20       season_1.0   -334.4663875762657   -753.0742270057004   \n",
       "21       season_2.0    85.69731487079282    66.08814637433402   \n",
       "22       season_4.0   221.74064082352493   356.04595725832917   \n",
       "23        temp_norm    468.7172378134785    680.2124587130347   \n",
       "24      weather_1.0   172.46275257419305    740.3489505883257   \n",
       "25      weather_2.0    -7.09676351535211    727.7028379089464   \n",
       "26   windspeed_norm  -215.92299481288222  -248.16085928013857   \n",
       "27       workingday    46.06102283239511                  0.0   \n",
       "\n",
       "                 Normal  \n",
       "0    312.43407188889796  \n",
       "1    -465.1450099570902  \n",
       "2    -256.6500506662598  \n",
       "3    -328.1845068957066  \n",
       "4     37.61277259517965  \n",
       "5    -71.64254439857939  \n",
       "6   -21.831674879257434  \n",
       "7    -616.6027102985504  \n",
       "8    -548.4929490582165  \n",
       "9    118.83581871663228  \n",
       "10     605.086722383922  \n",
       "11    231.5174639383212  \n",
       "12   207.77591139109774  \n",
       "13   358.01671712429106  \n",
       "14   452.18490513402173  \n",
       "15    53.02331873120545  \n",
       "16   -673.4270797861232  \n",
       "17  -1161.1511875434717  \n",
       "18   -657.6396711861653  \n",
       "19    523.9803848202395  \n",
       "20  -1032.8815748448567  \n",
       "21    -134.052535354319  \n",
       "22    193.3049678072952  \n",
       "23    925.7338498566318  \n",
       "24   1581.9782836079432  \n",
       "25   1565.4116995877023  \n",
       "26  -255.12258899234206  \n",
       "27  -24.093293946099294  "
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(Xtrain, ytrain)\n",
    "\n",
    "predictors = np.array(predictors)\n",
    "coefficients = np.transpose([predictors, ridge.coef_, lasso.coef_, linreg.coef_])\n",
    "coefficients_df = pd.DataFrame(coefficients, columns=['predictors', 'Ridge', 'Lasso', 'Normal'])\n",
    "coefficients_df.head(len(coefficients_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next analyze the performance of the two shrinkage methods for different training sample sizes:\n",
    "- Generate random samples of sizes 100, 150, ..., 400 from the training set. You may use the following code to draw a random sample of a specified size from the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3Xd4VWW2+PHvOun9hCS0FAIJoUMKApIgxQIqggVEscQy\nKo4641x/Ok7x3pm5o+Ncp1671zKjo2BDRcA2QtTQEkooAQJJgBBKGklIIf39/bHPiRExnCSnJXk/\nz5MHzj777L0iksXea71ri1IKTdM0TTsfk6sD0DRN03oHnTA0TdM0m+iEoWmaptlEJwxN0zTNJjph\naJqmaTbRCUPTNE2ziU4YmqZpmk10wtA0TdNsohOGpmmaZhNPVwdgT+Hh4So2NtbVYWiapvUa27Zt\nK1dKRdiyb59KGLGxsWzdutXVYWiapvUaInLE1n31LSlN0zTNJjphaJqmaTbRCUPTNE2zSZ+qYWia\nptmqubmZ4uJiGhoaXB2KU/j6+hIVFYWXl1e3j6EThqZp/VJxcTFBQUHExsYiIq4Ox6GUUlRUVFBc\nXMzw4cO7fRx9S0rTtH6poaGBsLCwPp8sAESEsLCwHl9N6YShaVq/1R+ShZU9vledMPqI8tpGvthb\nwiuZh2hobnV1OJrWJy15cRNLXtzk6jBcRtcweqGmljb2njjNjqJKdhRVseNoJUdPnWl/3+znxXUp\nUS6MUNM0WwQGBlJbW+vqMGymE4abU0pxvLrh2+RQVMme46dpamkDYEiIL0kxZm6dFktijJl73tjG\nhoJynTA0TbM7nTDcTH1TC7uKq8k5WtWeJEprGgHw8TQxMSqE26bHkhRtJjHGzJAQv+98fnpcGBvy\ny1FK9av7s5rWm9XW1rJw4UIqKytpbm7m97//PQsXLqSuro7rr7+e4uJiWltbeeyxx1iyZAmPPvoo\nq1atwtPTk8suu4w//elPHD58mDvuuIPy8nIiIiJ47bXXiImJsWucOmG4UFub4lBFXfuVw46iKvJK\namhtUwAMDw8gLT6cxBgzSdGhjB4ShJdH52WntPhwVu86QUFZLfEDg5zxbWhar/fbj3PZe/z0effb\ne8LYx5Y6xtihwfzXVeNsOr+vry8ffPABwcHBlJeXM23aNBYsWMCnn37K0KFDWbNmDQDV1dVUVFTw\nwQcfsH//fkSEqqoqAB544AHS09NJT0/n1Vdf5Sc/+QkffvihTee3lU4YTlRd30xO8bfJIedoFdVn\nmgEI8vEkMcbMfWPiSIoJZVK0mQEB3l0+R2p8OACZB8t1wtC0XkIpxS9/+Uu+/vprTCYTx44do6Sk\nhAkTJvDQQw/x85//nPnz5zNjxgxaWlrw9fXlzjvvZP78+cyfPx+ATZs2sXLlSgBuueUWHnnkEbvH\nqROGg7S0tpFXUmO5eqgi52glBWV1AIjAqEFBXDFhMEnRoSTFmImLCMRk6vktpOgB/gwL8yczv4Lb\nUru/QEfT+hNbrwSsVxZv33OhXc//5ptvUlZWxrZt2/Dy8iI2NpaGhgYSEhLYvn07a9eu5de//jUX\nX3wx//mf/0lWVhZffvkl7733Hs888wzr1q2zazw/RCcMOymtaWhPDjuKKtlVXM0ZS3trWIA3STFm\nrk2OIinazMRoM4E+jvtPPz0unI93HqeltQ3P89zC0jTN9aqrqxk4cCBeXl6sX7+eI0eMiePHjx9n\nwIAB3HzzzZjNZl5++WVqa2upr6/niiuuIDU1lREjRgAwffp0VqxYwS233MKbb77JjBkz7B6nThjd\n0NjSSu7x09+pPRyrMtpavTyEsUNDWHJBNEkxZpJjQokK9XNqATotPpzlWUXsLK4mZVio086raVr3\n3HTTTVx11VVMmDCByZMnM3r0aAB2797Nww8/jMlkwsvLi+eff56amhoWLlxIQ0MDSin+8pe/APD0\n009z++2389RTT7UXve3NoQlDROYBfwc8gJeVUk+eY59ZwN8AL6BcKTXTst0MvAyMBxRwh1LK6Stm\nlFIUV55he1GlpXOpir3HT9PUarS1Rpr9SIwxc3tqLEkxoYwbGoyvl4ezw/yOC+PCEIEN+eU6YWia\nG7OuwQgPD2fTpu//eIuNjWXu3Lnf256VlfW9bcOGDXP4rSmHJQwR8QCeBS4FioFsEVmllNrbYR8z\n8BwwTylVJCIDOxzi78CnSqlFIuIN+Dsq1o5qG1vYVVz1ndpDeW0TAH5eHkyMCuGOtOEkRptJijEz\nKNjXGWF1yYAAb8YNDSYzv5yfXDzS1eFoWp9h79pFb+PIK4wpQL5SqhBARFYAC4G9HfZZCqxUShUB\nKKVKLfuGABcBt1m2NwFNjgiyqaWND3OOtd9eOlBSg6WrlRERAcxMGEhSjJEcRg0K6jU1gdT4cF7N\nPERdYwsBDqyXaJrWfzjyJ0kkcLTD62Jg6ln7JABeIpIBBAF/V0q9DgwHyoDXRGQSsA34qVKqzt5B\nepqE/169FwESY0KZO24wSTFmEqPNmP273tbqUuX58PmvYf5fSIsP58WvCsk6fIrZowae/7Oapmnn\n4ep/enoCKcDFgB+wSUQ2W7YnAw8opbaIyN+BR4HHzj6AiNwN3A10a1WjySR8/rOLGBTka5e2Vpep\nPwVvLYZThZA7gwsmL8Pb08TG/HKdMDRNswtH3l85BkR3eB1l2dZRMfCZUqpOKVUOfA1MsmwvVkpt\nsez3HkYC+R6l1EtKqclKqckRERHdCnRIiF/vThYtjbDiJqguhoAIKFiHr5cHk4eFkplf4ero+r2f\nLN/B/3t3p6vD0LQec2TCyAZGishwS9H6BmDVWft8BKSJiKeI+GPcstqnlDoJHBWRUZb9Lua7tQ/N\nSin4+KdQtBGufh7GXweHN0BzA6nx4ew7cZry2kZXR9lv1Ta28MmeE3yw4xgnq/vHo0D7tNeuNL76\nKYclDKVUC3A/8BmwD3hHKZUrIstEZJlln33Ap8AuIAuj9XaP5RAPAG+KyC4gEXjCUbH2at/8CXYu\nh1m/hAmLIG4OtJyBo1tIs4wJ2VigrzJcZUN+Oc2titY2xbtbj57/A1q/dcUVV7TPheroN7/5DX/6\n059cENH3ObSGoZRaC6w9a9sLZ71+CnjqHJ/NASY7Mr5eb8/7sO73MHEJzLTMjRmWCiYvKFjH+Isv\nItjXkw0Hy1kwaahrY+2nMvJKCfTxZNzQYFZkH+XHs+Px6M23PzWHUEqxevVqTCb37sJ07+i0H3Y0\nCz64F2IuhAVPGwOqAHwCIWYaFKzDwyRMjwsn0zLuXHMupRQZeWU8NGgbP4/K5VjVGb45WObqsDQ3\ncfjwYUaNGsWtt97K+PHj8fDwoLy8HIDHH3+chIQE0tLSyMvLa/9MdnY2EydOJDExkYcffpjx48cD\n0NraysMPP8wFF1zAxIkTefHFFx0Ss6u7pLTuqDwMy2+E4CGw5E3w9Pnu+yNmwbr/htoyUkeG82nu\nSY5U1BMbHuCCYPuvAyW1lFTXs7TtRbxrTAz2f5blWUXM0l1r7ueTR+Hk7vPvd3KX8astdYzBE+Dy\n7w23+I6DBw/yz3/+k2nTphEbGwvAtm3bWLFiBTk5ObS0tJCcnExKSgoAt99+O//3f//HhRdeyKOP\nPtp+nFdeeYWQkBCys7NpbGwkNTWVyy67jOHD7TuAVF9h9DYN1fDWEmhrhqXvQkDY9/eJm2P8WpjR\nXsfIzC93YpAaGLejJkghPs1VyJlTPDoin3/vK6X0tC5+a4Zhw4Yxbdq072z75ptvuOaaa/D39yc4\nOJgFCxYAUFVVRU1NDRdeaKw2X7p0aftnPv/8c15//XUSExOZOnUqFRUVHDx40O7x6iuM3qS1Gd5J\nh4p8uHklRCSce78hk8BvABSsI3bCIiLNfmzIL+fmacOcG28/l5FXxnXB+6FRIGgwcxs+48G2ON7d\nVsx9s+NdHZ7W0XmuBNpZryxuX2OX0wYE2OeqXynF008/fc65U/akrzB6C6Vg7cNQuB7m/xVGzPzh\nfU0exm2pwvUIkBofxsaCivYn+WmOV9vYwtYjp5jjtRsik+GCO/ErzuTqYY0szyqiTf9ZaD/goosu\n4sMPP+TMmTPU1NTw8ccfA2A2mwkKCmLLFmN52ooVK9o/M3fuXJ5//nmam40Hsh04cIC6OrsPxtAJ\no9fY/Bxsew1SH4TkW8+/f9wcqDkBZftJjQ+n+kwzucerHR+nBhjttP6tNUTW5UL8JZB4M4iJ+0I2\nUlx5Rt8i1H5QcnIyS5YsYdKkSVx++eVccMEF7e+98sor3HXXXSQmJlJXV0dISAgAP/rRjxg7dizJ\nycmMHz+ee+65h5aWFrvHpm9J9Qb718Jnv4IxV8HF/2XbZ+JmG78WrGP6+B8BRh1jYpTZQUFqHWXk\nlXGJz15EtRkJI3gIjJxL/LGPGOh/McuzirgooXuTCbS+ITY2lj179rS/Pnz4cPvvf/WrX/GrX/3q\ne58ZN24cu3YZhfcnn3ySyZONlQcmk4knnniCJ55w7HI1fYXh7o7nwPt3wtBEuOYlsLVPOyQKwhOg\nYB0RQT6MHhzEBv2vWqdQSvFVXinXBe8DXzNEGh0upKQjdaU8MuIwX+wtobRGF797ndvX2K1+0R1r\n1qwhMTGR8ePH88033/DrX//aqefXCcOdnT4Oy28wCtg3rgDvLj4SJG5O+5iQtPhwsg9X0mB5bKzm\nOAdKajlefYakpm3Gn4HJ8kCt+EshaAhXNH1OS5vivW3Frg1U63WWLFlCTk4Oe/bsYc2aNXR3fl53\n6YThrhprjfbZxhpY+jYEDe76MdrHhGwmdWQ4TS1tbD1caf9Yte/IyCtljBTh11hu3I6y8vCEpJvx\nL1rPlcNaWZF1VBe/Xaw/LWi1x/eqE4Y7amuFlXdByR5Y9BoMHt+947SPCVnPlNgBeJpEF1udICOv\njEUh+40X8Rd/982kWwB4IHQzRafq9ZwvF/L19aWioqJfJA2lFBUVFfj69uwJobro7Y6++E/IWwuX\nPwUJl3X/OB3GhARc+luSY0J1HcPBrO20Tw7YDSETvn9lGDoM4mYz6viHhPnNYHlWEWkjw10TbD8X\nFRVFcXExZWX9Y1yLr68vUVFRPTqGThjuJvsV2PQMTLkHpt7d8+PFzYYvf2eMCYkP529fHqCyronQ\ngF72NMFeYkN+OT6tdUTX7oKJ9597p+R05N10/t/I4zyWqyiraSQiyOfc+2oO4+XlZffRGX2dviXl\nTvK/NBbnjbwM5tqpPW6Epb22MIO0kWEoBZsK9W0QR8nIK2OOz35MqgVGXnrunUZdAf7hXNXyBS1t\nive36+K31jvohOEuSvfBu7dBxGhY9KpRILWHDmNCJkaZCfTx1HUMB2lvpw3ZD95BEDXl3Dt6ekPi\nUgKPfMGlMbBCr/zWegmdMNxBbSm8eT14+RkdUT5B9ju2dUxIwTq8TMK0EQPYqBOGQxwsNdppU5q3\nG6NbPDu57Zd8K7S18GBYNocr6tmsr/q0XkAnDFdrPgMrlkJdGdy4HMzR5/9MV8XNgdqTULqP1Phw\nDlfUc/RUvf3P08+t319KnBwn8Mzx73dHnS18JAxLZczJDzH7evBWVpFzgtS0HtAJw5Xa2uDDH0Nx\nNlz70rcrgu3NOiakcH2Hx7bqqwx7y8grY3F7O+0lne8MkJyOqfIQ/zGylM9yT1Khn72uuTmHJgwR\nmScieSKSLyKP/sA+s0QkR0RyReSrDtsPi8huy3tbHRmny2Q8Abkr4ZLfwtgFjjtPSBSEj4KCdcQP\nDGRgkA+Z+foWiD1Z22kv895j/Lc2x5z/Q2MXgG8I16h/09yqi9+a+3NYwhARD+BZ4HJgLHCjiIw9\nax8z8BywQCk1Dlh81mFmK6USlVJ979neOcvh66eMhVypP3X8+eJmw+ENSEsjafHhbMwv14VWO9qQ\nX45HawOxtTtsu7oAo2Y18QaCDn3C7BgPlmcd7ReLyLTey5FXGFOAfKVUoVKqCVgBLDxrn6XASqVU\nEYBSqtSB8biPwxtg1QMw/CK48i/fPo/bkTqOCYkPp6Kuif0naxx/3n4iI6+Mmd55mNqazl+/6Cgl\nHVqbeDBiO4fK69hceMpxQWpaDzkyYUQCRzu8LrZs6ygBCBWRDBHZJiIdH/SggH9bttthBZubqCiA\nt2+C0Fi4/vXOO2nsqX1MyDpSLXUMverbPqzttNeb88DTz/hvbatB4yByMhNKPiTY14MV2br4rbkv\nVxe9PYEU4EpgLvCYiFifO5qmlErEuKV1n4hcdK4DiMjdIrJVRLa6/RL/+lPw5mJA4KZ3wC/Ueefu\nMCZkcIgv8QMD9XoMOzHaaRu4oGU7DJ8BXl2c15OSjqk8j58mVPHJ7pNU1jU5JlBN6yFHJoxjQMce\n0SjLto6Kgc+UUnVKqXLga2ASgFLqmOXXUuADjFtc36OUekkpNVkpNdnZo367pKUJ3r4Fqo/CDW/B\ngBHOjyFuNpzcDbVlpMWHk3XoFI0tetx5T2XklRIjJQTXH7G9ftHRuGvBO5Dr5N80tbbp4rfmthyZ\nMLKBkSIyXES8gRuAVWft8xGQJiKeIuIPTAX2iUiAiAQBiEgAcBmwh95KKfj4p3AkExY+C8MudE0c\ncXOMXwszSI0P50xzKzuKqlwTSx9itNPmGS+6kzB8AmHCIswFq0mL8mJ5VpEufmtuyWEJQynVAtwP\nfAbsA95RSuWKyDIRWWbZZx/wKbALyAJeVkrtAQYBmSKy07J9jVLqU0fF6nCZf4Gdb8HMR2Hi9a6L\nY/C3Y0KmjhiAh0l0HaOHahtbyD58inm+e4y6VHevHJNvhZYz/GzwTgrK6sjWzy3R3JBDp9UqpdYC\na8/a9sJZr58CnjprWyGWW1O9Xu4HxrTY8Ytg1jmXojiPydQ+JiTYx5NJUSFk5pfz0GWjXBtXL7Yh\nvxxpbWJE7XZIWtr9jrehyTBoAkllqwjyncjyrCKmDB9g32A1rYdcXfTu24q3wgfLIHqqcSvKGe2z\n59NhTEhafDg7j1ZxuqHZ1VH1Whl5ZaR5H8Sjpd54BGt3iRjF75Jd/HhUDWt2n6CqXhe/NfeiE4aj\nVB4xnscdOMgocne1c8ZRrGNCLO21bQo266e+dYu1nXZJ6AHw8IbYtJ4dcMJi8PRjiWk9TS1trNx+\ndo+IprmWThiO0FBtPI+7pQluehcC3OiJatYxIYXrSYoJxc/LQ9cxusnaTju1dTvEXGgUr3vCzwzj\nrmZAwUdMjfLVxW/N7eiEYW+tLfDu7VBxEJa8DhFuWB+ImwOHN+Ctmpg6YoBej9FNGXmlDKYCc21+\n97qjziU5HZpqeGhoLgdLa9l2RBe/NfehE4Y9KQWfPAIFX8KVfzYKzO4obva3Y0Liwikoq+NE9RlX\nR9XrZOSVcX3oAeOFvRJGzDQIH0VKxSoCfTz12HPNreiEYU9bXoCtr8D0n0DKba6O5oedc0yIrmN0\nhbWd9grfPRAcCQPH2OfAIpB8Kx7Hsrl7dANrdp2gul43JWjuQScMe8n7BD79BYyeb4wrd2cdxoSM\nHhxEWIC3rmN00cb8clRrM/G12cawQXt2wE26EUxe3OSVQWNLGx/s0Cu/NfegE4Y9nNgF791pPD/7\n2peM9Q7uzjImxFRfxvT4cDLzy3WBtQvW55VxofchPJtr7Xc7yiogDMbMJ6zgAyZH+umx55rb6AU/\n2dzc6RNGR5SfGW5cAd4Bro7INh3GhKTFh1FW08jB0lrXxtRLWNtpbxhwAMQDhs+0/0mS0+FMJQ9F\n5ZFXUsN2PcJFcwM6YfREUx0sXwKNp2Hp2xA8xNUR2a59TMh6Pe68i6zttBe2bYfoKcY/Fuxt+Eww\nD2NK5WoCvD1YrovfmhvQCaO72lrh/buM6a+LXoXBE1wdUdd0GBMSZfYjNsxfJwwbZeSVEk41A07v\ns//tKCuTySh+F2Vyx9g2Vu86TvUZXfzWXEsnjO76939B3hqY+wdImOvqaLqnw5iQ1PhwNheeorm1\nzdVRub2MvDIW27ud9lySbgbx4Fbvr2hobuOjHL3yW3MtnTC6Y+trsPFpuOAumHqPq6Ppvg5jQtLi\nw6ltbGFXsb5X3hlrO+18/1wIiIDBEx13sqDBkDCPiIL3SRwawFtb9MpvzbV0wuiqgnWw5iHjX5bz\nnnSPgYLdZR0TUrCOC+PCEIHMg3o9Rmc25pfT2tpKQm02xF3s+I64lHSoK+M/hhWy/2QNOUd1Qtdc\nRyeMrijdD++kQ8RoWPQaeDh0OrxzxM2BIxswe7UxITJE1zHOI+NAGVO8j+DVWAkjezCd1lbxl0Bw\nJNOrV+Ovi9+ai+mEYavaMnhrMXj6Gh1RvsGujsg+4uZAS4MxJiQ+nO1FldQ1trg6KrdktNOWcWPY\nAUBgxGzHn9TkAUk341m4jvSxJj7eeUKPo9dcRicMWzSfgRU3Gklj6QowR5//M71F7LdjQtLiw2lp\nU2QdOuXqqNzSwdJajlWdYbrKgchkY4GdMyTdDMBtfpmcaW7lo5zjzjmvpp1FJ4zzaWuDD38Mxdlw\n7YsQmeLqiOzLO6B9TEjKsFB8PE16eu0PyMgrJYRawqt3O7Y76mzmGIibw8CC95gwRBe/NddxaMIQ\nkXkikici+SJyzueTisgsEckRkVwR+eqs9zxEZIeIrHZknJ3K+APkroRLfgNjF7osDIeyjAnxbazg\ngtgBuo7xA4x22nxEtTk3YQCkpCOnj/Gz4UfZd+I0u4qrnXt+TcOBCUNEPIBngcuBscCNIjL2rH3M\nwHPAAqXUOGDxWYf5KbDPUTGeV85y+Pp/jFsCqQ+6LAyH6zAmJDU+nP0nayitaXBtTG7G2k57VUAu\n+Jqdf6WZcDkERDCjZg1+Xrr4rbmGTQlDRKaJyK2W34eJSIwNH5sC5CulCpVSTcAK4Ox/oi8FViql\nigCUUqUdzhkFXAm8bEuMdnd4A6x6AGJnwJV/7d3ts+fTPibEqGMAbNKPbf2OjfnlNLe2MaY2y0iw\nJg/nBuDpDYlL8cr/nKVjvVi18zg1uvitOdl5E4aI/Br4L+DXlk2+wFs2HDsSONrhdbFlW0cJQKiI\nZIjINmtSsvgb8Ajg/KXHFQXw9k0QGgtL3jD+svZlJpNxW6pgPWOHBGH29yLzoL4t1VHGgTKSvYvx\nbihz/u0oq+R0UK3cGbiJ+qZWVu3UxW/NuWy5wlgEXAHUASiljgH26in1BFIwriTmAo+JSIKIzAdK\nlVLbzncAEblbRLaKyNaysrKeR1R/Ct66HhCjfdYvtOfH7A1GzIbak3iU72d6XBgb9LjzdtZ22pvC\nDxob4i92TSBhcRA7gyGF7zJmUIC+LaU5nS0Jo1EZPzkUgIj423jsY0DH/tMoy7aOioHPlFJ1Sqly\n4GtgEpAKLBCRwxi3suaIyL/OdRKl1EtKqclKqckRERE2hvYDWprgnVuhqghueNP4C9pfdBgTkhof\nzvHqBg6V17k2JjeRb2mnTWMnDJpgjOxwleR0pPIwPxtZwp5jp9mti9+aE9mSMFaKyLNAiIjcDnwO\nvGrD57KBkSIyXES8gRuAVWft8xGQJiKelkQ0FdinlPqFUipKKRVr+dw6pdTNNn5P3aMUrP4ZHP4G\nFjwDw6Y79HRup8OYkDQ97vw71ueVEkg9A6t2wEgX3Y6yGnMV+IUyq3Ytvl4m/cxvzanOmzCUUn8E\nVmP8sJ8EPK6U+psNn2sB7gc+w+h0ekcplSsiy0RkmWWffcCnwC4gC3hZKbWnu99Mj2T+FXL+BTN/\nDpOWuCQEl7OMCYkJNhEV6qfXY1hk5JWxaEAh0tbiuvqFlZcvTLwB74NruX6sP6tyjumV+ZrTdJow\nLOsgvlBKfaKU+plS6kGl1Ce2HlwptVYplaCUilNKPW7Z9oJS6oUO+zyllBqrlBp/rkSklMpQSs3v\nyjfVZbkfwpe/hfGLYNYvHHoqt2YZEyJFm0mNC2djQQWtbf27jmFtp10QsA+8gyBqiqtDguRbobWJ\nu4KzqGtq5WNd/NacpNOEoZRqBTxEpI8MTjqH+lPw0X3GD4KFz/bt9tnz6TAmJHVkODUNLew+1r/v\nkVvbacfWZ8GIme7RMTdoLERdQNShdxg1MFAXvzWnsaWGUQ3sFJEXReQv1i9HB+Y0/gNgyb/ghreM\ny/3+zDompHA90+OMOUn9vY6RcaCM8d4l+NYdc1131LkkpyPlB/hpQgU7i6vZ088Tu+YctiSM1cDv\nMWoMuR2++o642RDYww6rvsIyJiScasYMCe7X6zGs7bS3tLfTurh+0dH4a8E7iEvOfIqPp4kV2foq\nQ3M8W4rerwD/BDZYvv5p2ab1RR3GhKTFh7HtSCVnmlpdG5OLWNtpLzLtNDrIzLYMOHAS7wCYsAjv\nvFVcNy6QD3ccp75JF781x7JlpfcMIB94BaOd9oCIpDo6MM1FOowJSY0Pp6m1ja1H+ue484y8Mnxp\nZHDlNve6urBKSYeWM9xj3kZtYwurd55wdURaH2fLLam/AlcopVKVUtMxVmX/3bFhaS7TPiZkHVNi\nQ/HykH7bXptxoJTrBhxGWhvdq35hNTQJBk8k5vC7jIwI0GsyNIezJWF4K6X2Wl9Y1k64QauI5jBx\nc6C2BP+qAyTHhPbLwndtYwtZh05xddA+8PSDYW56UZ2SjpTs4f4xteQcrWLv8dOujkjrw2xJGNtF\n5AURSbN8PQ/scHRgmgtZHz1asJ60+HByj5/mVF2Ta2NyMqOdVjG+PguGz3DfDroJi8HLn3mNn+Gt\ni9+ag9mSMJYBhRiTYx+x/P4eRwaluVhIZPuYkNSR4SjV/8adZxwoY7R3GX41h92zfmHlGwLjrsFn\n30quGRfCB9uP9dsmBc3xbH2A0p+UUguUUguAPzsyIM1NWMaETBzkQ5CPZ7+qY7S300YUGBvcOWGA\nsfK7qZZ7wnZS09jC6l165bfmGLYkjPVAQIfXAcA6x4SjuQ3LmBDP4i1Ms4w77y+s7bQzPXYaz0QZ\nMMLVIXUueiqEj2L4kfeIi9BjzzXHsSVh+CmlaqwvLL+3dcS51lt1GBOSFh9O0al6iirqXR2VU2Tk\nleFNM0MrsyH+UvcfFyNiFL+PbeXHYxvZXlTF/pO6+K3Zny0Jo15EJllfiEgioB/43NdZx4QUrCfV\nOu68oH+t0LcXAAAgAElEQVRcZWQcKOXqsCJMzfXufzvKauIN4OHNlc2f4+1hYkXW0fN/RtO6yJaE\n8TPgAxFZLyIZwPvATxwaleYe4mZDyW7i/OoYHOzbL+oYdY0tZB+q5Nqg/eDhDbFprg7JNgFhMOYq\nfPe+y4JxoazcXqyL35rd2TIaZAswBiNxPAiMUUplOTowzQ1YxoTIoa9IjQ9nY345bX183PnGggqa\nWtuY0JANMReCT6CrQ7Jdcjo0VLNsYC6nG1pYu1uv/Nbs6wcThoikiMggAKVUIzAOeAx4UkTMTopP\nc6UOY0LSRoZRWd/M3hN9+974+rxSRnhXEVB1oPfcjrKKnQGhw4k7+j4jwnXxW7O/zq4wXgJaAEQk\nDfgT8A7QaHlP6+s6jAlJHdH3x51b22nTB+YbG0Ze6tqAuspkguRbkSMbWDZesfVIJQdKas7/OU2z\nUWcJw1MpZV2tdQPwklLqbaXUL4BRjg9NcwuWMSEDGwpJGBTYp+sY1nba2R67IDgSIka7OqSuS7wJ\nxIOrWr/Ay0P0VYZmV50lDA8R8bD8/mK+u/bCpgV/IjJPRPJEJF9EHv2BfWaJSI6I5IrIV5ZtviKS\nJSI7Ldt/a8v5NAdoHxNiTK/NPnyKhua+WUzNyCvDkxaiKrcYwwbdvZ32XIIGwajL8ct9myvGhrFy\n+7E+++elOV9nP/jfAdaLyPtAE/ANgIjEAee9zrUkm2eBy4GxwI0iMvasfczAc8ACpdQ4YLHlrUZg\njlJqEpAIzBORaV35xjQ76TAmJC0+nIbmNrYXVbo6KofIOFDK/AHHMTXV9L76RUfJ6VBfzo8HH6D6\nTDOf7NHFb80+fjBhKKV+B/wSWAGkKaXaLG95YVtb7RQgXylVqJRqshxn4Vn7LAVWKqWKLOcstfyq\nlFK1Hc7nBfTt9hx3FjcHjmxkSrQ/Hibpk3UMazvtYvN+EA8YPtPVIXVf/MUQHEXCsZXEhvmzfIte\nk6HZR6e3lpRSmUqpd89a6b1fKbXVhmNHAh3/Ty22bOsoAQgVkQwR2SYit1rfEBEPEckBSoEvLO29\nmitYxoQElWwlMdpMZn7fG0Robaed1JBtjNrw68WNgCYPSLoZKVzPXRM8yTp8ivxSXfzWes7W4YOO\n4gmkYDyUaS7wmIgkACilWpVSiUAUMEVExp/rACJyt4hsFZGtZWVlzoq7f+kwJiQ1PpzdxVVUn2l2\ndVR2lZFXSox3DYGnct3zYUldlXQzANewzlL81lcZWs85MmEcA6I7vI6ybOuoGPhMKVWnlCoHvgYm\nddxBKVWFMQBx3rlOopR6SSk1WSk1OSIiwm7Bax10GBOSFh9Om4LNhX3nKkMpRUZeGemDDhkbenP9\nwsocDfGX4J+7nHljInh/e7Eufms95siEkQ2MFJHhIuKN0Zq76qx9PgLSRMRTRPyBqcA+EYmwLg4U\nET/gUmC/A2PVziduDpTsJjG0EX9vjz5Vx7C2087x3AUBETB4oqtDso+UdKg5wb2RhVTVN/NZ7klX\nR6T1cp2t9I4UkX9ZZkg9IiKeHd57/3wHVkq1APcDnwH7gHeUUrkiskxElln22Qd8CuwCsoCXlVJ7\ngCEYHVq7MBLPF0qp1d3/NrUeizPaa72PfM3U4QP61HqMjLwyTLQRU7kZ4i42FsD1BQnzIGAgY058\nQMwAf97aotdkaD3T2d+MV4HNwMPAcIwf4KGW92x6QIBSaq1SKkEpFaeUetyy7QWl1Asd9nlKKTVW\nKTVeKfU3y7ZdSqkkpdREy/bfdeeb0+zIOiak0JheW1hWx/GqM66Oyi4yDpRyRdhJPBpO9b7V3Z3x\n8IKkm5CDn3PHJF+2HDpFQVnt+T+naT+gs4QxUCn1jFJqq1LqXuAV4GsRGY5uce1/OowJSYvvO2NC\n2ttpQ/YD8u1Cxb4i6RZQrSwyZeBpElbold9aD3SWMHxExMf6Qin1D4yrjS+AwQ6OS3NHljEho+Qo\n4YHefSJhWNtpk5q2QWSyMSa8LwmLg9gZBOYu57IxEby3rZjGFl381rqns4TxGnBhxw1KqU8xitd5\njgxKc1OWf32L5bZUZn4FSvXui82MvFKGep8hqGJn3+iOOpeU26DqCMtiiqmsb+bz3BJXR6T1Up2t\n9H5KKZVxju1bgSscGZTmpkIsA/ks6zHKaxs5UNJ774m3t9MOPoyoNuNxrH3R6PngF8qEkg+JCvXT\nAwm1buu0HUREBolIorVDSkTCReR3QL5TotPcz4jZcGQjqbHGg4V6c7dUQZnRTnuJ1y7wNRu3pPoi\nL1+YdCOyfw23JwaxsaCCQ+V1ro5K64U6a6t9ANgL/B+wRURuw7gVFYqxXkLrjyxjQiKrcxgRHtCr\n6xjr95cBitjKTcb3ZfI472d6reR0aGvmeq9v8DAJK7L1VYbWdZ1dYdwLjFJKXQBcB7wIXK6UekAp\nVeyU6DT3c9aYkM2FFTS3tp3/c24o40Apc8PK8agv7bv1C6uBoyF6KkG5b3HJ6Aje21pMU0vv/HPT\nXKezhNFgGdeBUuowkKef5a11HBOSGh9OfVMrOUerXB1Vl1nbaZeEWvo3+sL8qPNJToeKgywbXkJF\nXRNf7NXFb61rOksYUSLyF+sXMPis11p/ZRkTMn1gCyaBzIO977aUtZ02pXkbDJ4AQf2gU3zc1eAT\nzKSyVUSadfFb67rOEsYvgNwOX2e/1vqruDkABJ/YwIQoc6+sY2TklRLh3Uhw2ba+fzvKyjsAJizG\ntPcj0hNDyMwv50iFLn5rtuusrfaVzr6cGaTmZgZPBP+w9lXfO45WUdPQe8adf9tOW4S0tfSfhAGQ\nfCu0NHCD32ZL8VuPPdds10emrGlOZTLBiFnGXKm4MFrbFFmHTrk6KptZ22nn+uwB7yCImuLqkJxn\naCIMmUTw3reYMyqCd7ce1cVvzWY6YWjdYxkTkuJ3El8vU69aj5GRZ7TTDq/aBCNmgqe3q0NyruR0\nKNnDPfFVlNc28eU+XfzWbKMThtY9ljEhPkcyuCB2QK+qY2TklTEnrBrPmuL+dTvKasJi8PInuXwV\nQ0N8eUsXvzUbnTdhWFZ3PyIiz4nIS9YvZwSnubEOY0LS4sM5UFJL6ekGV0d1XnWNLWQdOsWNAyzP\n4+oP7bRn8w2Gcddi2vM+NyeF8c3Bco6eqnd1VFovYMsVxkfAICAT+LLDl9bfxc2BIxtJs4wJ2VDg\n/lcZ1nbayS07IHwUmGNcHZJrpKRDcx1LA7MxCXrlt2YTWxJGgFLqIaXUW0qpt61fDo9Mc38jZkNL\nA2OacjH7e5F50P2f852RV8oA7xbMpVn983aUVdQFEDEG8963mD1qIO9sLe61K/Y157ElYXwiIpc5\nPBKt97GMCTEdWkdqXDgbC8rdety5tZ321iHFSGtj/7wdZSViXGUc385dCXWU1TTy5b5SV0eluTlb\nEsYy4FMRqRWRUyJSKSI29VCKyDwRyRORfBF59Af2mSUiOSKSKyJfWbZFW54lvtey/ae2f0ua07SP\nCckgNT6cE9UNFLrxFFRrO+08nz3g6QfDUl0dkmtNXAIePkypXM3gYF+98ls7L1sSRjjgBYQAEZbX\nEef7kIh4AM8ClwNjgRtFZOxZ+5iB54AFSqlxwGLLWy3AQ0qpscA04L6zP6u5CcuYkIuGGLcz3Llb\nyminhbjTm2H4DGPsd3/mPwDGLsC0+x2WJkfw9cEyXfzWOtXZePORlt+O+4Gv85kC5CulCpVSTcAK\nYOFZ+ywFViqligCUUqWWX08opbZbfl8D7AMibf2mNCeyjAmJqtxC9AA/t54rlZFXxkXhNXhVFfbv\n+kVHybdCQzU3h+QA8M5WvfJb+2GdXWFYbyE9e46vZ2w4diTQ8f++Yr7/Qz8BCBWRDBHZJiK3nn0Q\nEYkFkoAtNpxTc7bvjAkJZ1NhBS1uWDy1ttMuDTtobNAJwxA7AwaMYMC+5cxKiODt7KNu+eenuYfO\nZkndafl1xjm+LrLT+T2BFOBKYC7wmIgkWN8UkUDgfeBBpdTpcx1ARO4Wka0isrWsrMxOYWk2s44J\nKVhHalwYNQ0t7D5W7eqovmeTpZ12SssOCI2FASNcHZJ7EDGuMoo2cufoZkprGlm3Xxe/tXOzaaW3\niIwWkWtFZKn1y4aPHQOiO7yOsmzrqBj4TClVZ3n2xtfAJMs5vTCSxZtKqZU/dBKl1EtKqclKqckR\nEectrWiOEDcH6kqZEWz8oHHHOsb6vFJCvNsILd1sPLtbxNUhuY/Em8DkyfTqtQwM8tHFb+0H2bLS\n+9fAS8ALGAXsvwGLbDh2NjBSRIaLiDdwA7DqrH0+AtJExFNE/DEe/bpPRAR4BdinlNLP3nB3ljEh\nIScyGTc02O3mSlnbaW8ZegJprtO3o84WOBBGXY5p13KWpgwi40AZx6rOuDoqzQ3ZcoWxBJgNnFBK\n3YJxBRBwvg8ppVqA+4HPMIrW7yilckVkmYgss+yzD/gU2AVkAS8rpfYAqcAtwBxLy22OiFzR9W9P\nc4qzxoRsP1JFfVOLq6NqZ22nvcJ3D3h4Q2yaq0NyP8m3QX0FN5uNR928rceea+dgS8I4o5RqBVpE\nJAg4CQyz5eBKqbVKqQSlVJxS6nHLtheUUi902OcppdRYpdR4pdTfLNsylVKilJqolEq0fK3t+ren\nOY1lTMiM2ECaWtvIPlzp6ojaWdtpR9ZsgZgLwSfQxRG5objZEBJN+IHlXDQygnd08Vs7B1sSxg7L\neolXga0YVwL62d7ad8XNgZYGLvDIw9vD5FZ1jIy8MqaFN+BVsR9GXurqcNyTyQOSboHCDO4YK5w8\n3dCeaDXNqtOEYakl/EYpVaWUehajm+kepdT32l+1fm7YdPDwxudIBinDQt1mPYa1nfaWcN1Oe15J\nN4GYSKv9hAhd/NbOodOEoYzBQF90eJ1vXVCnad/hHQDRU6FgPWkjw9l74jQVtY2ujqq9nXZq2w4I\nttRatHMLiYL4S/DY+RZLUgazPq+U47r4rXVgyy2pHBFJcngkWu8XNwdK9jBzqHHve2OB66fXZhwo\nJdhbEVaywRg2qNtpO5ecDjUnSA8/SJvSK7+17+psNIin5bdJQLZliOB2EdkhIvoqQ/s+y5iQsWe2\nE+Tr6fI6hrWddmlkGdJYo29H2SJhLgQOIuLACmaMDOft7KO0trnvBOKuaG5tY3dxNa9vOszP3s7h\nppc3s/NolavD6lU8O3kvC0gGFjgpFq23s4wJMRWuZ3rcXXxz0Bh3Li76V31BWS3FlWe4ckguiIex\nIl3rnIeXsZBvw9+4Y96j3P5BA18dKGXO6EGujqzLSk83sL2oih1FlewoqmLXsSoamo2r34ggHwAW\nv7iJx68ez+LJ0Z0dSrPoLGEIgFKqwEmxaL2dyWQs4itYR1rqo3yWW0LRqXqGhZ132Y5DWLt8RtVu\nNuorviEuiaPXSb4FMv/CRfWfEx6YwvKso26fMBpbWsk9fpodHRKEdfGhl4cwbmgIS6cMIynGTFKM\nmUizH5X1zdz/1nYefm8XucdP86srx+DlYdPwi36rs4QRISL/8UNv6hXY2jnFzYY97zE71PhhnZlf\n7tKEMTm8Ge/S3TDnMZfE0CsNGAHDZ+KR8y8Wp1zFS98c5mR1A4ND3GMcvFKK49UNbD9iJIYdRyvJ\nPXaaJsu6kUizH4kxZu5IG05SjJmxQ4Lx9fL43nEGBHjz+h1TePKT/byceYi9J07z3E3JhAf6OPtb\n6jU6SxgeQCCWKw1Ns4llTEjkqc0MDRnHhvxybppq0zpPu7K20z6VUAC16PpFV6Wkw3t3cNugQzzf\nBu9uPcoDF488/+cc4ExTK7uPVbOjqJLtlquH0hqjA8/H08SkKDO3p8Zarh5CGRRse2Lz9DDx6/lj\nmRAVwiPv7WLB05m8cEsKE6PMjvp2erXOEsYJpdTvnBaJ1jdYxoRIwTqmx8/k3/tKaG1TeJic++8O\nazvthSoHAiKM+opmu9HzwW8Agw6uIC3+flZkH+XHs+Md/ueolKLoVH17YthRVMW+E6dpsRTeh4X5\nMz0ujORhoSRFhzJ6SJBdbiMtTIwkLiKQe97YxqIXNvGHayZwXUpUj4/b15y3hqFpXRY3B7a+ykXz\nAnlvWzF7j59mQpRz6wcZB0oJ9BYiSjKNzh+TvjfdJZ4+kLgUtrxA+rxHuGtlOd8cLGPWqIF2PU1t\nYwu7jlax42iVcYvpaBWn6poACPD2YFK0mXtmjiApOpSkGDNhPb1d1NoCZfvg2HaoLYXJd0BAGADj\nI0P4+IE07ntzOw+9u5M9x6v55RW6rtFRZwnjYqdFofUtcXNg83PM8MkHYENBuVMThrWd9saoU8jx\nU/p2VHcl3wqbnmF245eEBYxjeVZRjxJGW5uisLzW0rlkFKcPlNRg7dqNiwjg4tEDSYoxkkPCoKCe\nXdEoBacK4fgOOLbNSBIndkJLh8WIW1+Ba16EETMBo67xxp1TeGLtfl7dcIh9J07z7NLknieqPuIH\nE4ZS6pQzA9H6EMuYkNAT3zBq0GVsyC9n2cw4p52+oKyO4sozzI/MBaS9rqJ1UcQoiJ6GZ84bLEp5\ng5czD1N6uoGBNtYIquubySn+9sohp6iS0w3GFONgX08SY0KZO24wycNCSYwyE+Lv1bN4a04aSeH4\ndiNBHN8BZyxDMD39YMgkmHw7DE2GyGRoqoX37oTXF0LagzD7V+DhhaeHif+8aizjI4P5xcrdLHhm\nAy/eksL4SN1l19kVhqZ1T4cxIanxS3lzyxEamlvP2aniCBl5xoOcRtdlGT8YLLcctG5ISYcP7+W2\ni47zYpvi3W3F3Dc7/nu7tbYpDpTUtF85bC+qpKCsDjAW148aFMSVE4eSFGMmOcbMiPBATD25emio\ntlw5dEgOpy3PZxMPGDgWxiww/vwjUyBiDHic48fdPV/Bp49C5l/h0Ndw3cvtT2O8NjmKhEFB3P36\nVq57fiNPXjeBa5L6d11DJwzNMeLmwJe/Zc60Vl7d0Mb2I5VMjw93yqkz8spIilD4nNwOFz3slHP2\nWWOvhk8eZUj+21w44k6WZxVx78w4KuubyDla1V6c3nm0irqmVsC4rZMUbeaapEiSY0KZGG0m0KcH\nP2qaG6Bkz7e3lY5tg4qD374/YIQxtj4yxUgQgyeCt79tx/YOgAVPQ9zF8PFP4IWL4Mo/w6QlgFHX\nWGWpa/zs7Z3sLj7NL68YjWc/rWvohKE5hiVhTG7dhacplMz8cqckDGs77R9GFUJNm/E4Vq37vP1h\n4mLY/ga3zvsZ975fQeof13GiugEAD5Mwdkgw16VEGW2t0aEMC/Pv/ur+tlYoy7NcNViSQ0kutFke\nyBU4yEgMk5YYt5aGJoH/gJ5/n+OuNo678i744G4o+BKu+BP4BhMe6MO/fjSVx9fs49UNh9h/8jTP\nLE1mQIB3z8/by+iEoTmGZUyIb9HXJMWkO22ulLWdNo0c8DUb/+LUeiY5HbJf5tLmDGaPmoy3p4nb\npseSFBPKhMgQ/Ly7eatRKag68u2Vw/EdcDwHmo1bWfgEGwlh+gPGD/OhyRA81HEDJM3RkL4avvkz\nfPUkHN0C170KUSl4eZj4zYJxTIgM4Rcf7OaqpzP7ZV1DJwzNMTqMCUlN/A/+vi6f6vrmnhc2zyPj\nQCn+3iYGlmYaVzkm59RN+rQhE2FoEp45b/Davfd2/wd2bdm3Vw3W4nS9ZaKxh49xnqSbv721NCDO\n+e3QHp4w6+cw/CLjauPVy4xieOqDYDJxXUoUIwdZ12ts5I/XTWRhYqRzY3QhhyYMEZkH/B1j1fjL\nSqknz7HPLOBvgBdQrpSaadn+KjAfKFVKjXdknJqDxM2BPe9xaVg5f1OwqbCceeOHOOx01nba66NO\nI8dLdDutPSWnw+oHoXgrRF9w/v0ba4yrhfYEsQOqLQ9kEpPxXJJRl1s6llKMIrWnG93iGXYhLPsG\nPn4QvvwtFK6Ha16C4CFMjDKz6v407ntrOz9dkcPu4moevbx/1DUcljBExAN4FrgUKMYYkb5KKbW3\nwz5m4DlgnlKqSEQ6Nnn/A3gGeN1RMWoOFme0s46u30qA9ygy8x2bMKzttAuiLf+LxeulRHYzYRF8\n9ivY/o/vJ4yWRktRevu3Vw5leYBlgYV5GERNhql3GwliyKTe8Vx1v1BY/A/Y8S/45BF4fjosfBZG\nX0FEkA9v/mgqv1+9l5czD7Hv5GmevrHv1zUceYUxBchXShUCiMgKYCGwt8M+S4GVSqkiAKVUqfUN\npdTXIhLrwPg0RwseChGj8Shcz7QR09mQ79gHKlnbacfWZ8HgCRA02KHn61d8gmD8tbDnfbjgR1C6\n79uOpZI90GqsziYgwkgK4641bisN7eVtzSLG9N6YafDe7bDiRrjgLrjsv/Hy8uO3C8czPjKEX324\nhwXPZPLSLZMZOzTY1VE7jCMTRiTQ8XFdxcDUs/ZJALxEJAMIAv6ulNJXFH1J3BzIfoWLLgrky/2l\nFFfWExVqY8tjF311oIyJESZ8j2cZhVLNvpLTYccb8NIs47V3oFGUnnbvt7eWQqL65lMNw0fCj76E\nL38Hm56BIxtg0aswcAyLJ0eTMCiIe97YxrXPb+B/Fk1iwaShro7YIVx9080TSAGuBOYCj4lIQlcO\nICJ3i8hWEdlaVlbmiBi1noibA62NXBxQCMBGB11l1DW2sKXwFDcPPGy0YOr6hf1FTYar/g5XPw8/\n3gKPFsFtq+HS3xltqebovpksrDx9YO7jcNP7UFdmJM7sl0EpJkWb+fiBNCZEhvCT5Tt4Yu0+Wizj\n1vsSRyaMY0DHx1hFWbZ1VAx8ppSqU0qVA18Dk7pyEqXUS0qpyUqpyRERET0KWHMAy5iQyIpNRAT5\nkOmg9lprO+0M2QneQcZKc82+RCDlNmMo4cDR/bcDbeQlcO9GiE2DNQ/Bipug/pSlrjGNW6YN46Wv\nC7n9H9lU1Te5Olq7cmTCyAZGishwEfEGbgBWnbXPR0CaiHiKiD/GLat9DoxJczbvAIiZhhSuJy0+\nnA355bQ54BnR1nbawWWZxiA5D8e272r9XOBAWPouzH0CDn5uFMQPfY23p4n/vno8/3PdRLYUnuKq\nZzLZd+K0q6O1G4clDKVUC3A/8BlGEnhHKZUrIstEZJlln33Ap8AujGeIv6yU2gMgIsuBTcAoESkW\nkTsdFavmYCNmQ8keLo5SVNQ1kVdSY9fDW9tpr4uuQ6qP6ttRmnOYTHDhfXDXl0Y9558L4N+/hdZm\nrr8gmrfvmUZTSxvXPreR1buOuzpau3BoDUMptVYplaCUilNKPW7Z9oJS6oUO+zyllBqrlBqvlPpb\nh+03KqWGKKW8lFJRSqlXHBmr5kBxcwBIM+0GsPuqb2s77cKg/cYG3U6rOdOQScYQw6SbIfMv8Oo8\nOHWIpJhQPn4gjXFDg7n/rR384ZN9tDrg6tqZXF301voDy5gQ84lM4iIC7F7HsLbTjqvPgvBRYI6x\n6/E17by8A2DhM8a6jfKD8MIM2PUOA4N8eeuuadw0NYYXvyrktteyenVdQycMzfHax4SsJy0ujC2F\np2hqsV8HyVcHyhgX4YXfsc36dpTmWuOugXszYdA4Y7TIynvwbq3j8Wsm8IdrJ7C5sIIFz2xg/8ne\nWdfQCUNzjrg5UFfK3IgKzjS3sqOo0i6HrW+ytNMOPgqtjUYHi6a5kjkGblsDs34Bu98xrjaObePG\nKTGsuPtCGppbufa5jazZdcLVkXaZThiac1jGhCS37MAk9qtjWNtpZ5pyjKeqxUy3y3E1rUc8PGHW\no3DbWmNd0CuXQeZfSYkOYfUDaYweHMR9b23nj5/u71V1DZ0wNOewjAnxPfIVE6PMbCiwzwK+9Xml\n+Ht7MLhsAwyfAV62PT5U05zCOsRw9JXw79/AG1czkEqW3z2NG6fE8HxGAXf8I5vq+mZXR2oTnTA0\n54mbA0c2Mmt4IDlHq6hp6NlfEms77cKYRkynCnT9QnNPfqGw+J/Gk/2Ks+H56fgUfM4frp3AE9dM\nYGNBOQuezSTvpH3bzR1BJwzNeSxjQi4LOkRrm2JL4akeHc7aTnt1sLWdVicMzU2JQPKtcPdXEBIJ\ny2+AtQ+zNDmCFXdPo76plWue28Anu927rqEThuY8ljEhCXVZ+HqZetxea22nnXhmK4QOh7A4e0Sp\naY4TkWAMMZx2H2S9BP83hxS/ElY/kMaowUHc++Z2nvrMfesaOmFozmMZE+J56CumDA/rceH7qwNl\njA73xq94g7660HoPTx+Y98R3hhgOyvsXK+6ayg0XRPPs+gLu/Gc21Wfcr66hE4bmXHFzoGQPl0a3\ncbC0lpLTDd06THs77dDjxjOgdcLQehvrEMNhqbDmIXzeT+cP8yL5/dXjyTxYztXPbuCgncfo9JRO\nGJpzjTDaa2d7Gc/R6u5VhrWddpbHLvDwNiaHalpvEzgQbnoPLnscDnyGvJDGzYOKWH73NGoaWrj6\n2Q18uuekq6NspxOG5lyWMSGRFZsYEODd7TpGRl4Z/t4eDC3fADEX9o5HfmrauZhMMP1++NG/wdsf\n/nkVFxQ8w+ofTyV+UBDL/rWNP3+e55Apz10O1dUBaP2MZUyIFGaQOiKUDfnlKNW1vwhKKdbnlXLl\nsDZMZftg5KUOClbTnGhootFFlXQTfPNnBr9/Ne8sHsz1k6N4el0+P3p9q8vrGjphaM5nGRNy5aBK\nSk43UlBW26WPW9tpr9XttFpf4xMIC5+FRa9B+UF8Xp7JHxPy+O+F4/j6QBlXP7uB/FLX1TV0wtCc\nzzImZBq7AMg82LXbUtZ22kkNWyE4EiJG2zc+TXO18ddahhiORVbexS0nn2TFbeOpaWjm6mc38nmu\na+oaOmFozhc8FCLGYD7+DcPC/Mns4nO+vzpQRkK4D/7F3xjPvujLz5HW+i9zjDGLauajsOttJn+6\nkE8WBRAXEcDdb2zjL18ccHpdQycMzTXiZsORjcwcEcTmwgpaWm0bd25tp10aWQaNp/XtKK1v8/CE\n2f4dg4oAAA6WSURBVL8wpt+2NBHxzlW8NzGbxclD+d8vD3L3G1s53cMRO12hE4bmGpYxIfNDDlHb\n2MLO4mqbPmZtp53juQvEA0bMcmiYmuYWhk03blGNvhKvdb/hfxp+w//MHcj6PGtdo2t1wO7SCUNz\nDcuYkIkN25EujDu3ttNGVWyA6KngG+LgQDXNTViHGF71v8jRLK7PXsKaebVU1zdz/YubqGtscXgI\nDk0YIjJPRPJEJF9EHv2BfWaJSI6I5IrIV135rNaLWcaE+BZ9xfihITatx1BKkXGglHmxJkwnd+pn\nd2v9jwikpBvtt8FDGb3uLr6a8Al/XJhAgM//b+/eg6ys7zuOvz+7LCCoIbKIhJvLCq14Da5bx3gh\nG0W8VBNHI5oMmTaTlDSa2Na2OrHWdiaTGsc0M6mJRTSaS+vYmhbGWDExXBQssFyEBQRh5RrjLnJR\nlC4sfPvH73fg8bDLniWcPc9z9vua2dnn/J7nOfv77m/3fM9zOd9fn6L/+KIlDEmVwKPAtcB44HZJ\n4/O2GQT8ELjRzM4Bbi10X1cGahugZTWTRsHyLbu6fIfUvOMDtu7cx80fWx8a/PqF660OFzH8c05e\n8QRXv3o7tBX/tFQxjzDqgQ1m1mxm+4FngJvytrkD+IWZbQEws5Zu7OuyrrYBgEn913LgoLF407HL\nnc95I/x5fHJ/Iww8PXxq3Lneqk8/mPwduOM/YNw1PVLtoJgJYziwNfF4W2xLGgd8XNJcSUslTe3G\nvgBI+qqkRkmNra2tJ6jrrkcMPQ8GVHPW+4vp26eCBV18HmPe+lbGVvdn4JZ54XRUhV+Cc45xk+Cq\nv++RH1Xq/7g+wEXA9cA1wN9JGtedJzCz6WZWZ2Z1Q4YMKUYfXbFUVMCYiVS+NZf60ce+jpG7nXbK\niJ2wb6efjnKuBIqZMLYDIxOPR8S2pG3AbDP7wMx2APOBCwrc15WDWCbkj4ft5o3fvc+OvW0dbpa7\nnfaqvqsAHa5665zrOcVMGEuAsZJqJPUFpgCz8raZCVwmqY+kAcAfAWsL3NeVg1gm5HKFMiELN3b8\nqe+561o5qaqSkTsXwvAJMHBwj3XRORcULWGYWTtwJzCbkASeNbPVkqZJmha3WQu8CKwEFgMzzKyp\ns32L1VdXQrFMyLAdr3Fq/z4dXsfI3U47qaaKiu1L4SyvTutcKRT1xl0zewF4Ia/tsbzHDwMPF7Kv\nK1O1DWjJDK6s+VtejeXOlagPlbud9ttjt8GWQ379wrkSKfVFb+fCaamDbdw0eDPbd+9j87sffmT1\n3HXh7reL9i+F/oPCKSnnXI/zhOFKL5YJubh9OcBRd0vNXddCbfUABm6dGy6SV1SWoJPOOU8YrvRi\nmZBTf/sKwwed9JG6UrnbaW8b9R7sfcdPRzlXQp4wXDrUNqCWNUwebSzc+C4HY53/3O20V/ddFbbz\n+lHOlYwnDJcOsUzI9QPXsWffAVb/NpQ7z91OO3rnQjjjPDjljFL20rlezROGS4dYJmT8vqUAh++W\nmru+hYaa/lRsW+Sno5wrMU8YLh1imZD+W+Zz9tCBLNiw4/DttLee1gyH2j1hOFdinjBcesQyIZ8b\nvoclm3YxO050X9e+DPqeEiZMcs6VjCcMlx6xTEhD3yb2tx9i+vxmaqsHcPLWeTDmSqisKnEHnevd\nPGG49IhlQmp2L6KqUuz+8AC3jN4He7b46SjnUsAThkuX2gYqt/4v9SMHADCpn99O61xaeMJw6VLb\nAAfbmPqJ7Zx+Sj/O3P0aVP8BDBpV6p451+t5wnDpEsuETOq3hgV/dQmVmxfCWK9O61waeMJw6dJ3\nAIy6BG2cQ9XW1+Bgm5+Oci4lPGG49KltgJbVsPyn0OckGHVpqXvknMMThkujWCaENTOh5nKo6l/a\n/jjnAE8YLo1imRDAb6d1LkU8Ybj0qag4/CE+TxjOpUdRE4akyZLWSdog6d4O1k+UtEfSivj1QGLd\nNyU1SVot6e5i9tOl0KXfgE/fD4NrS90T51xUtDm9JVUCjwJXA9uAJZJmmdmavE1fMbMb8vY9F/gK\nUA/sB16U9LyZbShWf13KDDs/fDnnUqOYRxj1wAYzazaz/cAzwE0F7ns2sMjMPjSzdmAecHOR+umc\nc64AxUwYw4GticfbYlu+SyWtlPQ/ks6JbU3A5ZIGSxoAXAeM7OiHSPqqpEZJja2trSey/8455xKK\ndkqqQMuAUWa2V9J1wH8DY81sraSHgJeAD4AVwMGOnsDMpgPTAerq6qxnuu2cc71PMY8wtvPRo4IR\nse0wM3vPzPbG5ReAKknV8fETZnaRmV0B7ALWF7GvzjnnulDMhLEEGCupRlJfYAowK7mBpDMkKS7X\nx/68Gx+fHr+PIly/+Lci9tU551wXinZKyszaJd0JzAYqgSfNbLWkaXH9Y8AtwNcktQP7gClmljut\n9JykwcAB4OtmtrtYfXXOOdc1HXl9zr66ujprbGwsdTeccy4zJC01s7pCtvVPejvnnCtIWR1hSGoF\nNh/n7tXAjhPYnVIql1jKJQ7wWNKoXOKA3y+W0WY2pJANyyph/D4kNRZ6WJZ25RJLucQBHksalUsc\n0HOx+Ckp55xzBfGE4ZxzriCeMI6YXuoOnEDlEku5xAEeSxqVSxzQQ7H4NQznnHMF8SMM55xzBekV\nCUPSk5JaJDUl2k6T9CtJb8bvH0+suy9O+rRO0jWl6XXHOonlQUnbExNRXZdYl8pYJI2UNEfSmjhJ\n1jdje+bG5RixZHFc+ktaLOn1GMs/xPYsjktnsWRuXCDMMSRpuaTn4+OeHxMzK/sv4ApgAtCUaPsu\ncG9cvhd4KC6PB14H+gE1wEagstQxdBHLg8A9HWyb2liAYcCEuHwKobjk+CyOyzFiyeK4CDg5LlcB\ni4BLMjouncWSuXGJ/ftLQk295+PjHh+TXnGEYWbzgZ15zTcBT8flp4HPJtqfMbM2M3sL2ECYDCoV\nOomlM6mNxczeNrNlcfl9YC1hvpTMjcsxYulMmmMxixWkCS+yVYCRzXHpLJbOpDYWSSOA64EZieYe\nH5NekTA6MdTM3o7LvwOGxuVCJ35Km7viRFRPJg5NMxGLpDOBTxLeAWZ6XPJigQyOSzz1sQJoAX5l\nZpkdl05igeyNy/eBvwEOJdp6fEx6c8I4zMJxXJZvF/sRMAa4EHgbeKS03SmcpJOB54C7zey95Lqs\njUsHsWRyXMzsoJldSJjDpl7SuXnrMzMuncSSqXGRdAPQYmZLO9ump8akNyeMdyQNA4jfW2J7lxM/\npY2ZvRP/MQ4Bj3Pk8DPVsUiqIrzA/tzMfhGbMzkuHcWS1XHJsTClwBxgMhkdl5xkLBkcl08BN0ra\nBDwDNEj6GSUYk96cMGYBX4rLXwJmJtqnSOonqQYYCywuQf8KlvujiT5HmBMdUhyLJAFPAGvN7HuJ\nVZkbl85iyei4DJE0KC6fBFwNvEE2x6XDWLI2LmZ2n5mNMLMzCRPR/cbMvkgpxqTUV/574gv4d8Kh\n5wHC+bwvA4OBl4E3gV8DpyW2/xbhzoJ1wLWl7n8BsfwUWAWsjH8sw9IeC3AZ4RB6JWHO9hXAdVkc\nl2PEksVxOR9YHvvcBDwQ27M4Lp3FkrlxSfRvIkfukurxMfFPejvnnCtIbz4l5Zxzrhs8YTjnnCuI\nJwznnHMF8YThnHOuIJ4wnHPOFcQThssESSbpkcTjeyQ9eIKe+ylJt5yI5+ri59wqaa2kOcX+WV30\nY5Ok6lL2wWWTJwyXFW3AzWl7oZPUpxubfxn4ipl9ulj9ca6YPGG4rGgnTEP5F/kr8o8QJO2N3ydK\nmidppqRmSf8k6QtxjoRVkmoTT3OVpEZJ62PtnlzhuoclLYmF6v4s8byvSJoFrOmgP7fH52+S9FBs\ne4DwAb8nJD2ct/0wSfPj3AxNki6P7T+KfTo8l0Ns3yTpO3H7RkkTJM2WtFHStEQf50v6ZZwT4TFJ\nR/2/S/pi/H2skPSvMebK+DttinEc9Tt3vVN33h05V2qPAislfbcb+1wAnE0oCd8MzDCzeoVJju4C\n7o7bnUmoKVQLzJF0FjAV2GNmF0vqByyQ9FLcfgJwroXy0YdJ+gTwEHARsAt4SdJnzewfJTUQ5mFo\nzOvjHcBsM/u2pEpgQGz/lpntjG0vSzrfzFbGdVvM7EJJ/ww8Rag31J/wiebH4jb1hLkRNgMvAjcD\n/5no69nAbcCnzOyApB8CXwBWA8PN7Ny43aBCftGu/PkRhssMCxVgfwJ8oxu7LbEwX0UboVRC7gV/\nFSFJ5DxrZofM7E1CYvlDYBIwVaE89iJCKYaxcfvF+ckiuhiYa2atZtYO/Jww6dUx+wj8Sbwmc56F\nOTUAPi9pGaG8xTmEF/+cWYk4FpnZ+2bWCrQlXuAXm1mzmR0klJS5LO/nfoaQ2JbEGD9DqOLaDIyR\n9ANJk4H3cA4/wnDZ831gGfDjRFs78c1PPO3SN7GuLbF8KPH4EB/9+8+vkWOEGdvuMrPZyRWSJgIf\nHF/3j2Zm8yVdQZgg5ylJ3wNeAe4BLjazXZKeIhxB5CTjyI8xF1dHMSUJeNrM7svvk6QLgGuAacDn\ngT/tblyu/PgRhssUM9sJPEu4gJyzifBOGeBGwsxq3XWrpIp4XWMMoWjbbOBrCqXLkTRO0sAunmcx\ncKWk6ngq6XZg3rF2kDQaeMfMHifMqDYBOJWQlPZIGgpcexwx1UuqiUn0NuDVvPUvA7dIOj324zRJ\no+ONBRVm9hxwf+yPc36E4TLpEeDOxOPHgZmSXiecqz+ed/9bCC/2pwLTzOz/JM0gnLZaJklAK0em\nweyQmb0t6V7C3AsCfmlmM4+1D6EC6V9LOgDsBaaa2VuSlhNKi28FFhxHTEuAfwHOiv35r7y+rpF0\nP+E6SwWhAvLXgX3AjxMXyY86AnG9k1erda4MxdNm95jZDaXuiysffkrKOedcQfwIwznnXEH8CMM5\n51xBPGE455wriCcM55xzBfGE4ZxzriCeMJxzzhXEE4ZzzrmC/D954tRFQ36REwAAAABJRU5ErkJg\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113f77a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+X/ARMqhhblObQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1140727b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#--------  sample\n",
    "# A function to select a random sample of size k from the training set\n",
    "# Input: \n",
    "#      x (n x d array of predictors in training data)\n",
    "#      y (n x 1 array of response variable vals in training data)\n",
    "#      k (size of sample) \n",
    "# Return: \n",
    "#      chosen sample of predictors and responses\n",
    "\n",
    "def get_samples(x, y, k):\n",
    "    n = x.shape[0] # No. of training points\n",
    "    \n",
    "    # Choose random indices of size 'k'\n",
    "    subset_ind = np.random.choice(np.arange(n), k)\n",
    "    \n",
    "    # Get predictors and reponses with the indices\n",
    "    x_subset = x[subset_ind, :]\n",
    "    y_subset = y[subset_ind]\n",
    "    return (x_subset, y_subset)\n",
    "\n",
    "sample_sizes = [100, 150, 200, 250, 300, 350, 400]\n",
    "r2s = {'ridge':{}, 'lasso':{}}\n",
    "\n",
    "for sample_size in sample_sizes:\n",
    "    X_sample, y_sample = get_samples(Xtrain.values, ytrain.values, sample_size)\n",
    "\n",
    "    r2s['ridge'][sample_size] = {'test': {'samples': [], 'std':None, 'mean':None}, 'train': {'samples': [], 'std':None, 'mean':None}}\n",
    "    r2s['lasso'][sample_size] = {'test': {'samples': [], 'std':None, 'mean':None}, 'train': {'samples': [], 'std':None, 'mean':None}}\n",
    "    \n",
    "    for i in range(10):\n",
    "        ridge = Ridge()\n",
    "        ridge.fit(X_sample, y_sample)\n",
    "        ridge_train_predictions = ridge.predict(X_sample)\n",
    "        r2s['ridge'][sample_size]['train']['samples'].append(r2_score(y_sample, ridge_train_predictions))\n",
    "        \n",
    "        ridge_test_predictions = ridge.predict(Xtest)\n",
    "        r2s['ridge'][sample_size]['test']['samples'].append(r2_score(ytest, ridge_test_predictions))\n",
    "        \n",
    "        lasso = Lasso()\n",
    "        lasso.fit(X_sample, y_sample)\n",
    "        lasso_train_predictions = lasso.predict(X_sample)\n",
    "        r2s['lasso'][sample_size]['train']['samples'].append(r2_score(y_sample, lasso_train_predictions))\n",
    "        \n",
    "        lasso_test_predictions = lasso.predict(Xtest)\n",
    "        r2s['lasso'][sample_size]['test']['samples'].append(r2_score(ytest, lasso_test_predictions))\n",
    "        \n",
    "        \n",
    "    r2s['ridge'][sample_size]['train']['mean'] = np.mean(r2s['ridge'][sample_size]['train']['samples'])\n",
    "    r2s['ridge'][sample_size]['train']['std'] = np.std(r2s['ridge'][sample_size]['train']['samples'])\n",
    "    \n",
    "    r2s['lasso'][sample_size]['train']['mean'] = np.mean(r2s['lasso'][sample_size]['train']['samples'])\n",
    "    r2s['lasso'][sample_size]['train']['std'] = np.std(r2s['lasso'][sample_size]['train']['samples'])\n",
    "    \n",
    "    r2s['ridge'][sample_size]['test']['mean'] = np.mean(r2s['ridge'][sample_size]['test']['samples'])\n",
    "    r2s['ridge'][sample_size]['test']['std'] = np.std(r2s['ridge'][sample_size]['test']['samples'])\n",
    "    \n",
    "    r2s['lasso'][sample_size]['test']['mean'] = np.mean(r2s['lasso'][sample_size]['test']['samples'])\n",
    "    r2s['lasso'][sample_size]['test']['std'] = np.std(r2s['lasso'][sample_size]['test']['samples'])\n",
    "\n",
    "# Plot means and standard deviations of R2 scores for sample sizes across ridge and lasso\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of samples\")\n",
    "plt.ylabel(\"Train R2 Score\")\n",
    "plt.errorbar(sample_sizes, [r2s['lasso'][s]['train']['mean'] for s in sample_sizes], yerr=[r2s['lasso'][s]['train']['std'] for s in sample_sizes], label='lasso')\n",
    "plt.errorbar(sample_sizes, [r2s['ridge'][s]['train']['mean'] for s in sample_sizes], yerr=[r2s['ridge'][s]['train']['std'] for s in sample_sizes], label='ridge')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "plt.xlabel(\"Test of samples\")\n",
    "plt.ylabel(\"R2 Score Standard Deviation\")\n",
    "\n",
    "plt.errorbar(sample_sizes, [r2s['lasso'][s]['test']['mean'] for s in sample_sizes], yerr=[r2s['lasso'][s]['test']['std'] for s in sample_sizes], label='lasso')\n",
    "plt.errorbar(sample_sizes, [r2s['ridge'][s]['test']['mean'] for s in sample_sizes], yerr=[r2s['ridge'][s]['test']['std'] for s in sample_sizes], label='ridge')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fit linear, Ridge and Lasso regression models to each of the generated sample. In each case, compute the $R^2$ score for the model on the training sample on which it was fitted, and on the test set.\n",
    "- Repeat the above experiment for 10 random trials/splits, and compute the average train and test $R^2$ across the trials for each training sample size. Also, compute the standard deviation (SD) in each case.\n",
    "- Make a plot of the mean training $R^2$ scores for the linear, Ridge and Lasso regression methods as a function of the training sample size. Also, show a confidence interval for the mean scores extending from **mean - SD** to **mean + SD**. Make a similar plot for the test $R^2$ scores.\n",
    "\n",
    "How do the training and test $R^2$ scores compare for the three methods? Give an explanation for your observations. How do the confidence intervals for the estimated $R^2$ change with training sample size? Based on the plots, which of the three methods would you recommend when one needs to fit a regression model using a small training sample?\n",
    "\n",
    "*Hint:* You may use `sklearn`'s `RidgeCV` and `LassoCV` classes to implement Ridge and Lasso regression. These classes automatically perform cross-validation to tune the parameter $\\lambda$ from a given range of values. You may use the `plt.errorbar` function to plot confidence bars for the average $R^2$ scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q: How do the estimated coefficients compare to or differ from the coefficients estimated by a plain linear regression (without shrinkage penalty) in Part (b) from HW 3? \n",
    "A:  Doing a quick visual comparison, refer to the \"coefficients_df.head\" output above, we observe the estimated coefficients for both Ridge and Lasso regression appear to be lower, on average, than plain linear regression with some (6) of the Lasso coefficients are zeroed out.  We verify this observation more rigorously by computing the mean and standard deviation of the absolute values of the estimated coefficients (see above).  Both the mean and standard deviation is lowest for Ridge, followed by Lasso, following by plain linear regression.  This is expected since coefficient shrinkage is the main goal of Ridge and Lasso regression, and in the case of Lasso, variable reduction, i.e., some coefficients are set to 0.\n",
    "\n",
    "#### Q: Is there a difference between coefficients estimated by the two shrinkage methods? If so, give an explantion for the difference. \n",
    "\n",
    "A: Yes, both methods perform coefficient shrinkage, however, Lasso also zeroes out coefficients thereby producing a sparser solution.  The reason for this difference is that Lasso speciifies a constraint defined on the sum of the absolute values of the estimated coefficients (L1 Norm) vs the sum of the squares of the estimated coefficients (L2 Norm). The nature of using the L1 Norm constraint has the effect of zeroing out coefficients while the L2 Norm constraint merely has the effect of reducing the estimated coefficients. \n",
    "\n",
    "#### Q: List the predictors that are assigned a coefficient value close to 0 (say < 1e-10) by the two methods. How closely do these predictors match the redundant predictors (if any) identified in Part (c) from HW 3? \n",
    "\n",
    "A:  For Ridge regression, no predictors were set very close to zero (< 1e-10).  For Lasso regression, six (6) coefficients are assigned to 0: day_of_week_3.0, day_of_week_4.0, month_2.0, month_5.0, month_11.0, and workingday.   In Part (c) of HW3, working days, holidays, and days of week were identified as having strong correlations.  Lasso zeroed out working days.   \n",
    "\n",
    "#### Q: Is there a difference in the way Ridge and Lasso regression assign coefficients to the predictors temp and atemp? If so, explain the reason for the difference. \n",
    "\n",
    "A: The predictors temp and atemp are highly correlated (0.98) predictors.  Ridge assigns both of these predictors approximately the same value (472 and 469).  Lasso assigns them different values (452 and 680), we would have expected Lasso to assign one of these predictors to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (g): Polynomial & Interaction Terms\n",
    "\n",
    "Moving beyond linear models, we will now try to improve the performance of the regression model in Part (b) from HW 3 by including higher-order polynomial and interaction terms. \n",
    "\n",
    "- For each continuous predictor $X_j$, include additional polynomial terms $X^2_j$, $X^3_j$, and $X^4_j$, and fit a multiple regression model to the expanded training set. How does the $R^2$ of this model on the test set compare with that of the linear model fitted in Part (b) from HW 3? Using a t-test, find out which of estimated coefficients for the polynomial terms are statistically significant at a significance level of 5%. \n",
    "\n",
    "- Fit a multiple linear regression model with additional interaction terms $\\mathbb{I}_{month = 12} \\times temp$ and $\\mathbb{I}_{workingday = 1} \\times \\mathbb{I}_{weathersit = 1}$ and report the test $R^2$ for the fitted model. How does this compare with the $R^2$ obtained using linear model in Part (b) from HW 3? Are the estimated coefficients for the interaction terms statistically significant at a significance level of 5%?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R2:\n",
      "0.669656240221\n",
      "Test R2:\n",
      "0.277238435086\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predictor</th>\n",
       "      <th>coef</th>\n",
       "      <th>std err</th>\n",
       "      <th>t</th>\n",
       "      <th>P&gt;|t|</th>\n",
       "      <th>[0.025</th>\n",
       "      <th>0.975]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>const</td>\n",
       "      <td>3651.0945</td>\n",
       "      <td>1061.216</td>\n",
       "      <td>3.440</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1564.133</td>\n",
       "      <td>5738.056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>temp_norm_2</td>\n",
       "      <td>-2759.4370</td>\n",
       "      <td>1002.647</td>\n",
       "      <td>-2.752</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-4731.218</td>\n",
       "      <td>-787.656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>weather_1.0</td>\n",
       "      <td>1984.3455</td>\n",
       "      <td>736.827</td>\n",
       "      <td>2.693</td>\n",
       "      <td>0.007</td>\n",
       "      <td>535.319</td>\n",
       "      <td>3433.372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>atemp_norm_4</td>\n",
       "      <td>-376.7993</td>\n",
       "      <td>147.915</td>\n",
       "      <td>-2.547</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-667.686</td>\n",
       "      <td>-85.913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>weather_2.0</td>\n",
       "      <td>1774.8474</td>\n",
       "      <td>694.078</td>\n",
       "      <td>2.557</td>\n",
       "      <td>0.011</td>\n",
       "      <td>409.890</td>\n",
       "      <td>3139.805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>atemp_norm_2</td>\n",
       "      <td>2198.9751</td>\n",
       "      <td>903.790</td>\n",
       "      <td>2.433</td>\n",
       "      <td>0.015</td>\n",
       "      <td>421.604</td>\n",
       "      <td>3976.346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>day_of_week_5.0</td>\n",
       "      <td>521.6191</td>\n",
       "      <td>230.018</td>\n",
       "      <td>2.268</td>\n",
       "      <td>0.024</td>\n",
       "      <td>69.271</td>\n",
       "      <td>973.967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>day_of_week_4.0</td>\n",
       "      <td>475.1143</td>\n",
       "      <td>223.309</td>\n",
       "      <td>2.128</td>\n",
       "      <td>0.034</td>\n",
       "      <td>35.960</td>\n",
       "      <td>914.269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>humidity_norm</td>\n",
       "      <td>-454.5818</td>\n",
       "      <td>215.594</td>\n",
       "      <td>-2.109</td>\n",
       "      <td>0.036</td>\n",
       "      <td>-878.564</td>\n",
       "      <td>-30.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>month_4.0</td>\n",
       "      <td>-1376.1298</td>\n",
       "      <td>715.532</td>\n",
       "      <td>-1.923</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-2783.277</td>\n",
       "      <td>31.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>temp_norm_4</td>\n",
       "      <td>361.9131</td>\n",
       "      <td>191.667</td>\n",
       "      <td>1.888</td>\n",
       "      <td>0.060</td>\n",
       "      <td>-15.014</td>\n",
       "      <td>738.841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>windspeed_norm_2</td>\n",
       "      <td>157.6822</td>\n",
       "      <td>109.340</td>\n",
       "      <td>1.442</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-57.344</td>\n",
       "      <td>372.708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>day_of_week_3.0</td>\n",
       "      <td>-307.2298</td>\n",
       "      <td>222.657</td>\n",
       "      <td>-1.380</td>\n",
       "      <td>0.168</td>\n",
       "      <td>-745.101</td>\n",
       "      <td>130.641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>holiday</td>\n",
       "      <td>664.2758</td>\n",
       "      <td>495.431</td>\n",
       "      <td>1.341</td>\n",
       "      <td>0.181</td>\n",
       "      <td>-310.026</td>\n",
       "      <td>1638.578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>month_11.0</td>\n",
       "      <td>-584.2320</td>\n",
       "      <td>469.408</td>\n",
       "      <td>-1.245</td>\n",
       "      <td>0.214</td>\n",
       "      <td>-1507.359</td>\n",
       "      <td>338.895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>season_4.0</td>\n",
       "      <td>646.5739</td>\n",
       "      <td>629.044</td>\n",
       "      <td>1.028</td>\n",
       "      <td>0.305</td>\n",
       "      <td>-590.488</td>\n",
       "      <td>1883.636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>windspeed_norm</td>\n",
       "      <td>-204.0507</td>\n",
       "      <td>203.277</td>\n",
       "      <td>-1.004</td>\n",
       "      <td>0.316</td>\n",
       "      <td>-603.810</td>\n",
       "      <td>195.709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>humidity_norm_2</td>\n",
       "      <td>-101.8628</td>\n",
       "      <td>123.931</td>\n",
       "      <td>-0.822</td>\n",
       "      <td>0.412</td>\n",
       "      <td>-345.582</td>\n",
       "      <td>141.856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>atemp_norm_3</td>\n",
       "      <td>-210.8119</td>\n",
       "      <td>257.823</td>\n",
       "      <td>-0.818</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-717.841</td>\n",
       "      <td>296.217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>workingday</td>\n",
       "      <td>-161.4396</td>\n",
       "      <td>201.612</td>\n",
       "      <td>-0.801</td>\n",
       "      <td>0.424</td>\n",
       "      <td>-557.924</td>\n",
       "      <td>235.045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>month_10.0</td>\n",
       "      <td>-405.5347</td>\n",
       "      <td>519.172</td>\n",
       "      <td>-0.781</td>\n",
       "      <td>0.435</td>\n",
       "      <td>-1426.527</td>\n",
       "      <td>615.457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>season_1.0</td>\n",
       "      <td>-501.2072</td>\n",
       "      <td>644.233</td>\n",
       "      <td>-0.778</td>\n",
       "      <td>0.437</td>\n",
       "      <td>-1768.140</td>\n",
       "      <td>765.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>day_of_week_0.0</td>\n",
       "      <td>-240.2722</td>\n",
       "      <td>324.616</td>\n",
       "      <td>-0.740</td>\n",
       "      <td>0.460</td>\n",
       "      <td>-878.653</td>\n",
       "      <td>398.109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>temp_norm</td>\n",
       "      <td>1284.5216</td>\n",
       "      <td>1817.555</td>\n",
       "      <td>0.707</td>\n",
       "      <td>0.480</td>\n",
       "      <td>-2289.837</td>\n",
       "      <td>4858.880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>month_2.0</td>\n",
       "      <td>-338.0900</td>\n",
       "      <td>528.390</td>\n",
       "      <td>-0.640</td>\n",
       "      <td>0.523</td>\n",
       "      <td>-1377.209</td>\n",
       "      <td>701.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>month_7.0</td>\n",
       "      <td>524.4671</td>\n",
       "      <td>903.709</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.562</td>\n",
       "      <td>-1252.745</td>\n",
       "      <td>2301.679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>month_5.0</td>\n",
       "      <td>-448.9093</td>\n",
       "      <td>782.753</td>\n",
       "      <td>-0.574</td>\n",
       "      <td>0.567</td>\n",
       "      <td>-1988.252</td>\n",
       "      <td>1090.433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>windspeed_norm_4</td>\n",
       "      <td>-12.1547</td>\n",
       "      <td>23.476</td>\n",
       "      <td>-0.518</td>\n",
       "      <td>0.605</td>\n",
       "      <td>-58.323</td>\n",
       "      <td>34.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>humidity_norm_3</td>\n",
       "      <td>39.5809</td>\n",
       "      <td>78.155</td>\n",
       "      <td>0.506</td>\n",
       "      <td>0.613</td>\n",
       "      <td>-114.117</td>\n",
       "      <td>193.279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>month_3.0</td>\n",
       "      <td>-254.9052</td>\n",
       "      <td>535.600</td>\n",
       "      <td>-0.476</td>\n",
       "      <td>0.634</td>\n",
       "      <td>-1308.203</td>\n",
       "      <td>798.392</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           predictor       coef   std err      t  P>|t|    [0.025    0.975]\n",
       "0              const  3651.0945  1061.216  3.440  0.001  1564.133  5738.056\n",
       "29       temp_norm_2 -2759.4370  1002.647 -2.752  0.006 -4731.218  -787.656\n",
       "25       weather_1.0  1984.3455   736.827  2.693  0.007   535.319  3433.372\n",
       "34      atemp_norm_4  -376.7993   147.915 -2.547  0.011  -667.686   -85.913\n",
       "26       weather_2.0  1774.8474   694.078  2.557  0.011   409.890  3139.805\n",
       "32      atemp_norm_2  2198.9751   903.790  2.433  0.015   421.604  3976.346\n",
       "7    day_of_week_5.0   521.6191   230.018  2.268  0.024    69.271   973.967\n",
       "6    day_of_week_4.0   475.1143   223.309  2.128  0.034    35.960   914.269\n",
       "9      humidity_norm  -454.5818   215.594 -2.109  0.036  -878.564   -30.600\n",
       "15         month_4.0 -1376.1298   715.532 -1.923  0.055 -2783.277    31.017\n",
       "31       temp_norm_4   361.9131   191.667  1.888  0.060   -15.014   738.841\n",
       "38  windspeed_norm_2   157.6822   109.340  1.442  0.150   -57.344   372.708\n",
       "5    day_of_week_3.0  -307.2298   222.657 -1.380  0.168  -745.101   130.641\n",
       "8            holiday   664.2758   495.431  1.341  0.181  -310.026  1638.578\n",
       "12        month_11.0  -584.2320   469.408 -1.245  0.214 -1507.359   338.895\n",
       "23        season_4.0   646.5739   629.044  1.028  0.305  -590.488  1883.636\n",
       "27    windspeed_norm  -204.0507   203.277 -1.004  0.316  -603.810   195.709\n",
       "35   humidity_norm_2  -101.8628   123.931 -0.822  0.412  -345.582   141.856\n",
       "33      atemp_norm_3  -210.8119   257.823 -0.818  0.414  -717.841   296.217\n",
       "28        workingday  -161.4396   201.612 -0.801  0.424  -557.924   235.045\n",
       "11        month_10.0  -405.5347   519.172 -0.781  0.435 -1426.527   615.457\n",
       "21        season_1.0  -501.2072   644.233 -0.778  0.437 -1768.140   765.725\n",
       "2    day_of_week_0.0  -240.2722   324.616 -0.740  0.460  -878.653   398.109\n",
       "24         temp_norm  1284.5216  1817.555  0.707  0.480 -2289.837  4858.880\n",
       "13         month_2.0  -338.0900   528.390 -0.640  0.523 -1377.209   701.029\n",
       "18         month_7.0   524.4671   903.709  0.580  0.562 -1252.745  2301.679\n",
       "16         month_5.0  -448.9093   782.753 -0.574  0.567 -1988.252  1090.433\n",
       "40  windspeed_norm_4   -12.1547    23.476 -0.518  0.605   -58.323    34.014\n",
       "36   humidity_norm_3    39.5809    78.155  0.506  0.613  -114.117   193.279\n",
       "14         month_3.0  -254.9052   535.600 -0.476  0.634 -1308.203   798.392"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def get_summary_df(X, y):\n",
    "    ols = sm.OLS(y, X).fit()\n",
    "    \n",
    "    summary_data = ols.summary().tables[1].data\n",
    "    header = summary_data.pop(0)\n",
    "    header[0] = 'predictor'\n",
    "\n",
    "    # We get the data as strings -- convert here. \n",
    "    for i in range(len(summary_data)):\n",
    "        summary_data[i][1] = float(summary_data[i][1])\n",
    "        summary_data[i][2] = float(summary_data[i][2])\n",
    "        summary_data[i][3] = float(summary_data[i][3])\n",
    "        summary_data[i][4] = float(summary_data[i][4])\n",
    "        summary_data[i][5] = float(summary_data[i][5])\n",
    "        summary_data[i][6] = float(summary_data[i][6])\n",
    "        \n",
    "    summary_df = pd.DataFrame(summary_data, columns=header)\n",
    "    return summary_df\n",
    "\n",
    "# Predictors\n",
    "numeric_cols = ['temp_norm', 'atemp_norm', 'humidity_norm', 'windspeed_norm']\n",
    "Xtrain_poly = Xtrain.copy()\n",
    "Xtest_poly = Xtest.copy()\n",
    "\n",
    "# Add exponent columns for all numeric columns of the form:\n",
    "# 'colname_exponent'\n",
    "for numeric_col in numeric_cols:\n",
    "    Xtrain_poly[numeric_col+'_2'] = Xtrain[numeric_col]**2\n",
    "    Xtrain_poly[numeric_col+'_3'] = Xtrain[numeric_col]**3\n",
    "    Xtrain_poly[numeric_col+'_4'] = Xtrain[numeric_col]**4\n",
    "    \n",
    "    Xtest_poly[numeric_col+'_2'] = Xtest[numeric_col]**2\n",
    "    Xtest_poly[numeric_col+'_3'] = Xtest[numeric_col]**3\n",
    "    Xtest_poly[numeric_col+'_4'] = Xtest[numeric_col]**4\n",
    "Xtrain_poly.head()\n",
    "\n",
    "# Get R2 values on test and train sets\n",
    "linreg_poly = LinearRegression().fit(Xtrain_poly, ytrain)\n",
    "trainPreds_poly = linreg_poly.predict(Xtrain_poly)\n",
    "print(\"Train R2:\")\n",
    "print(r2_score(ytrain, trainPreds_poly))\n",
    "\n",
    "testPreds_poly = linreg_poly.predict(Xtest_poly)\n",
    "print(\"Test R2:\")\n",
    "print(r2_score(ytest, testPreds_poly))\n",
    "\n",
    "Xtest_intercept = sm.add_constant(Xtest_poly)\n",
    "summary_df = get_summary_df(Xtest_intercept, ytest)\n",
    "summary_df.sort_values('P>|t|', inplace=True)\n",
    "summary_df.head(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R2:\n",
      "0.669158868766\n",
      "Test R2:\n",
      "0.260947229917\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predictor</th>\n",
       "      <th>coef</th>\n",
       "      <th>std err</th>\n",
       "      <th>t</th>\n",
       "      <th>P&gt;|t|</th>\n",
       "      <th>[0.025</th>\n",
       "      <th>0.975]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>const</td>\n",
       "      <td>6783.4530</td>\n",
       "      <td>1715.005</td>\n",
       "      <td>3.955</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3410.735</td>\n",
       "      <td>10200.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>temp_month_11.0</td>\n",
       "      <td>-4200.0521</td>\n",
       "      <td>1183.507</td>\n",
       "      <td>-3.549</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-6527.529</td>\n",
       "      <td>-1872.575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>temp_month_7.0</td>\n",
       "      <td>-5256.5515</td>\n",
       "      <td>1361.750</td>\n",
       "      <td>-3.860</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-7934.562</td>\n",
       "      <td>-2578.541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>temp_month_6.0</td>\n",
       "      <td>-6280.8525</td>\n",
       "      <td>1163.323</td>\n",
       "      <td>-5.399</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-8568.636</td>\n",
       "      <td>-3993.069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>temp_norm</td>\n",
       "      <td>5688.0955</td>\n",
       "      <td>1408.594</td>\n",
       "      <td>4.038</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2917.963</td>\n",
       "      <td>8458.228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>month_11.0</td>\n",
       "      <td>-4310.6131</td>\n",
       "      <td>1158.983</td>\n",
       "      <td>-3.719</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-6589.862</td>\n",
       "      <td>-2031.364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>month_4.0</td>\n",
       "      <td>-3661.5456</td>\n",
       "      <td>1114.181</td>\n",
       "      <td>-3.286</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-5852.687</td>\n",
       "      <td>-1470.404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>month_10.0</td>\n",
       "      <td>-2651.7798</td>\n",
       "      <td>915.024</td>\n",
       "      <td>-2.898</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-4451.260</td>\n",
       "      <td>-852.299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>temp_month_9.0</td>\n",
       "      <td>-3201.8996</td>\n",
       "      <td>1121.956</td>\n",
       "      <td>-2.854</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-5408.331</td>\n",
       "      <td>-995.468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>temp_month_5.0</td>\n",
       "      <td>-3208.4561</td>\n",
       "      <td>1166.250</td>\n",
       "      <td>-2.751</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-5501.995</td>\n",
       "      <td>-914.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>month_3.0</td>\n",
       "      <td>-2845.0347</td>\n",
       "      <td>1054.093</td>\n",
       "      <td>-2.699</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-4918.008</td>\n",
       "      <td>-772.061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>temp_month_3.0</td>\n",
       "      <td>-2369.1437</td>\n",
       "      <td>904.921</td>\n",
       "      <td>-2.618</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-4148.757</td>\n",
       "      <td>-589.531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>month_9.0</td>\n",
       "      <td>-2891.5783</td>\n",
       "      <td>1109.342</td>\n",
       "      <td>-2.607</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-5073.203</td>\n",
       "      <td>-709.954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>temp_month_8.0</td>\n",
       "      <td>-3848.3243</td>\n",
       "      <td>1496.742</td>\n",
       "      <td>-2.571</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-6791.809</td>\n",
       "      <td>-904.840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>month_5.0</td>\n",
       "      <td>-2879.4152</td>\n",
       "      <td>1163.059</td>\n",
       "      <td>-2.476</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-5166.680</td>\n",
       "      <td>-592.151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>month_2.0</td>\n",
       "      <td>-3104.6446</td>\n",
       "      <td>1264.508</td>\n",
       "      <td>-2.455</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-5591.418</td>\n",
       "      <td>-617.872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>windspeed_norm</td>\n",
       "      <td>-249.2160</td>\n",
       "      <td>104.077</td>\n",
       "      <td>-2.395</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-453.894</td>\n",
       "      <td>-44.538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>temp_month_4.0</td>\n",
       "      <td>-2394.2769</td>\n",
       "      <td>1023.133</td>\n",
       "      <td>-2.340</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-4406.365</td>\n",
       "      <td>-382.189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>temp_month_2.0</td>\n",
       "      <td>-2276.7701</td>\n",
       "      <td>996.622</td>\n",
       "      <td>-2.284</td>\n",
       "      <td>0.023</td>\n",
       "      <td>-4236.720</td>\n",
       "      <td>-316.820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>humidity_norm</td>\n",
       "      <td>-264.2430</td>\n",
       "      <td>122.741</td>\n",
       "      <td>-2.153</td>\n",
       "      <td>0.032</td>\n",
       "      <td>-505.626</td>\n",
       "      <td>-22.860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>temp_month_10.0</td>\n",
       "      <td>-2265.4009</td>\n",
       "      <td>1095.649</td>\n",
       "      <td>-2.068</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-4420.098</td>\n",
       "      <td>-110.704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>atemp_norm</td>\n",
       "      <td>-2031.3825</td>\n",
       "      <td>1064.496</td>\n",
       "      <td>-1.908</td>\n",
       "      <td>0.057</td>\n",
       "      <td>-4124.815</td>\n",
       "      <td>62.050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>temp_month_1.0</td>\n",
       "      <td>-1746.5548</td>\n",
       "      <td>1013.306</td>\n",
       "      <td>-1.724</td>\n",
       "      <td>0.086</td>\n",
       "      <td>-3739.317</td>\n",
       "      <td>246.207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>day_of_week_3.0</td>\n",
       "      <td>-492.4222</td>\n",
       "      <td>294.576</td>\n",
       "      <td>-1.672</td>\n",
       "      <td>0.095</td>\n",
       "      <td>-1071.734</td>\n",
       "      <td>86.889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>holiday</td>\n",
       "      <td>878.9675</td>\n",
       "      <td>549.689</td>\n",
       "      <td>1.599</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-202.048</td>\n",
       "      <td>1959.983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>month_1.0</td>\n",
       "      <td>-1866.8480</td>\n",
       "      <td>1390.294</td>\n",
       "      <td>-1.343</td>\n",
       "      <td>0.180</td>\n",
       "      <td>-4600.991</td>\n",
       "      <td>867.295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>month_8.0</td>\n",
       "      <td>-2403.9545</td>\n",
       "      <td>1810.948</td>\n",
       "      <td>-1.327</td>\n",
       "      <td>0.185</td>\n",
       "      <td>-5965.354</td>\n",
       "      <td>1157.445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>day_of_week_2.0</td>\n",
       "      <td>-388.9136</td>\n",
       "      <td>300.438</td>\n",
       "      <td>-1.294</td>\n",
       "      <td>0.196</td>\n",
       "      <td>-979.752</td>\n",
       "      <td>201.925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>working_weather_1.0</td>\n",
       "      <td>1758.1043</td>\n",
       "      <td>1488.253</td>\n",
       "      <td>1.181</td>\n",
       "      <td>0.238</td>\n",
       "      <td>-1168.685</td>\n",
       "      <td>4684.894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>workingday</td>\n",
       "      <td>-1389.0598</td>\n",
       "      <td>1255.685</td>\n",
       "      <td>-1.106</td>\n",
       "      <td>0.269</td>\n",
       "      <td>-3858.482</td>\n",
       "      <td>1080.363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              predictor       coef   std err      t  P>|t|    [0.025  \\\n",
       "0                 const  6783.4530  1715.005  3.955  0.000  3410.735   \n",
       "39      temp_month_11.0 -4200.0521  1183.507 -3.549  0.000 -6527.529   \n",
       "35       temp_month_7.0 -5256.5515  1361.750 -3.860  0.000 -7934.562   \n",
       "34       temp_month_6.0 -6280.8525  1163.323 -5.399  0.000 -8568.636   \n",
       "24            temp_norm  5688.0955  1408.594  4.038  0.000  2917.963   \n",
       "12           month_11.0 -4310.6131  1158.983 -3.719  0.000 -6589.862   \n",
       "15            month_4.0 -3661.5456  1114.181 -3.286  0.001 -5852.687   \n",
       "11           month_10.0 -2651.7798   915.024 -2.898  0.004 -4451.260   \n",
       "37       temp_month_9.0 -3201.8996  1121.956 -2.854  0.005 -5408.331   \n",
       "33       temp_month_5.0 -3208.4561  1166.250 -2.751  0.006 -5501.995   \n",
       "14            month_3.0 -2845.0347  1054.093 -2.699  0.007 -4918.008   \n",
       "31       temp_month_3.0 -2369.1437   904.921 -2.618  0.009 -4148.757   \n",
       "20            month_9.0 -2891.5783  1109.342 -2.607  0.010 -5073.203   \n",
       "36       temp_month_8.0 -3848.3243  1496.742 -2.571  0.011 -6791.809   \n",
       "16            month_5.0 -2879.4152  1163.059 -2.476  0.014 -5166.680   \n",
       "13            month_2.0 -3104.6446  1264.508 -2.455  0.015 -5591.418   \n",
       "27       windspeed_norm  -249.2160   104.077 -2.395  0.017  -453.894   \n",
       "32       temp_month_4.0 -2394.2769  1023.133 -2.340  0.020 -4406.365   \n",
       "30       temp_month_2.0 -2276.7701   996.622 -2.284  0.023 -4236.720   \n",
       "9         humidity_norm  -264.2430   122.741 -2.153  0.032  -505.626   \n",
       "38      temp_month_10.0 -2265.4009  1095.649 -2.068  0.039 -4420.098   \n",
       "1            atemp_norm -2031.3825  1064.496 -1.908  0.057 -4124.815   \n",
       "29       temp_month_1.0 -1746.5548  1013.306 -1.724  0.086 -3739.317   \n",
       "5       day_of_week_3.0  -492.4222   294.576 -1.672  0.095 -1071.734   \n",
       "8               holiday   878.9675   549.689  1.599  0.111  -202.048   \n",
       "10            month_1.0 -1866.8480  1390.294 -1.343  0.180 -4600.991   \n",
       "19            month_8.0 -2403.9545  1810.948 -1.327  0.185 -5965.354   \n",
       "4       day_of_week_2.0  -388.9136   300.438 -1.294  0.196  -979.752   \n",
       "40  working_weather_1.0  1758.1043  1488.253  1.181  0.238 -1168.685   \n",
       "28           workingday -1389.0598  1255.685 -1.106  0.269 -3858.482   \n",
       "\n",
       "       0.975]  \n",
       "0   10200.000  \n",
       "39  -1872.575  \n",
       "35  -2578.541  \n",
       "34  -3993.069  \n",
       "24   8458.228  \n",
       "12  -2031.364  \n",
       "15  -1470.404  \n",
       "11   -852.299  \n",
       "37   -995.468  \n",
       "33   -914.917  \n",
       "14   -772.061  \n",
       "31   -589.531  \n",
       "20   -709.954  \n",
       "36   -904.840  \n",
       "16   -592.151  \n",
       "13   -617.872  \n",
       "27    -44.538  \n",
       "32   -382.189  \n",
       "30   -316.820  \n",
       "9     -22.860  \n",
       "38   -110.704  \n",
       "1      62.050  \n",
       "29    246.207  \n",
       "5      86.889  \n",
       "8    1959.983  \n",
       "10    867.295  \n",
       "19   1157.445  \n",
       "4     201.925  \n",
       "40   4684.894  \n",
       "28   1080.363  "
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain_interaction = Xtrain.copy()\n",
    "Xtest_interaction = Xtest.copy()\n",
    "\n",
    "# Numeric strings for each of the month and weather columns\n",
    "months = ['1.0', '2.0', '3.0', '4.0', '5.0', '6.0', '7.0', '8.0', '9.0', '10.0', '11.0']\n",
    "weathers = ['1.0', '2.0']\n",
    "\n",
    "# Create new columns for temperature and each month\n",
    "for month in months:\n",
    "    Xtrain_interaction['temp_month_'+month] = Xtrain['month_'+month]* Xtrain['temp_norm']\n",
    "    Xtest_interaction['temp_month_'+month] = Xtest['month_'+month]* Xtest['temp_norm']\n",
    "\n",
    "# Create new columsn for weather and working day\n",
    "for weather in weathers:\n",
    "    Xtrain_interaction['working_weather_'+weather] = Xtrain['weather_'+weather] * Xtrain['workingday']\n",
    "    Xtest_interaction['working_weather_'+weather] = Xtest['weather_'+weather] * Xtest['workingday']\n",
    "    \n",
    "Xtrain_interaction.head()\n",
    "\n",
    "# Get test and train R2s\n",
    "linreg = LinearRegression().fit(Xtrain_interaction, ytrain)\n",
    "trainPreds = linreg.predict(Xtrain_interaction)\n",
    "print(\"Train R2:\")\n",
    "print(r2_score(ytrain, trainPreds))\n",
    "\n",
    "testPreds = linreg.predict(Xtest_interaction)\n",
    "print(\"Test R2:\")\n",
    "print(r2_score(ytest, testPreds))\n",
    "\n",
    "Xtest_intercept = sm.add_constant(Xtest_interaction)\n",
    "summary_df = get_summary_df(Xtest_intercept, ytest)\n",
    "summary_df.sort_values('P>|t|', inplace=True)\n",
    "summary_df.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q: Polynomial model: How does the  R2  of this model on the test set compare with that of the linear model fitted in Part (b) from HW 3? Using a t-test, find out which of estimated coefficients for the polynomial terms are statistically significant at a significance level of 5%.\n",
    "\n",
    "A: Both the R2 scores for the train and test set are slightly higher than they were in part B of HW3. HW3 test data R2:0.576 train data R2: 0.249. The train and test R2 scores with this model combining interaction terms was 0.670 and 0.277\n",
    "\n",
    "The set of predictors in this model that have a significance level of 5% are\n",
    "temp_norm_2, weather_1.0, atemp_norm_4, weather_2.0, atemp_norm_2, day_of_week_5.0, day_of_week_4.0, humidity_norm\n",
    "\n",
    "In part B of HW3 the significant predictors were weather_1.0, weather_2.0, day_of_week_5.0, temp, atemp_norm, and humidity_norm. The predictors between the two sets are very similar, although temperature squared and adjusted temperature^4 appeared to increase in predictive power. I believe this is because the relationship between temperature and bike rentals may have an underlying exponential component.\n",
    "\n",
    "\n",
    "#### Q: Interaction model: How does this compare with the  R2R2  obtained using linear model in Part (b) from HW 3? Are the estimated coefficients for the interaction terms statistically significant at a significance level of 5%?\n",
    "\n",
    "A: As with the polynomial model, Both the R2 scores for the train and test set are slightly higher than they were in part B of HW3. The train and test R2 scores with this model combining interaction terms was 0.669 and 0.265\n",
    "\n",
    "There are a much larger set of predictors that have a significance level of 5%: temp_month_11.0, temp_month_7.0, temp_month_6.0, temp_norm, month_11.-, month_4.0, month_10.0, temp_month_9.0, temp_month_5.0, month_3.0, temp_month_3.0, month_9.0, temp_month_8.0, month_5.0, month_2.0, windspeed_norm, temp_month_4.0, temp_month_2.0, humidity_norm\n",
    "\n",
    "Most of these significant predictors incorporate temperature and/or are months. I was surprised that some of the \"month\" variables seemed to have improved significance, but were not significant in the previous model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (h): PCA to deal with high dimensionality\n",
    "\n",
    "We would like to fit a model to include all main effects, polynomial terms up to the $4^{th}$ order, and all interactions between all possible predictors and polynomial terms (not including the interactions between $X^1_j$, $X^2_j$, $X^3_j$, and $X^4_j$ as they would just create higher order polynomial terms).  \n",
    "\n",
    "- Create an expanded training set including all the desired terms mentioned above.  What are the dimensions of this 'design matrix' of all the predictor variables?   What are the issues with attempting to fit a regression model using all of these predictors?\n",
    "\n",
    "- Instead of using the usual approaches for model selection, let's instead use principal components analysis (PCA) to fit the model.  First, create the principal component vectors in python (consider: should you normalize first?).  Then fit 5 different regression models: (1) using just the first PCA vector, (2) using the first two PCA vectors, (3) using the first three PCA vectors, etc...  Briefly summarize how these models compare in the training set.\n",
    "\n",
    "- Use the test set to decide which of the 5 models above is best to predict out of sample.  How does this model compare to the previous models you've fit?  What are the interpretations of this model's coefficients?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>atemp_norm</th>\n",
       "      <th>day_of_week_0.0</th>\n",
       "      <th>day_of_week_1.0</th>\n",
       "      <th>day_of_week_2.0</th>\n",
       "      <th>day_of_week_3.0</th>\n",
       "      <th>day_of_week_4.0</th>\n",
       "      <th>day_of_week_5.0</th>\n",
       "      <th>holiday</th>\n",
       "      <th>humidity_norm</th>\n",
       "      <th>month_1.0</th>\n",
       "      <th>...</th>\n",
       "      <th>windspeed_norm_4Xday_of_week_2.0</th>\n",
       "      <th>windspeed_norm_4Xday_of_week_1.0</th>\n",
       "      <th>windspeed_norm_4Xmonth_3.0</th>\n",
       "      <th>windspeed_norm_4Xday_of_week_4.0</th>\n",
       "      <th>windspeed_norm_4Xday_of_week_0.0</th>\n",
       "      <th>windspeed_norm_4Xday_of_week_3.0</th>\n",
       "      <th>windspeed_norm_4Xday_of_week_5.0</th>\n",
       "      <th>windspeed_norm_4Xmonth_2.0</th>\n",
       "      <th>windspeed_norm_4Xweather_1.0</th>\n",
       "      <th>windspeed_norm_4Xworkingday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.310000e+02</td>\n",
       "      <td>331.000000</td>\n",
       "      <td>331.000000</td>\n",
       "      <td>331.000000</td>\n",
       "      <td>331.000000</td>\n",
       "      <td>331.000000</td>\n",
       "      <td>331.000000</td>\n",
       "      <td>331.000000</td>\n",
       "      <td>3.310000e+02</td>\n",
       "      <td>331.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.310000e+02</td>\n",
       "      <td>3.310000e+02</td>\n",
       "      <td>3.310000e+02</td>\n",
       "      <td>3.310000e+02</td>\n",
       "      <td>3.310000e+02</td>\n",
       "      <td>3.310000e+02</td>\n",
       "      <td>3.310000e+02</td>\n",
       "      <td>3.310000e+02</td>\n",
       "      <td>3.310000e+02</td>\n",
       "      <td>3.310000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-5.433720e-17</td>\n",
       "      <td>0.160121</td>\n",
       "      <td>0.175227</td>\n",
       "      <td>0.135952</td>\n",
       "      <td>0.123867</td>\n",
       "      <td>0.123867</td>\n",
       "      <td>0.145015</td>\n",
       "      <td>0.033233</td>\n",
       "      <td>2.012489e-17</td>\n",
       "      <td>0.078550</td>\n",
       "      <td>...</td>\n",
       "      <td>5.970384e-17</td>\n",
       "      <td>2.549153e-17</td>\n",
       "      <td>4.645495e-17</td>\n",
       "      <td>-1.133702e-16</td>\n",
       "      <td>1.304764e-16</td>\n",
       "      <td>-6.230330e-17</td>\n",
       "      <td>1.931151e-16</td>\n",
       "      <td>3.354148e-17</td>\n",
       "      <td>-8.989117e-17</td>\n",
       "      <td>6.641213e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.367273</td>\n",
       "      <td>0.380736</td>\n",
       "      <td>0.343256</td>\n",
       "      <td>0.329929</td>\n",
       "      <td>0.329929</td>\n",
       "      <td>0.352649</td>\n",
       "      <td>0.179515</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.269442</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.572131e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.648736e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.556308e-01</td>\n",
       "      <td>-1.341264e-01</td>\n",
       "      <td>-1.111391e-01</td>\n",
       "      <td>-1.352397e-01</td>\n",
       "      <td>-1.285487e-01</td>\n",
       "      <td>-1.707624e-01</td>\n",
       "      <td>-1.210952e-01</td>\n",
       "      <td>-1.113881e-01</td>\n",
       "      <td>-2.341945e-01</td>\n",
       "      <td>-2.759446e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-8.603176e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-7.452412e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.556308e-01</td>\n",
       "      <td>-1.341264e-01</td>\n",
       "      <td>-1.111391e-01</td>\n",
       "      <td>-1.352397e-01</td>\n",
       "      <td>-1.285487e-01</td>\n",
       "      <td>-1.707624e-01</td>\n",
       "      <td>-1.210952e-01</td>\n",
       "      <td>-1.113881e-01</td>\n",
       "      <td>-2.341945e-01</td>\n",
       "      <td>-2.759446e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.466312e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.562743e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.556308e-01</td>\n",
       "      <td>-1.341264e-01</td>\n",
       "      <td>-1.111391e-01</td>\n",
       "      <td>-1.352397e-01</td>\n",
       "      <td>-1.285487e-01</td>\n",
       "      <td>-1.707624e-01</td>\n",
       "      <td>-1.210952e-01</td>\n",
       "      <td>-1.113881e-01</td>\n",
       "      <td>-2.334611e-01</td>\n",
       "      <td>-2.741981e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.508005e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.055719e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.556308e-01</td>\n",
       "      <td>-1.341264e-01</td>\n",
       "      <td>-1.111391e-01</td>\n",
       "      <td>-1.352397e-01</td>\n",
       "      <td>-1.285487e-01</td>\n",
       "      <td>-1.707624e-01</td>\n",
       "      <td>-1.210952e-01</td>\n",
       "      <td>-1.113881e-01</td>\n",
       "      <td>-1.772616e-01</td>\n",
       "      <td>-1.735898e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.959139e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.362380e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.212031e+01</td>\n",
       "      <td>1.518684e+01</td>\n",
       "      <td>1.434265e+01</td>\n",
       "      <td>1.563538e+01</td>\n",
       "      <td>1.626381e+01</td>\n",
       "      <td>1.114297e+01</td>\n",
       "      <td>1.589538e+01</td>\n",
       "      <td>1.123906e+01</td>\n",
       "      <td>9.744872e+00</td>\n",
       "      <td>9.827090e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         atemp_norm  day_of_week_0.0  day_of_week_1.0  day_of_week_2.0  \\\n",
       "count  3.310000e+02       331.000000       331.000000       331.000000   \n",
       "mean  -5.433720e-17         0.160121         0.175227         0.135952   \n",
       "std    1.000000e+00         0.367273         0.380736         0.343256   \n",
       "min   -2.572131e+00         0.000000         0.000000         0.000000   \n",
       "25%   -8.603176e-01         0.000000         0.000000         0.000000   \n",
       "50%    1.466312e-01         0.000000         0.000000         0.000000   \n",
       "75%    7.508005e-01         0.000000         0.000000         0.000000   \n",
       "max    1.959139e+00         1.000000         1.000000         1.000000   \n",
       "\n",
       "       day_of_week_3.0  day_of_week_4.0  day_of_week_5.0     holiday  \\\n",
       "count       331.000000       331.000000       331.000000  331.000000   \n",
       "mean          0.123867         0.123867         0.145015    0.033233   \n",
       "std           0.329929         0.329929         0.352649    0.179515   \n",
       "min           0.000000         0.000000         0.000000    0.000000   \n",
       "25%           0.000000         0.000000         0.000000    0.000000   \n",
       "50%           0.000000         0.000000         0.000000    0.000000   \n",
       "75%           0.000000         0.000000         0.000000    0.000000   \n",
       "max           1.000000         1.000000         1.000000    1.000000   \n",
       "\n",
       "       humidity_norm   month_1.0             ...               \\\n",
       "count   3.310000e+02  331.000000             ...                \n",
       "mean    2.012489e-17    0.078550             ...                \n",
       "std     1.000000e+00    0.269442             ...                \n",
       "min    -2.648736e+00    0.000000             ...                \n",
       "25%    -7.452412e-01    0.000000             ...                \n",
       "50%    -6.562743e-03    0.000000             ...                \n",
       "75%     7.055719e-01    0.000000             ...                \n",
       "max     2.362380e+00    1.000000             ...                \n",
       "\n",
       "       windspeed_norm_4Xday_of_week_2.0  windspeed_norm_4Xday_of_week_1.0  \\\n",
       "count                      3.310000e+02                      3.310000e+02   \n",
       "mean                       5.970384e-17                      2.549153e-17   \n",
       "std                        1.000000e+00                      1.000000e+00   \n",
       "min                       -1.556308e-01                     -1.341264e-01   \n",
       "25%                       -1.556308e-01                     -1.341264e-01   \n",
       "50%                       -1.556308e-01                     -1.341264e-01   \n",
       "75%                       -1.556308e-01                     -1.341264e-01   \n",
       "max                        1.212031e+01                      1.518684e+01   \n",
       "\n",
       "       windspeed_norm_4Xmonth_3.0  windspeed_norm_4Xday_of_week_4.0  \\\n",
       "count                3.310000e+02                      3.310000e+02   \n",
       "mean                 4.645495e-17                     -1.133702e-16   \n",
       "std                  1.000000e+00                      1.000000e+00   \n",
       "min                 -1.111391e-01                     -1.352397e-01   \n",
       "25%                 -1.111391e-01                     -1.352397e-01   \n",
       "50%                 -1.111391e-01                     -1.352397e-01   \n",
       "75%                 -1.111391e-01                     -1.352397e-01   \n",
       "max                  1.434265e+01                      1.563538e+01   \n",
       "\n",
       "       windspeed_norm_4Xday_of_week_0.0  windspeed_norm_4Xday_of_week_3.0  \\\n",
       "count                      3.310000e+02                      3.310000e+02   \n",
       "mean                       1.304764e-16                     -6.230330e-17   \n",
       "std                        1.000000e+00                      1.000000e+00   \n",
       "min                       -1.285487e-01                     -1.707624e-01   \n",
       "25%                       -1.285487e-01                     -1.707624e-01   \n",
       "50%                       -1.285487e-01                     -1.707624e-01   \n",
       "75%                       -1.285487e-01                     -1.707624e-01   \n",
       "max                        1.626381e+01                      1.114297e+01   \n",
       "\n",
       "       windspeed_norm_4Xday_of_week_5.0  windspeed_norm_4Xmonth_2.0  \\\n",
       "count                      3.310000e+02                3.310000e+02   \n",
       "mean                       1.931151e-16                3.354148e-17   \n",
       "std                        1.000000e+00                1.000000e+00   \n",
       "min                       -1.210952e-01               -1.113881e-01   \n",
       "25%                       -1.210952e-01               -1.113881e-01   \n",
       "50%                       -1.210952e-01               -1.113881e-01   \n",
       "75%                       -1.210952e-01               -1.113881e-01   \n",
       "max                        1.589538e+01                1.123906e+01   \n",
       "\n",
       "       windspeed_norm_4Xweather_1.0  windspeed_norm_4Xworkingday  \n",
       "count                  3.310000e+02                 3.310000e+02  \n",
       "mean                  -8.989117e-17                 6.641213e-17  \n",
       "std                    1.000000e+00                 1.000000e+00  \n",
       "min                   -2.341945e-01                -2.759446e-01  \n",
       "25%                   -2.341945e-01                -2.759446e-01  \n",
       "50%                   -2.334611e-01                -2.741981e-01  \n",
       "75%                   -1.772616e-01                -1.735898e-01  \n",
       "max                    9.744872e+00                 9.827090e+00  \n",
       "\n",
       "[8 rows x 1000 columns]"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_predictors = list(Xtrain_poly)\n",
    "non_poly_predictors = set(list(Xtrain)) - set(numeric_cols)\n",
    "\n",
    "Xtrain_big = Xtrain_poly.copy()\n",
    "\n",
    "Xtest_big = Xtest_poly.copy()\n",
    "for predictor1 in all_predictors:\n",
    "    for predictor2 in non_poly_predictors:\n",
    "        Xtrain_big[predictor1+'X'+predictor2] = Xtrain_poly[predictor1]* Xtrain_poly[predictor2]\n",
    "        Xtest_big[predictor1+'X'+predictor2] = Xtest_poly[predictor1]* Xtest_poly[predictor2]\n",
    "\n",
    "\n",
    "numeric_strings = ['humidity', 'temp', 'windspeed']\n",
    "# Get a list of all the columns with either temp, humidity, or windspeed\n",
    "\n",
    "numeric_cols = [name for name in list(Xtrain_big) if any(num in name for num in numeric_strings)]\n",
    "\n",
    "#Normalize the data\n",
    "for numeric_col in numeric_cols:\n",
    "    Xtest_big[numeric_col] = Xtest_big[numeric_col].transform(lambda x, m = Xtrain_big[numeric_col].mean(), \n",
    "                                                                          s = Xtrain_big[numeric_col].std()\n",
    "                                                                          : (x - m)/s )\n",
    "# Rescale train data using mean and std computed from train\n",
    "Xtrain_big[numeric_cols] = Xtrain_big[numeric_cols].transform(lambda x: (x - x.mean()) / x.std())\n",
    "\n",
    "\n",
    "Xtrain_big.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R2 LR Test Score with 1 principal components:  0.190\n",
      "\n",
      "R2 LR Test Score with 2 principal components:  0.196\n",
      "\n",
      "R2 LR Test Score with 3 principal components:  0.208\n",
      "\n",
      "R2 LR Test Score with 4 principal components:  0.222\n",
      "\n",
      "R2 LR Test Score with 5 principal components:  0.224\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "\n",
    "for n_comp in range(1,6):\n",
    "    pca = PCA(n_components = n_comp)\n",
    "#   Apply Scaling to Xtrain_big and Xtest_big\n",
    "    std_scale = preprocessing.StandardScaler().fit(Xtrain_big)\n",
    "    X_train_std = std_scale.transform(Xtrain_big)\n",
    "    X_test_std = std_scale.transform(Xtest_big)\n",
    "#   Perform PCA\n",
    "    pca.fit(X_train_std)\n",
    "#   Apply the dimensionality reduction on X_train_std and X_test_std\n",
    "    Xtrain_big_pca = pca.transform(X_train_std)\n",
    "    Xtest_big_pca  = pca.transform(X_test_std)\n",
    "#   Create LR model\n",
    "    lr = LinearRegression()\n",
    "    #  Fit the LR model with reduced dimensional train data\n",
    "    lr.fit(Xtrain_big_pca, ytrain)\n",
    "    # Predict on reduced dimensional test datatest\n",
    "    ytest_pred = lr.predict(Xtest_big_pca)\n",
    "    # Get R2 test score\n",
    "    r2_score_test = r2_score(ytest, ytest_pred)\n",
    "    print(\"\\nR2 LR Test Score with \" + str(n_comp) + \" principal components:  %5.3f\" % r2_score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Create an expanded training set including all the desired terms mentioned above. \n",
    "\n",
    "#### Q:  What are the dimensions of this 'design matrix' of all the predictor variables? \n",
    "\n",
    "A:  The 'design matrix' has 1000 dimensions, IOW, it now has 1000 predictors (columns).\n",
    "\n",
    "\n",
    "#### Q:  What are the issues with attempting to fit a regression model using all of these predictors?\n",
    "\n",
    "A:  Because there are more dimensions (1000) than training samples (331), the model's predictive ability may exhibit high variance (overfitting).   The interpretability of the model will suffer too.  It will be more challenging, if not impossible, to easily ascertain the most important predictors of the model.  Also, many of the interaction terms created by multiplying a continuous valued predictor with the one-hot encoded predictors derived from the same categorical predictor are collinear.  This will add to the instability of the fitted model.  Training performance of the model will also suffer, i.e., slower training time, more memory/CPU resources are required.\n",
    "\n",
    "\n",
    "#### Q: How does this model compare to the previous models you've fit?  <br><br> \n",
    "\n",
    "A:  The model the first five (5) PCA vectors yielded the best $R^2$ Test  score (0.225), followed by the first four, then the first three, etc.  This is expected since the PCA vectors are ordered, in decreasing order, in terms of the amount of variance in the data they explain and since the PCA vectors are orthogonal, we'd expect the addition of each PCA vector to improve predictive power ($R^2$), with diminishing improvements.\n",
    "\n",
    "#### Q:  What are the interpretations of this model's coefficients? \n",
    "\n",
    "A:  To interpret each component, we must compute the correlations between the original training set for each predictor and each principal component.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (i): Beyond Squared Error\n",
    "\n",
    "We have seen in class that the multiple linear regression method optimizes the Mean Squared Error (MSE) on the training set. Consider the following alternate evaluation metric, referred to as the Root Mean Squared Logarthmic Error (RMSLE):\n",
    "\n",
    "$$\n",
    "\\sqrt{\\frac{1}{n}\\sum_{i=1}^n (log(y_i+1) - log(\\hat{y}_i+1))^2}.\n",
    "$$\n",
    "\n",
    "The *lower* the RMSLE the *better* is the performance of a model. The RMSLE penalizes errors on smaller responses more heavily than errors on larger responses. For example, the RMSLE penalizes a prediction of $\\hat{y} = 15$ for a true response of $y=10$ more heavily than a prediction of $\\hat{y} = 105$ for a true response of $100$, though the difference in predicted and true responses are the same in both cases. \n",
    "\n",
    "This is a natural evaluation metric for bike share demand prediction, as in this application, it is more important that the prediction model is accurate on days where the demand is low (so that the few customers who arrive are served satisfactorily), compared to days on which the demand is high (when it is less damaging to lose out on some customers).\n",
    "\n",
    "The following code computes the RMSLE for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#--------  rmsle\n",
    "# A function for evaluating Root Mean Squared Logarithmic Error (RMSLE)\n",
    "# of the linear regression model on a data set\n",
    "# Input: \n",
    "#      y_test (n x 1 array of response variable vals in testing data)\n",
    "#      y_pred (n x 1 array of response variable vals in testing data)\n",
    "# Return: \n",
    "#      RMSLE (float) \n",
    "\n",
    "def rmsle(y, y_pred): \n",
    "    # Evaluate sqaured error, against target labels\n",
    "    # rmsle = \\sqrt(1/n \\sum_i (log (y[i]+1) - log (y_pred[i]+1))^2)\n",
    "    rmsle_ = np.sqrt(np.mean(np.square(np.log(y+1) - np.log(y_pred+1))))\n",
    "    \n",
    "    return rmsle_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the above code to compute the training and test RMSLE for the polynomial regression model you fit in Part (g). \n",
    "\n",
    "You are required to develop a strategy to fit a regression model by optimizing the RMSLE on the training set. Give a justification for your proposed approach. Does the model fitted using your approach yield lower train RMSLE than the model in Part (g)? How about the test RMSLE of the new model? \n",
    "\n",
    "**Note:** We do not require you to implement a new regression solver for RMSLE. Instead, we ask you to think about ways to use existing built-in functions to fit a model that performs well on RMSLE. Your regression model may use the same polynomial terms used in Part (g)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSLE - Part (g) Polynomial regression model:\n",
      "0.310644877348\n",
      "Test RMSLE - Part (g) Polynomial regression model:\n",
      "0.524272332738\n",
      "\n",
      "\n",
      "Polynomial regression model with predictors transformed into logspace.\n",
      "Predictions are then transformed back into original space:\n",
      "\n",
      "Train RMSLE - Our approach:\n",
      "0.291169985367\n",
      "Test RMSLE - Our approach:\n",
      "0.52354849583\n"
     ]
    }
   ],
   "source": [
    "# Our approach:\n",
    "# Transform the count values into log scale and then perform linear regression\n",
    "# Note: log1p = log(1 + x), 1 is added in case count is zero, though, it isnt\n",
    "# in our train and test sets\n",
    "#\n",
    "weighted_linreg = LinearRegression().fit(Xtrain_poly, np.log1p(ytrain))\n",
    "#\n",
    "# Make predictions on train and test and then transform predictions back into the original space, \n",
    "# compute the inverse transform of np.log using np.exp:\n",
    "# Note: expm1 = exp(x) - 1, the inverse of log1p\n",
    "#\n",
    "weighted_train_pred = np.expm1(weighted_linreg.predict(Xtrain_poly))\n",
    "weighted_test_pred  = np.expm1(weighted_linreg.predict(Xtest_poly))\n",
    "#\n",
    "# Compute train and test RMSLE for our approach\n",
    "#\n",
    "rmsleVals_train_weighted = rmsle(ytrain, weighted_train_pred)\n",
    "rmsleVals_test_weighted = rmsle(ytest, weighted_test_pred)\n",
    "\n",
    "    \n",
    "# Compute training and test RMSLE for the polynomial regression model you fit in Part (g)     \n",
    "\n",
    "print(\"Train RMSLE - Part (g) Polynomial regression model:\")\n",
    "print (rmsle(ytrain, trainPreds_poly))\n",
    "print(\"Test RMSLE - Part (g) Polynomial regression model:\")\n",
    "print (rmsle(ytest, testPreds_poly))\n",
    "\n",
    "# Compute training and test RMSLE for our approach     \n",
    "\n",
    "print(\"\\n\\nPolynomial regression model with predictors transformed into logspace.\\n\\\n",
    "Predictions are then transformed back into original space:\")\n",
    "print(\"\\nTrain RMSLE - Our approach:\")\n",
    "print(rmsleVals_train_weighted)\n",
    "print(\"Test RMSLE - Our approach:\")\n",
    "print(rmsleVals_test_weighted)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the above code to compute the training and test RMSLE for the polynomial regression model you fit in Part (g).  Give a justification for your proposed approach. \n",
    "Our approach: Since the error function, RMSLE, is based on the log of the response variable, we thought it might make sense to come up with an approach that transforms the response variable into log space and then fit a linear regression model using the transformed response variable.  After we make predictions on our model, we then transform the predicted response back to the original space using the exponential function, the inverse of the log function.   \n",
    "\n",
    "#### Q: Does the model fitted using your approach yield lower train RMSLE than the model in Part (g)? \n",
    "\n",
    "A:  The train RMSLE from the model fitted using our approach was lower than the model in Part (g), 0.2911 vs 0.3106.  \n",
    "\n",
    "#### Q: How about the test RMSLE of the new model? \n",
    "\n",
    "A:  The test RMSLE from the model fitted using our approach was marginally lower than the model in Part (g),  0.5235 vs 0.5243.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (j): Dealing with Erroneous Labels\n",
    "\n",
    "Due to occasional system crashes, some of the bike counts reported in the data set have been recorded manually. These counts are not very unreliable and are prone to errors. It is known that roughly 5% of the labels in the training set are erroneous (i.e. can be arbitrarily different from the true counts), while all the labels in the test set were confirmed to be accurate. Unfortunately, the identities of the erroneous records in the training set are not available. Can this information about presence of 5% errors in the training set labels (without details about the specific identities of the erroneous rows) be used to improve the performance of the model in Part (g)? Note that we are interested in improving the $R^2$ performance of the model on the test set (not the training $R^2$ score). \n",
    "\n",
    "As a final task, we require you to come up with a strategy to fit a regression model, taking into account the errors in the training set labels. Explain the intuition behind your approach (we do not expect a detailed mathematical justification). Use your approach to fit a regression model on the training set, and compare its test $R^2$ with the model in Part (g).\n",
    "\n",
    "**Note:** Again, we do not require you to implement a new regression solver for handling erroneous labels. It is sufficient that you to come up with an approach that uses existing built-in functions. Your regression model may use the same polynomial terms used in Part (g)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R2 LR Test Score with full train\n",
      "0.249342111465\n",
      "\n",
      "R2 LR Test Score with partial train after removing 5% (16) of the worst residuals\n",
      "0.250203096354\n"
     ]
    }
   ],
   "source": [
    "# Fit a LR model and then remove 5% of the samples with the highest absolute residual error \n",
    "# and then refit a model without those samples.\n",
    "#\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "#  Fit the LR model\n",
    "lr.fit(Xtrain, ytrain)\n",
    "# Predict on train\n",
    "ytrain_pred = lr.predict(Xtrain)\n",
    "# Predict on test\n",
    "ytest_pred = lr.predict(Xtest)\n",
    "# Get R2 test score\n",
    "r2_score_test = r2_score(ytest, ytest_pred)\n",
    "print(\"\\nR2 LR Test Score with full train\")\n",
    "print (r2_score_test)\n",
    "#  Get the residuals\n",
    "abs_residuals = abs(ytrain_pred - ytrain)\n",
    "# Sort of residual absolute values\n",
    "abs_res = abs_residuals.sort_values(axis=0, ascending= False)\n",
    "# Get indexes of Xtrain that DON'T include the 5% (16) worst residuals \n",
    "inds = abs_res.index[int(len(abs_res)*0.05):len(abs_res)]\n",
    "\n",
    "#  Fit the LR model on the train subset with worst residual indexes removed\n",
    "lr.fit(Xtrain.loc[inds], ytrain.loc[inds])\n",
    "# Predict on test\n",
    "ytest_pred = lr.predict(Xtest)\n",
    "# Get R2 test score\n",
    "r2_score_test = r2_score(ytest, ytest_pred)\n",
    "print(\"\\nR2 LR Test Score with partial train after removing 5% (16) of the worst residuals\")\n",
    "print (r2_score_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.264275142854\n",
      "0.29332896244\n",
      "0.32086037704\n",
      "0.351212586565\n",
      "0.385042485886\n",
      "0.406321264825\n",
      "0.41529792149\n",
      "0.437240550728\n",
      "\n",
      "Final R2 Score on Train Data\n",
      "0.644791642815\n",
      "\n",
      "R2 Score on Test Data\n",
      "0.248176970476\n"
     ]
    }
   ],
   "source": [
    "def outliers(X, y, n):\n",
    "    # Performs a linear regression, removes n outliers and returns the\n",
    "    # X and y values\n",
    "    linreg = LinearRegression().fit(X, y)\n",
    "    preds = linreg.predict(X)\n",
    "    print(r2_score(preds, y))\n",
    "    differences = np.array(np.abs(preds - y))\n",
    "    for i in range(n):\n",
    "        maxIdx = np.argmax(differences)\n",
    "        differences = np.delete(differences, [maxIdx])\n",
    "        X.drop(X.index[maxIdx], inplace=True)\n",
    "        y.drop(y.index[maxIdx], inplace=True)\n",
    "    return X, y, linreg\n",
    "\n",
    "\n",
    "X_train_outlier = Xtrain.copy()\n",
    "y_train_outlier = ytrain.copy()\n",
    "\n",
    "# Remove 2 outliers for each pass, repeat 8 times\n",
    "# 16 points is approximately 5% of the data\n",
    "for i in range(8):\n",
    "    X_train_outlier, y_train_outlier, linreg = outliers(X_train_outlier, y_train_outlier, 2)\n",
    "\n",
    "# Get final R2 for test and train data\n",
    "linreg = LinearRegression().fit(X_train_outlier, y_train_outlier)\n",
    "train_preds = linreg.predict(X_train_outlier)\n",
    "print(\"\\nFinal R2 Score on Train Data\")\n",
    "print(r2_score(y_train_outlier, train_preds))\n",
    "\n",
    "test_preds = linreg.predict(Xtest)\n",
    "print(\"\\nR2 Score on Test Data\")\n",
    "print(r2_score(ytest, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Explain the intuition behind your approach (we do not expect a detailed mathematical justification).\n",
    "We fit a linear regression model and then removed 5% of the training samples (16) with the highest absolute residual error and then refit a model without those samples.  Our rational behind this was that these samples would correspond to erroneous samples and removing then would improve test $R^2$.  This resulted in a very modest improvement in test $R^2$, 0.2502, vs test $R^2$ on the full train set of 0.2493. <br><br> We then modified our approach to see if we could improve the $R^2$ performance further, we fitted a LR model, removed the two training samples with the highest absolute residual error, measured train $R^2$ and then refitted the model again, removed the next two training samples with the highest absolute residual error, and so on, until 5% train samples were removed.  This approach yielded a modest decline in test $R^2$ performance, 0.2481 (though train $R^2$ performance jumped to 0.6448, suggesting overfitting).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
